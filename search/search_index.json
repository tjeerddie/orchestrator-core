{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Orchestrator-Core","text":"<p>Production ready Orchestration Framework to manage product lifecycle and workflows. Easy to use, Built on top of FastAPI</p> <p> </p> <p> The orchestrator core is a project developed by SURF to facilitate the orchestration of services. Together with ESnet this project has been opensourced in the commons conservancy to help facilitate collaboration. We invite all who are interested to take a look and to contribute!</p>"},{"location":"#orchestration","title":"Orchestration","text":"<p>When do you orchestrate and when do you automate? The answer is you probably need both. Automation helps you execute repetitive tasks reliably and easily, Orchestration adds a layer and allows you to add more intelligence to the tasks you need to automate and  to have a complete audit log of changes.</p>"},{"location":"#orchestrate-transitive-verb","title":"Orchestrate* - Transitive Verb","text":"<p>/\u02c8\u00f4rk\u0259\u02ccstr\u0101t/ /\u02c8\u0254rk\u0259\u02ccstre\u026at/</p> <p>1: Arrange or score (music) for orchestral performance.</p> <p>\u2018the song cycle was stunningly arranged and orchestrated\u2019</p> <p>2:  Arrange or direct the elements of (a situation) to produce a desired effect, especially surreptitiously.</p> <p>\u2018the developers were able to orchestrate a favorable media campaign\u2019</p>"},{"location":"#project-goal","title":"Project Goal","text":"<p>This orchestrator-core provides a framework through which you can manage service orchestration for your end users. It enables you to define products to which users can subscribe, and helps you intelligently manage the lifecycle of Creation, Modification, Termination and Validation of a customer subscription.</p>"},{"location":"#usage","title":"Usage","text":"<p>This project can be installed as follows:</p>"},{"location":"#step-1","title":"Step 1:","text":"<p>Install the core. <pre><code>pip install orchestrator-core\n</code></pre></p>"},{"location":"#step-2","title":"Step 2:","text":"<p>Create a postgres database: <pre><code>createuser -sP nwa\ncreatedb orchestrator-core -O nwa\n</code></pre></p>"},{"location":"#step-3-optional","title":"Step 3 (optional):","text":"<p>When using multiple workers, you will need a redis server for live updates with websockets.</p> <p>By default it will use memory which works with only one worker. <pre><code>export WEBSOCKET_BROADCASTER_URL=\"memory://\"\n</code></pre></p> <p>For the redis connection you need to set the env variable with the connection url. <pre><code>export WEBSOCKET_BROADCASTER_URL=\"redis://localhost:6379\"\n</code></pre></p> <p>Websockets can also be turned off with: <pre><code>export ENABLE_WEBSOCKETS=False\n</code></pre></p> <p>More broadcaster info here</p> <p>If you want to use pickle for CACHE serialization you will need to set the <code>CACHE_HMAC_SECRET</code>: <pre><code>export CACHE_HMAC_SECRET=\"SOMESECRET\"\n</code></pre> NOTE: The key can be any length. However, the recommended size is 1024 bits.</p>"},{"location":"#step-4","title":"Step 4:","text":"<p>Create a <code>main.py</code> file.</p> <pre><code>from orchestrator import OrchestratorCore\nfrom orchestrator.cli.main import app as core_cli\nfrom orchestrator.settings import AppSettings\n\napp = OrchestratorCore(base_settings=AppSettings())\n\nif __name__ == \"__main__\":\n    core_cli()\n</code></pre>"},{"location":"#step-5-optional","title":"Step 5 (Optional):","text":"<p>OrchestratorCore comes with a graphql interface that can to be registered after you create your OrchestratorApp. If you add it after registering your <code>SUBSCRIPTION_MODEL_REGISTRY</code> it will automatically create graphql types for them. More info can be found in <code>docs/architecture/application/graphql.md</code></p> <p>example: <pre><code>app = OrchestratorCore(base_settings=AppSettings())\n# register SUBSCRIPTION_MODEL_REGISTRY\napp.register_graphql()\n</code></pre></p>"},{"location":"#step-6","title":"Step 6:","text":"<p>Initialize the migration environment. <pre><code>PYTHONPATH=. python main.py db init\nPYTHONPATH=. python main.py db upgrade heads\n</code></pre></p>"},{"location":"#step-7","title":"Step 7:","text":"<p>Profit :)</p> <p>Authentication and authorization are default enabled, to disable set <code>OAUTH2_ACTIVE</code> and <code>OAUTH2_AUTHORIZATION_ACTIVE</code> to <code>False</code>.</p> <pre><code>uvicorn --reload --host 127.0.0.1 --port 8080 main:app\n</code></pre> <p>Visit http://127.0.0.1:8080/api/redoc to view the api documentation.</p>"},{"location":"#setting-up-a-development-environment","title":"Setting up a development environment","text":"<p>To add features to the repository follow the following procedure to setup a working development environment.</p>"},{"location":"#installation-development-standalone","title":"Installation (Development standalone)","text":"<p>Install the project and its dependencies to develop on the code.</p>"},{"location":"#step-1-install-flit","title":"Step 1 - install flit:","text":"<pre><code>python3 -m venv venv\nsource venv/bin/activate\npip install flit\n</code></pre>"},{"location":"#step-2-install-the-development-code","title":"Step 2 - install the development code:","text":"<p>Danger</p> <p>Make sure to use the flit binary that is installed in your environment. You can check the correct path by running <pre><code>which flit\n</code></pre></p> <p>To be sure that the packages will be installed against the correct venv you can also prepend the python interpreter that you want to use:</p> <pre><code>flit install --deps develop --symlink --python venv/bin/python\n</code></pre>"},{"location":"#running-tests","title":"Running tests","text":"<p>Run the unit-test suite to verify a correct setup.</p>"},{"location":"#step-1-create-a-database","title":"Step 1 - Create a database","text":"<pre><code>createuser -sP nwa\ncreatedb orchestrator-core-test -O nwa\n</code></pre>"},{"location":"#step-2-run-tests","title":"Step 2 - Run tests","text":"<p><pre><code>pytest test/unit_tests\n</code></pre> or with xdist:</p> <pre><code>pytest -n auto test/unit_tests\n</code></pre> <p>If you do not encounter any failures in the test, you should be able to develop features in the orchestrator-core.</p>"},{"location":"#installation-development-symlinked-into-orchestrator-surf","title":"Installation (Development symlinked into orchestrator SURF)","text":"<p>If you are working on a project that already uses the <code>orchestrator-core</code> and you want to test your new core features against it, you can use some <code>flit</code> magic to symlink the dev version of the core to your project. It will automatically replace the pypi dep with a symlink to the development version of the core and update/downgrade all required packages in your own orchestrator project.</p>"},{"location":"#step-1-install-flit_1","title":"Step 1 - install flit:","text":"<pre><code>python - m venv venv\nsource venv/bin/activate\npip install flit\n</code></pre>"},{"location":"#step-2-symlink-the-core-to-your-own-project","title":"Step 2 - symlink the core to your own project","text":"<pre><code>flit install --deps develop --symlink --python /path/to/a/orchestrator-project/venv/bin/python\n</code></pre> <p>So if you have the core and your own orchestrator project repo in the same folder and the main project folder is <code>orchestrator</code> and you want to use relative links, this will be last step:</p> <pre><code>flit install --deps develop --symlink --python ../orchestrator/venv/bin/python\n</code></pre>"},{"location":"#increasing-the-version-number-for-a-pre-release","title":"Increasing the version number for a (pre) release.","text":"<p>When your PR is accepted you will get a version number.</p> <p>You can do the necessary change with a clean, e.g. every change committed, branch:</p> <pre><code>bumpversion patch --new-version 0.4.1-rc3\n</code></pre>"},{"location":"architecture/tldr/","title":"Architecture; TLDR","text":"<p>The architecture of how the orchestrator-core is setup can be split in two sections. The orchestration philosophy of how workflows are setup and run, and how the application can be used to define products that can be subscribed to by customers.</p>"},{"location":"architecture/tldr/#application-architecture","title":"Application architecture","text":"<p>The Application extends a FastAPI application and therefore can make use of all the awesome features of FastAPI, pydantic and asyncio python.</p>"},{"location":"architecture/tldr/#step-engine","title":"Step Engine","text":"<p>At its core the Orchestrator workflow engine will execute a list of functions in order and store the result of each function to the database. The Orchestrator is able to execute any list of functions that the user envisions so long as they return a dictionary and/or consume variables stored in keys under that dictionary.</p> <pre><code>@workflow(\"Name of the workflow\", initial_input_form=input_form_generator)\ndef workflow():\n    return (\n        init\n        &gt;&gt; arbitrary_step_func_1\n        &gt;&gt; arbitrary_step_func_2\n        &gt;&gt; arbitrary_step_func_3\n        &gt;&gt; done\n    )\n</code></pre> <p>The <code>@workflow</code> decorator converts what the function returns into a <code>StepList</code> which the engine executes sequentially. If and when the step functions raise an exception, the workflow will fail at that step and allow the user to retry.</p>"},{"location":"architecture/tldr/#products-and-subscriptions","title":"Products and Subscriptions","text":"<p>The second part of the orchestrator is a product database that allows a developer to define a collection of logically grouped resources, that when filled in create a Subscription, given to a customer. The description of a product is done in the <code>Product</code>, <code>FixedInput</code>, <code>ProductBlock</code> and <code>ResourceType</code> tables. When a workflow creates a subscription for a customer it creates instances of a <code>Product</code>, <code>ProductBlock</code> and <code>ResourceType</code> and stores them as <code>Subscriptions</code>, <code>SubscriptionInstances</code> and <code>`SubscriptionInstanceValues.</code></p> <p>It is therefore possible to have N number of Subscriptions to a single product. A workflow is typically executed to manipulate a Subscription and transition it from one lifecycle state to another (<code>Initial</code>, <code>Provisioning</code>, <code>Active</code>, <code>Terminated</code>).</p>"},{"location":"architecture/application/api/","title":"Api documentation","text":""},{"location":"architecture/application/cli/","title":"CLI","text":"<p>CLI commands</p>"},{"location":"architecture/application/cli/#db-migrate-domain-models-command","title":"db migrate-domain-models command","text":"<p>The purpose of this CLI script is to automatically generate the data migrations that you'll need when you add or change a Domain Model. It will inspect your DB and the existing domain models, analyse the differences and it will generate an Alembic data migration in the correct folder.</p> <p>Features:</p> <ul> <li>detect a new Domain Model attribute / resource type</li> <li>detect a renamed Domain Model attribute / resource type</li> <li>detect a removed Domain Model attribute / resource type</li> <li>detect a new Domain Model</li> <li>detect a removed Domain Model</li> <li>ability to ask for human input when needed</li> </ul> <p>Below in the documentation these features are discussed in more detail.</p> <p>BACKUP DATABASE BEFORE USING THE MIGRATION!</p>"},{"location":"architecture/application/cli/#args","title":"Args:","text":"<ul> <li><code>message</code>: Message/description of the generated migration.</li> <li><code>--test</code>: Optional boolean if you don't want to generate a migration file.</li> <li><code>--inputs</code>: stringified dict to prefill inputs.     The inputs and updates argument is mostly used for testing, prefilling the given inputs, here examples:<ul> <li>new product: <code>inputs = { \"new_product_name\": { \"description\": \"add description\", \"product_type\": \"add_type\", \"tag\": \"add_tag\" }}</code></li> <li>new product fixed input: <code>inputs = { \"new_product_name\": { \"new_fixed_input_name\": \"value\" }}</code></li> <li>new product block: <code>inputs = { \"new_product_block_name\": { \"description\": \"add description\", \"tag\": \"add_tag\" } }</code></li> <li>new resource type: <code>inputs = { \"new_resource_type_name\": { \"description\": \"add description\", \"value\": \"add default value\", \"new_product_block_name\": \"add default value for block\" }}</code><ul> <li><code>new_product_block_name</code> prop inserts value specifically for that block.</li> <li><code>value</code> prop is inserted as default for all existing instances it is added to.</li> </ul> </li> </ul> </li> <li><code>--updates</code>: stringified dict to prefill inputs.<ul> <li>renaming a fixed input:<ul> <li><code>updates = { \"fixed_inputs\": { \"product_name\": { \"old_fixed_input_name\": \"new_fixed_input_name\" } } }</code></li> </ul> </li> <li>renaming a resource type to a new resource type:<ul> <li><code>inputs = { \"new_resource_type_name\": { \"description\": \"add description\" }}</code></li> <li><code>updates = { \"resource_types\": { \"old_resource_type_name\": \"new_resource_type_name\" } }</code></li> </ul> </li> <li>renaming a resource type to existing resource type: <code>updates = { \"resource_types\": { \"old_resource_type_name\": \"new_resource_type_name\" } }</code></li> </ul> </li> </ul>"},{"location":"architecture/application/cli/#example","title":"Example","text":"<p>You need products in the <code>SUBSCRIPTION_MODEL_REGISTRY</code>, for this example I will use these models (taken out of example-orchestrator):</p> <ul> <li> <p>UserGroup Block:     <pre><code>from orchestrator.domain.base import SubscriptionModel, ProductBlockModel\nfrom orchestrator.types import SubscriptionLifecycle\n\n\nclass UserGroupBlockInactive(\n    ProductBlockModel,\n    lifecycle=[SubscriptionLifecycle.INITIAL],\n    product_block_name=\"UserGroupBlock\",\n):\n    group_name: str | None = None\n    group_id: int | None = None\n\n\nclass UserGroupBlockProvisioning(\n    UserGroupBlockInactive, lifecycle=[SubscriptionLifecycle.PROVISIONING]\n):\n    group_name: str\n    group_id: int | None = None\n\n\nclass UserGroupBlock(\n    UserGroupBlockProvisioning, lifecycle=[SubscriptionLifecycle.ACTIVE]\n):\n    group_name: str\n    group_id: int\n</code></pre></p> </li> <li> <p>UserGroup Product:     <pre><code>from orchestrator.domain.base import SubscriptionModel\nfrom orchestrator.types import SubscriptionLifecycle\n\nfrom products.product_blocks.user_group import (\n    UserGroupBlock,\n    UserGroupBlockInactive,\n    UserGroupBlockProvisioning,\n)\n\n\nclass UserGroupInactive(\n    SubscriptionModel, is_base=True, lifecycle=[SubscriptionLifecycle.INITIAL]\n):\n    settings: UserGroupBlockInactive\n\n\nclass UserGroupProvisioning(\n    UserGroupInactive, lifecycle=[SubscriptionLifecycle.PROVISIONING]\n):\n    settings: UserGroupBlockProvisioning\n\n\nclass UserGroup(UserGroupProvisioning, lifecycle=[SubscriptionLifecycle.ACTIVE]):\n    settings: UserGroupBlock\n</code></pre></p> </li> <li> <p>User Block:     <pre><code>from orchestrator.domain.base import ProductBlockModel\nfrom orchestrator.types import SubscriptionLifecycle\n\nfrom products.product_blocks.user_group import (\n    UserGroupBlock,\n    UserGroupBlockInactive,\n    UserGroupBlockProvisioning,\n)\n\n\nclass UserBlockInactive(\n    ProductBlockModel,\n    lifecycle=[SubscriptionLifecycle.INITIAL],\n    product_block_name=\"UserBlock\",\n):\n    group: UserGroupBlockInactive\n    username: str | None = None\n    age: int | None = None\n    user_id: int | None = None\n\n\nclass UserBlockProvisioning(\n    UserBlockInactive, lifecycle=[SubscriptionLifecycle.PROVISIONING]\n):\n    group: UserGroupBlockProvisioning\n    username: str\n    age: int | None = None\n    user_id: int | None = None\n\n\nclass UserBlock(UserBlockProvisioning, lifecycle=[SubscriptionLifecycle.ACTIVE]):\n    group: UserGroupBlock\n    username: str\n    age: int | None = None\n    user_id: int\n</code></pre></p> </li> <li> <p>User Product:     <pre><code>from orchestrator.domain.base import SubscriptionModel\nfrom orchestrator.types import SubscriptionLifecycle, strEnum\n\nfrom products.product_blocks.user import (\n    UserBlock,\n    UserBlockInactive,\n    UserBlockProvisioning,\n)\n\n\nclass Affiliation(strEnum):\n    internal = \"internal\"\n    external = \"external\"\n\n\nclass UserInactive(SubscriptionModel, is_base=True):\n    affiliation: Affiliation\n    settings: UserBlockInactive\n\n\nclass UserProvisioning(UserInactive, lifecycle=[SubscriptionLifecycle.PROVISIONING]):\n    affiliation: Affiliation\n    settings: UserBlockProvisioning\n\n\nclass User(UserProvisioning, lifecycle=[SubscriptionLifecycle.ACTIVE]):\n    affiliation: Affiliation\n    settings: UserBlock\n</code></pre></p> </li> <li> <p><code>SUBSCRIPTION_MODEL_REGISTRY</code>:     <pre><code>from orchestrator.domain import SUBSCRIPTION_MODEL_REGISTRY\n\nfrom products.product_types.user import User\nfrom products.product_types.user_group import UserGroup\n\n# Register models to actual definitions for deserialization purposes\nSUBSCRIPTION_MODEL_REGISTRY.update(\n    {\n        \"User group\": UserGroup,\n        \"User internal\": User,\n        \"User external\": User,\n    }\n)\n</code></pre></p> </li> </ul> <p>Running the command:</p> <ul> <li> <p>only with a message     <pre><code>python main.py db migrate-domain-models \"message\"\n</code></pre></p> </li> <li> <p>Running it as test     <pre><code>python main.py db migrate-domain-models \"message\" --test\n</code></pre></p> </li> <li> <p>Running the command with inputs prefilled     <pre><code>python main.py db migrate-domain-models \"message\" --inputs \"{ \"\" }\"\n</code></pre></p> </li> </ul> <p>The command will first go through all products and map the differences with the database. debug log example: <pre><code>2022-10-27 11:45:10 [debug] ProductTable blocks diff [orchestrator.domain.base] fixed_inputs_in_db=set() fixed_inputs_model=set() missing_fixed_inputs_in_db=set() missing_fixed_inputs_in_model=set() missing_product_blocks_in_db=set() missing_product_blocks_in_model=set() product_block_db=User group product_blocks_in_db={'UserGroupBlock'} product_blocks_in_model={'UserGroupBlock'}\n</code></pre></p> <p>You will be prompted with inputs when updates are found.</p> <ul> <li> <p>rename of resource type input (renaming <code>age</code> to <code>user_age</code> in User Block). Only works when the resource type is renamed in all Blocks:</p> <p>Update resource types Do you wish to rename resource type age to user_age? [y/N]:</p> </li> <li> <p>rename of fixed input (renaming <code>affiliation</code> to <code>affiliationing</code> in User Product):</p> <p>Update fixed inputs Do you wish to rename fixed input affiliation to affiliationing for product User internal? [y/N]:</p> </li> <li> <p>update of resource type per block (renaming <code>age</code> to <code>user_age</code> in User Block and not chosing to rename resource type). The input will loop until skipped or when there are no options anymore:</p> <ul> <li>first you get to choose which old resource type to update, skip will create/delete all resource types.     &gt; Update block resource types <pre><code>Which resource type would you want to update in UserBlock Block?\n1) age\nq) skip\n?\n</code></pre></li> <li>then you get to choose which new resource type to update with, skip will give you the first question again.     <pre><code>Which resource type should update age?\n1) user_age\nq) skip\n?\n</code></pre></li> <li>with 1 and 1, the log level difference would look like:     <pre><code>2023-02-08 14:11:25 [info] update_block_resource_types [orchestrator.cli.migrate_domain_models] update_block_resource_types={'UserBlock': {'age': 'user_age'}}\n</code></pre></li> </ul> </li> </ul> <p>It will log the differences on info level:</p> <pre><code>2022-10-27 11:45:10 [info] create_products                   [orchestrator.cli.migrate_domain_models] create_products={'User group': &lt;class 'products.product_types.user_group.UserGroup'&gt;, 'User internal': &lt;class 'products.product_types.user.User'&gt;, 'User external': &lt;class 'products.product_types.user.User'&gt;}\n2022-10-27 11:45:10 [info] delete_products                   [orchestrator.cli.migrate_domain_models] delete_products=set()\n2022-10-27 11:45:10 [info] create_product_fixed_inputs       [orchestrator.cli.migrate_domain_models] create_product_fixed_inputs={'affiliation': {'User external', 'User internal'}}\n2022-10-27 11:45:10 [info] update_product_fixed_inputs       [orchestrator.cli.migrate_domain_models] update_product_fixed_inputs={}\n2022-10-27 11:45:10 [info] delete_product_fixed_inputs       [orchestrator.cli.migrate_domain_models] delete_product_fixed_inputs={}\n2022-10-27 11:45:10 [info] create_product_to_block_relations [orchestrator.cli.migrate_domain_models] create_product_to_block_relations={'UserGroupBlock': {'User group'}, 'UserBlock': {'User external', 'User internal'}}\n2022-10-27 11:45:10 [info] delete_product_to_block_relations [orchestrator.cli.migrate_domain_models] delete_product_to_block_relations={}\n2022-10-27 11:45:10 [info] create_resource_types             [orchestrator.cli.migrate_domain_models] create_resource_types={'username', 'age', 'group_name', 'user_id', 'group_id'}\n2022-10-27 11:45:10 [info] rename_resource_types             [orchestrator.cli.migrate_domain_models] rename_resource_types={}\n2022-10-27 11:45:10 [info] delete_resource_types             [orchestrator.cli.migrate_domain_models] delete_resource_types=set()\n2022-10-27 11:45:10 [info] create_resource_type_relations    [orchestrator.cli.migrate_domain_models] create_resource_type_relations={'group_name': {'UserGroupBlock'}, 'group_id': {'UserGroupBlock'}, 'username': {'UserBlock'}, 'age': {'UserBlock'}, 'user_id': {'UserBlock'}}\n2022-10-27 11:45:10 [info] delete_resource_type_relations    [orchestrator.cli.migrate_domain_models] delete_resource_type_relations={}\n2022-10-27 11:45:10 [info] create_product_blocks             [orchestrator.cli.migrate_domain_models] create_blocks={'UserGroupBlock': &lt;class 'products.product_blocks.user_group.UserGroupBlock'&gt;, 'UserBlock': &lt;class 'products.product_blocks.user.UserBlock'&gt;}\n2022-10-27 11:45:10 [info] delete_product_blocks             [orchestrator.cli.migrate_domain_models] delete_blocks=set()\n2022-10-27 11:45:10 [info] create_product_block_relations    [orchestrator.cli.migrate_domain_models] create_product_block_relations={'UserGroupBlock': {'UserBlock'}}\n2022-10-27 11:45:10 [info] delete_product_block_relations    [orchestrator.cli.migrate_domain_models] delete_product_block_relations={}\n</code></pre> <p>You will be asked to confirm the actions in order to continue:</p> <p>WARNING: Deleting products will also delete its subscriptions. Confirm the above actions [y/N]:</p> <p>After confirming, it will start generating the SQL, logging the SQL on debug level and prompt the user for new resources:</p> <ul> <li> <p>new product example:</p> <p>Create new products Product: UserGroup User group Supply the production description: User group product Supply the product tag: GROUP <pre><code>2022-10-27 11:45:10 [debug] generated SQL [orchestrator.cli.domain_gen_helpers.helpers] sql_string=INSERT INTO products (name, description, product_type, tag, status) VALUES ('User group', 'User group product', 'UserGroup', 'GROUP', 'active') RETURNING products.product_id\n</code></pre></p> </li> <li> <p>new fixed input (the type isn't checked, so typing an incorrect value will insert in db):</p> <p>Create fixed inputs Supply fixed input value for product User internal and fixed input affiliation: internal Supply fixed input value for product User external and fixed input affiliation: external <pre><code>2022-10-27 11:45:10 [debug] generated SQL [orchestrator.cli.domain_gen_helpers.helpers] sql_string=INSERT INTO fixed_inputs (name, value, product_id) VALUES ('affiliation', 'internal', (SELECT products.product_id FROM products WHERE products.name IN ('User internal'))), ('affiliation', 'external', (SELECT products.product_id FROM products WHERE products.name IN ('User external')))\n</code></pre></p> </li> <li> <p>new product block:</p> <p>Create product blocks Product block: UserGroupBlock Supply the product block description: User group settings Supply the product block tag: UGS <pre><code>2022-10-27 11:45:10 [debug] generated SQL [orchestrator.cli.domain_gen_helpers.helpers] sql_string=`#!sql INSERT INTO product_blocks (name, description, tag, status) VALUES ('UserGroupBlock', 'User group settings', 'UGS', 'active') RETURNING product_blocks.product_block_id`\n</code></pre></p> </li> <li> <p>new resource type:</p> <p>Create resource types Supply description for new resource type group_name: Unique name of user group <pre><code>2022-10-27 11:45:10 [debug] generated SQL [orchestrator.cli.domain_gen_helpers.helpers] sql_string=INSERT INTO resource_types (resource_type, description) VALUES ('group_name', 'Unique name of user group') RETURNING resource_types.resource_type_id\n</code></pre></p> </li> <li> <p>default value for resource type per product block (necessary for adding a default value to existing instances):</p> <p>Create subscription instance values Supply default subscription instance value for resource type group_name and product block UserGroupBlock: group <pre><code>2022-10-27 11:45:10 [debug] generated SQL [orchestrator.cli.domain_gen_helpers.resource_type_helpers] sql_string=\nWITH subscription_instance_ids AS (\nSELECT subscription_instances.subscription_instance_id\nFROM   subscription_instances\nWHERE  subscription_instances.product_block_id IN (\nSELECT product_blocks.product_block_id\nFROM   product_blocks\nWHERE  product_blocks.name = 'UserGroupBlock'\n)\n)\n\nINSERT INTO\nsubscription_instance_values (subscription_instance_id, resource_type_id, value)\nSELECT\nsubscription_instance_ids.subscription_instance_id,\nresource_types.resource_type_id,\n'group'\nFROM resource_types\nCROSS JOIN subscription_instance_ids\nWHERE resource_types.resource_type = 'group_name'\n</code></pre></p> </li> </ul> <p>Last part generates the migration with the generated SQL: <pre><code>Generating migration file\n2022-10-27 11:45:10 [info] Version Locations [orchestrator.cli.database] locations=/home/tjeerddie/projects_surf/example-orchestrator/migrations/versions/schema /home/tjeerddie/projects_surf/example-orchestrator/.venv/lib/python3.10/site-packages/orchestrator/migrations/versions/schema\n  Generating /home/tjeerddie/projects_surf/example-orchestrator/migrations/versions/schema/2022-10-27_a8946b2d1647_test.py ...  done\nMigration generated. Don't forget to create a database backup before migrating!\n</code></pre></p> <p>If you are running with <code>--test</code>, the SQL file will not be generated.</p>"},{"location":"architecture/application/domainmodels/","title":"Domain Models - Why do we need them?","text":"<p>Domain Models are designed to help the developer manage complex subscription models and interact with the objects in a user-friendly way. Domain models leverage the Pydantic with some extra sauce to dynamically cast variables from the database where they are stored as a string to their correct type in Python at runtime.</p> <p>Domain Model benefits</p> <ul> <li>Strict MyPy typing and validation in models.</li> <li>Type Safe serialisation to and from the database</li> <li>Subscription lifecycle transition enforcement</li> <li>Hierarchy enforcement with domain models</li> <li>Customer Facing resources vs resource facing resources</li> </ul> <p>When implementing domain models it is possible to link all resources together as they are nodes in a graph through the relations defined in the domain models.</p>"},{"location":"architecture/application/domainmodels/#type-safety-during-serialisation","title":"Type Safety during serialisation","text":"<p>Logic errors that depend on type evaluations/comparisons are prevented by using domain models to serialise database objects. This has a number of benefits as it saves the user the effort of casting the database result to the correct type and allows the developer to be more <code>Type safe</code> whilst developing.</p>"},{"location":"architecture/application/domainmodels/#example","title":"Example","text":"<p>Example</p> <p>The main reason for developing domain models was to make sure bugs like this occurred less.</p>"},{"location":"architecture/application/domainmodels/#pre-domain-models","title":"Pre domain models","text":"<pre><code>&gt;&gt;&gt; some_subscription_instance_value = SubscriptionInstanceValueTable.get(\"ID\")\n&gt;&gt;&gt; instance_value_from_db = some_subscription_instance_value.value\n&gt;&gt;&gt; instance_value_from_db\n\"False\"\n&gt;&gt;&gt; if instance_value_from_db is True:\n...    print(\"True\")\n... else:\n...    print(\"False\")\n\"True\"\n</code></pre>"},{"location":"architecture/application/domainmodels/#post-domain-models","title":"Post domain models","text":"<pre><code>&gt;&gt;&gt; some_subscription_instance_value = SubscriptionInstanceValueTable.get(\"ID\")\n&gt;&gt;&gt; instance_value_from_db = some_subscription_instance_value.value\n&gt;&gt;&gt; type(instance_value_from_db)\n&lt;class str&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; subscription_model = SubscriptionModel.from_subscription(\"ID\")\n&gt;&gt;&gt; type(subscription_model.product_block.instance_from_db)\n&lt;class bool&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; subscription_model.product_block.instance_from_db\nFalse\n&gt;&gt;&gt;\n&gt;&gt;&gt; if subscription_model.product_block.instance_from_db is True:\n...    print(\"True\")\n... else:\n...    print(\"False\")\n\"False\"\n</code></pre>"},{"location":"architecture/application/domainmodels/#lifecycle-transitions","title":"Lifecycle transitions","text":"<p>When transitioning from <code>Initial</code> -&gt; <code>Provisioning</code> -&gt; <code>Active</code> -&gt; <code>Terminated</code> in the Subscription Lifecycle the domain model definitions make sure that all resource types and product blocks are assigned correctly. Typically the <code>Initial</code> status is less strict compared to the <code>Active</code> lifecycle. When assigning product blocks from other subscriptions as dependent on a product block from the subscription that is being modified, the domain-models respect Subscription boundaries and do not update variables and resources in the related Subscription product block.</p>"},{"location":"architecture/application/domainmodels/#enforcing-hierarchy","title":"Enforcing Hierarchy","text":"<p>When defining and modelling products it's often necessary to model resources that are in use by or dependent on other product blocks. A product block of a subscription can also be dependent on a product block from another subscription. This way a hierarchy of product blocks from all subscriptions can be build where the ownership of any product block is determined by the subscription it belongs to.</p>"},{"location":"architecture/application/domainmodels/#a-couple-of-examples-of-subscription-hierarchies","title":"A couple of Examples of subscription hierarchies","text":"<p>We will describe some practical examples to explain how you can deal with complex customers requirements, and how to layer subscriptions to represent a complex portfolio of network services.</p> <ol> <li> <p>Consider the relation between a Node and a Port: When you create Node and Port subscriptions. You should not be allowed to Terminate the Node subscriptions when the Port subscriptions are still being used by customers.</p> </li> <li> <p>Consider a scenario for networking with a layer 2 circuit, one needs at least two interfaces and VLAN configuration to create the circuit. The interfaces may be owned by different customers than the owner of the circuit. Typically we assign a subscription to a customer which contains the interface resource. That interface resource is then used again in the circuit subscription, as a resource.</p> </li> </ol>"},{"location":"architecture/application/domainmodels/#code-examples","title":"Code examples","text":""},{"location":"architecture/application/domainmodels/#product-block-model","title":"Product Block Model","text":"<p>Product block models are reusable Pydantic classes that enable the user to reuse product blocks in multiple Products. They are defined in lifecycle state and can be setup to be very restrictive or less restrictive. The orchestrator supports hierarchy in the way product block models reference each other. In other words, a product block model, may have a property that references one or more other product block models.</p> <p>Info</p> <p>The Product block model should be modeled as though it is a resource that can be re-used in multiple products. In networking the analogy would be: A physical interface may be used in a Layer 2 service and Layer 3 service It is not necessary to define two different physical interface types.</p>"},{"location":"architecture/application/domainmodels/#product-block-model-inactive","title":"Product Block Model - Inactive","text":"<p><pre><code>class ServicePortBlockInactive(ProductBlockModel, product_block_name=\"Service Port\"):\n\"\"\"Object model for a SN8 Service Port product block.\"\"\"\n\nnso_service_id: Optional[UUID] = None\nport_mode: Optional[PortMode] = None\n    lldp: Optional[bool] = None\n    ims_circuit_id: Optional[int] = None\n    auto_negotiation: Optional[bool] = None\n    node: Optional[NodeProductBlock] = None\n</code></pre> As you can see in this model we define it as an Inactive Class. As parameter we pass the name of the product_block in the database. In the second highlighted line you see a variable. This references a <code>resource_type</code> in the database, and annotates what type it should be at runtime. In the <code>Inactive</code> or <code>Initial</code> phase of the Subscription lifecycle we are least restrictive in annotating the properties; All fields/resource types are Optional.</p>"},{"location":"architecture/application/domainmodels/#product-block-model-provisioning","title":"Product Block Model - Provisioning","text":"<p><pre><code>class ServicePortBlockProvisioning(\nServicePortBlockInactive , lifecycle=[SubscriptionLifecycle.PROVISIONING]\n):\n\"\"\"Object model for a SN8 Service Port product block in active state.\"\"\"\n\nnso_service_id: UUID\nport_mode: PortMode\n    lldp: bool\n    ims_circuit_id: Optional[int] = None\n    auto_negotiation: Optional[bool] = None\n    node: NodeProductBlock\n</code></pre> In this stage whe have changed the way a Subscription domain model should look like in a certain Lifecycle state. You also see that the <code>resource_type</code> now no-longer is Optional. It must exist in this instantiation of the class. The model will raise a <code>ValidationError</code> upon <code>.save()</code> if typing is not filled in correctly.</p>"},{"location":"architecture/application/domainmodels/#product-block-model-active","title":"Product Block Model - Active","text":"<pre><code>class ServicePortBlock(ServicePortBlockProvisioning, lifecycle=[SubscriptionLifecycle.ACTIVE]):\n\"\"\"Object model for a SN8 Service Port product block in active state.\"\"\"\n\nnso_service_id: UUID\nport_mode: PortMode\nlldp: bool\nims_circuit_id: int\nauto_negotiation: Optional[bool] = None\nnode: NodeProductBlock\n</code></pre> <p>The Class is now defined in its most strict form, in other words in the Active lifecycle of a subscription, this product block model must have all resource_types filled in except for <code>auto_negotiation</code> to function correctly.</p> <p>Tip</p> <p>The stricter you are in defining your product block models the more you are able to leverage the built in validation of<code>Pydantic</code>.</p>"},{"location":"architecture/application/domainmodels/#product-model-aka-subscriptionmodel","title":"Product Model a.k.a SubscriptionModel","text":"<p>Product models are very similar to Prodblock Models in that they adhere to the same principles as explained above. However the difference to Product Block models is that they create <code>Subscriptions</code> in the database. They must always have a reference to a customer and instead of containing other <code>ProductBlockModel</code> or <code>resource_types</code> they contain either <code>fixed_inputs</code> which basically describe fixed product attributes or other <code>ProductBlockModels.</code></p>"},{"location":"architecture/application/domainmodels/#product-model-inactive","title":"Product Model - Inactive","text":"<pre><code>class ServicePortInitial(\nSubscriptionModel, is_base=True, lifecycle=[SubscriptionLifecycle.INITIAL, SubscriptionLifecycle.TERMINATED]\n):\n    domain: Domain\nport_speed: PortSpeed\nport: Optional[ServicePortBlockInactive] = None\n</code></pre> <p>In the above example you can observe the lifecycle definition as per the <code>ProductBlockModels</code>. Below that you see <code>fixed_inputs</code> These can be of any type, however if they are a <code>SubClass</code> of a <code>ProductBlockModel</code> the code will automatically create a database instance of that object.</p>"},{"location":"architecture/application/domainmodels/#product-model-provisioning-and-active","title":"Product Model -  Provisioning and Active","text":"<pre><code>class ServicePortProvisioning(\n    ServicePortInitial, lifecycle=[SubscriptionLifecycle.PROVISIONING]\n):\n    domain: Domain\n    port_speed: PortSpeed\nport: ServicePortBlockProvisioning\nclass ServicePort(ServicePortProvisioning, lifecycle=[SubscriptionLifecycle.ACTIVE]):\n    domain: Domain\n    port_speed: PortSpeed\nport: ServicePortBlock\n</code></pre> <p>Again you can observe how the Product definition changes depending on the lifecycle. It annotates a different type to the <code>port</code> property in <code>SubscriptionLifecycle.ACTIVE</code> compared to <code>SubscriptionLifecycle.PROVISIONING</code>.</p>"},{"location":"architecture/application/domainmodels/#advanced-use-cases","title":"Advanced Use Cases","text":""},{"location":"architecture/application/domainmodels/#crossing-the-subscription-boundary","title":"Crossing the subscription boundary","text":"<p>As mentioned before an advanced use case would be to use <code>ProductBlockModels</code> from other Subscriptions.</p> <p>Example</p> <pre><code>&gt;&gt;&gt; first_service_port = ServicePort.from_subscription(subscription_id=\"ID\")\n&gt;&gt;&gt; first_service_port.customer_id\n\"Y\"\n&gt;&gt;&gt;\n&gt;&gt;&gt; second_service_port = ServicePort.from_product(product_id=\"ID\", customer_id=\"ID\")\n&gt;&gt;&gt; second_service_port.port = first_service_port.port\n&gt;&gt;&gt; second_service_port.save()\n&gt;&gt;&gt;\n&gt;&gt;&gt; second_service_port.port.subscription == first_service_port.subscription\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; second_service_port.port.subscription == second_service_port.subscription\nFalse\n</code></pre> <p>This is valid use of the domain models. The code will detect that <code>port</code> is part of <code>first_service_port</code> and respect ownership. It basically will treat it as a <code>read-only</code> property.</p>"},{"location":"architecture/application/domainmodels/#union-types","title":"Union types","text":"<p>There may also be a case where a user would like to define two different types to a <code>ProductBlockModel</code> propery. This can be achieved by using the <code>Union</code> type decorator.</p> <p>Danger</p> <p>When using this method be sure as to declare the Most specific type first. This is how Pydantic attempts to cast types to the property. For more background as to why, read here</p> <pre><code>class ServicePort(ServicePortProvisioning, lifecycle=[SubscriptionLifecycle.ACTIVE]):\n    domain: Domain\n    port_speed: PortSpeed\nport: Union[ServicePortBlock, DifferentServicePortBlock]\n</code></pre>"},{"location":"architecture/application/forms/","title":"Form input logic","text":"<p>In the orchestrator core, form input elements are now class based and subclass the <code>FormPage</code> class in the core.</p> <pre><code>from orchestrator.forms import FormPage, ReadOnlyField\n</code></pre> <p>And the validators module exposes validators that also function as \"input type widgets\":</p> <pre><code>from orchestrator.forms.validators import OrganisationId, choice_list, Choice\n</code></pre> <p>It's worth poking around in that module to see the various input types the core library exposes.</p>"},{"location":"architecture/application/forms/#form-examples","title":"Form examples","text":"<p>Here is a relatively simple input form:</p> <pre><code>equipment = get_planned_equipment()\nchoices = [f\"{eq['name']}\" for eq in equipment]\n\nEquipmentList = choice_list(\n    Choice(\"EquipmentEnum\", zip(choices, choices)),\n    min_items=1,\n    max_items=1,\n    unique_items=True,\n)\n\n\nclass CreateNodeEnrollmentForm(FormPage):\n    class Config:\n        title = product_name\n\n    organisation: OrganisationId = ReadOnlyField(OrganisationId(ESNET_ORG_UUID))\n    select_node_choice: EquipmentList\n\n\n# Don't call like this CreateNodeEnrollmentForm() or you'll\n# get a vague error.\nuser_input = yield CreateNodeEnrollmentForm\n</code></pre> <p>It has a read only ORG ID and exposes a list of devices pulled from ESDB for the user to choose from.</p>"},{"location":"architecture/application/forms/#choice-widgets","title":"Choice widgets","text":"<p>Of note: <code>min_items</code> and <code>max_items</code> do not refer to the number of elements in the list. This UI construct allows for an arbitrary number of choices to be made - there are <code>+</code> and <code>-</code> options exposed in the UI allowing for multiple choices selected by the user. So <code>min 1 / max 1</code> tells the UI to display one pull down list of choices one of which must be selected, and additional choices can not be added.</p> <p>If one defined something like <code>min 1 / max 3</code> it would display one pulldown box by default and expose a <code>+</code> element in the UI. The user could click on it to arbitrarily add a second or a third pulldown list. <code>min 0</code> would not display any list by default but the user could use <code>+</code> to add some, etc.</p> <p>Since multiple choices are allowed, the results are returned as a list even if there is only a single choice element:</p> <pre><code>eq_name = user_input.select_node_choice[0]\n</code></pre> <p>The <code>zip()</code> maneuver takes the list and makes it into a dict with the same keys and values. So the display text doesn't have to be the same as the value returned.</p>"},{"location":"architecture/application/forms/#accept-actions","title":"Accept actions","text":"<p>Confirming actions is a common bit of functionality. This bit of code displays some read only NSO payload and lets the user ok the dry run:</p> <pre><code>from orchestrator.forms import FormPage, ReadOnlyField\nfrom orchestrator.forms.validators import Accept, LongText\n\n\ndef confirm_dry_run_results(dry_run_results: str) -&gt; State:\n    class ConfirmDryRun(FormPage):\n        nso_dry_run_results: LongText = ReadOnlyField(dry_run_results)\n        confirm_dry_run_results: Accept\n\n    user_input = yield ConfirmDryRun\n\n    return user_input\n</code></pre>"},{"location":"architecture/application/forms/#generic-python-types","title":"Generic python types","text":"<p>It is possible to mix generic python types in the with the defined validation fields:</p> <pre><code>class CreateLightPathForm(FormPage):\n    class Config:\n        title = product_name\n\n    organisation: OrganisationId\n    contact_persons: ContactPersonList = []  # type: ignore\n    ticket_id: JiraTicketId = \"\"  # type: ignore\n    service_ports: ListOfTwo[ServicePort]  # type: ignore\n    service_speed: bandwidth(\"service_ports\")  # type: ignore # noqa: F821\n    speed_policer: bool = False\n    remote_port_shutdown: bool = True\n</code></pre>"},{"location":"architecture/application/forms/#multi-step-form-input","title":"Multi step form input","text":"<p>Similar to the original \"list based\" form <code>Input</code> elements, to do a multistep form flow yield multiple times and then combine the results at the end:</p> <pre><code>def initial_input_form_generator(product: UUIDstr, product_name: str) -&gt; FormGenerator:\n    class CreateNodeForm(FormPage):\n        class Config:\n            title = product_name\n\n        organisation: OrganisationId = ReadOnlyField(\n            OrganisationId(SURFNET_NETWORK_UUID)\n        )\n        location_code: LocationCode\n        ticket_id: JiraTicketId = \"\"  # type:ignore\n\n    user_input = yield CreateNodeForm\n\n    class NodeIdForm(FormPage):\n        class Config:\n            title = product_name\n\n        ims_node_id: ims_node_id(\n            user_input.location_code, node_status=\"PL\"\n        )  # type:ignore # noqa: F821\n\n    user_input_node = yield NodeIdForm\n\n    return {**user_input.dict(), **user_input_node.dict()}\n</code></pre>"},{"location":"architecture/application/forms/#custom-form-field","title":"custom form field","text":"<p>You can create a custom field component in the frontend. The components in <code>orchestrator-gui/src/lib/uniforms-surfnet/src</code> can be used to study reference implementations for a couple of custom form field types.</p> <p>For it to show up in the form, you have to do 2 things, a pydantic type/class in the backend and add the component to the <code>AutoFieldLoader.tsx</code>.</p> <p>as an example I will create a custom field with name field and group select field.</p>"},{"location":"architecture/application/forms/#pydantic-typeclass-in-backend","title":"pydantic type/class in backend","text":"<p>Create a pydantic type/class.</p> <pre><code>from uuid import UUID\n\n\nclass ChooseUser(str):\n    group_id: UUID  # type:ignore\n\n    @classmethod\n    def __modify_schema__(cls, field_schema: dict[str, Any]) -&gt; None:\n        uniforms: dict[str, Any] = {}\n\n        if cls.group_id:\n            uniforms[\"groupId\"] = cls.group_id\n\n        field_schema.update(format=\"ChooseUser\", uniforms=uniforms)\n</code></pre> <p>And add it to a form:</p> <pre><code>def initial_input_form_generator(product: UUIDstr, product_name: str) -&gt; FormGenerator:\n    class ChoseUserForm(FormPage):\n        class Config:\n            title = product_name\n\n        user: ChooseUser\n\n    user_input = yield ChoseUserForm\n</code></pre> <p>To prefill the user_id, you need to add the value to the prop, for prefilling the group_id you need to create a new class:</p> <pre><code>def user_choice(group_id: int | None = None) -&gt; type:\n    namespace = {\"group_id\": group_id}\n    return new_class(\n        \"ChooseUserValue\", (ChooseUser,), {}, lambda ns: ns.update(namespace)\n    )\n\n\ndef initial_input_form_generator(product: UUIDstr, product_name: str) -&gt; FormGenerator:\n    class ChoseUserForm(FormPage):\n        class Config:\n            title = product_name\n\n        user: user_choice(\"group_id_1\") = \"user_id_1\"\n\n    user_input = yield ChoseUserForm\n</code></pre>"},{"location":"architecture/application/forms/#auto-field-loader","title":"auto field loader","text":"<p>The auto field loader is for loading the correct field component in the form. It has switches that check the field type and the field format. You have to add your new form field here.</p> <p>for this example, we would need to add to a <code>ChooseUser</code> case to the String switch:</p> <pre><code>...\nimport ChooseUserField from \"custom/uniforms/ChooseUserField\";\n\nexport function autoFieldFunction(props: GuaranteedProps&lt;unknown&gt; &amp; Record&lt;string, any&gt;, uniforms: Context&lt;unknown&gt;) {\nconst { allowedValues, checkboxes, fieldType, field } = props;\nconst { format } = field;\n\nswitch (fieldType) {\n...\ncase String:\nswitch (format) {\n...\ncase \"ChooseUser\":\nreturn ChooseUserField;\n}\nbreak;\n}\n\n...\n}\n</code></pre>"},{"location":"architecture/application/forms/#custom-field-example","title":"custom field example","text":"<p>example custom field to select a user by group.</p> <pre><code>import { EuiFlexItem, EuiFormRow, EuiText } from \"@elastic/eui\";\nimport { FieldProps } from \"lib/uniforms-surfnet/src/types\";\nimport React, { useCallback, useContext, useEffect, useState } from \"react\";\nimport { WrappedComponentProps, injectIntl } from \"react-intl\";\nimport ReactSelect, { SingleValue } from \"react-select\";\nimport { getReactSelectTheme } from \"stylesheets/emotion/utils\";\nimport { connectField, filterDOMProps } from \"uniforms\";\nimport ApplicationContext from \"utils/ApplicationContext\";\nimport { Option } from \"utils/types\";\nimport { css } from \"@emotion/core\";\n\nexport const ChoosePersonFieldStyling = css`\n    section.group-user {\n        display: flex;\n        flex-direction: row;\n        flex-wrap: wrap;\n\n        div.group-select {\n            width: 50%;\n        }\n        div.user-select {\n            width: 50%;\n            padding-left: 5px;\n        }\n    }\n`;\n\ninterface Group {\nid: string;\nname: string;\n}\n\ninterface User {\nid: string;\nname: string;\nage: number;\n}\n\n\nexport type ChooseUserFieldProps = FieldProps&lt;\nstring,\n{\ngroupId?: string;\n} &amp; WrappedComponentProps\n&gt;;\n\nconst groupToOption = (group: Group): Option =&gt; {\nreturn {\nvalue: group.id,\nlabel: `${group.id.substring(0, 8)} ${group.name}`,\n};\n}\n\nconst userToOption = (user: User): Option =&gt; {\nreturn {\nvalue: user.id,\nlabel: `${user.name} (${user.age})`,\n};\n}\n\ndeclare module \"uniforms\" {\ninterface FilterDOMProps {\ngroupId: never;\n}\n}\nfilterDOMProps.register(\"groupId\");\n\nfunction ChoosePerson({\nid,\nname,\nlabel,\ndescription,\nonChange,\nvalue,\ndisabled,\nplaceholder,\nreadOnly,\nerror,\nshowInlineError,\nerrorMessage,\ngroupId,\nintl,\n...props\n}: ChooseUserFieldProps) {\nconst { apiClient, customApiClient, theme } = useContext(ApplicationContext);\n\nconst [groups, setGroups] = useState&lt;Group[]&gt;([]);\nconst [selectedGroupId, setGroupId] = useState&lt;number | string | undefined&gt;(groupId);\nconst [users, setUsers] = useState&lt;User[]&gt;([]);\nconst [loading, setLoading] = useState(true);\n\nconst onChangeGroup = useCallback(\n(option: SingleValue&lt;Option&gt;) =&gt; {\nlet value = option?.value;\nif (value === undefined) return;\n\nsetLoading(true);\nsetGroupId(value);\nsetUsers([]);\n\n// do api call to get users by group id and set users with the fetched data.\nsetTimeout(() =&gt; {\nlet users = [{ id: \"user_id_1\", name: \"user 1\", age: 25 }, { id: \"user_id_2\", name: \"user 2\", age: 30 }]\nif (value == \"group_id_2\") {\nusers = [{ id: \"user_id_3\", name: \"user 3\", age: 35 }]\n} else if (value == \"group_id_3\") {\nusers = [{ id: \"user_id_4\", name: \"user 4\", age: 40 }, { id: \"user_id_5\", name: \"user 5\", age: 45 }]\n}\nsetUsers(users)\nsetLoading(false);\n}, 1000)\n},\n[customApiClient]\n);\n\nuseEffect(() =&gt; {\nsetLoading(true);\n\n// do api call to get groups for the first select.\n\nsetTimeout(() =&gt; {\nsetGroups([\n{ id: \"group_id_1\", name: \"group 1\" },\n{ id: \"group_id_2\", name: \"group 2\" },\n{ id: \"group_id_3\", name: \"group 3\" }\n]);\nsetLoading(false);\n\nif (groupId) {\nonChangeGroup({ value: groupId } as Option);\n}\n}, 1000)\n}, [onChangeGroup, apiClient, groupId]);\n\n// use i18n translations.\nconst groupsPlaceholder = loading ? \"Loading...\" : \"Select a group\";\nconst userPlaceholder = loading ? \"Loading...\" : selectedGroupId ? \"Select a user\" : \"Select a group first\";\n\nconst group_options: Option[] = (groups as Group[])\n.map(groupToOption)\n.sort((x, y) =&gt; x.label.localeCompare(y.label));\nconst group_value = group_options.find((option) =&gt; option.value === selectedGroupId?.toString());\n\nconst user_options: Option&lt;string&gt;[] = users\n.map(userToOption)\n.sort((x, y) =&gt; x.label.localeCompare(y.label));\nconst user_value = user_options.find((option) =&gt; option.value === value);\n\nconst customStyles = getReactSelectTheme(theme);\n\nreturn (\n&lt;EuiFlexItem css={ChoosePersonFieldStyling}&gt;\n&lt;section {...filterDOMProps(props)}&gt;\n&lt;EuiFormRow\nlabel={label}\nlabelAppend={&lt;EuiText size=\"m\"&gt;{description}&lt;/EuiText&gt;}\nerror={showInlineError ? errorMessage : false}\nisInvalid={error}\nid={id}\nfullWidth\n&gt;\n&lt;section className=\"group-user\"&gt;\n&lt;div className=\"group-select\"&gt;\n&lt;EuiFormRow label=\"Group\" id={`${id}.group`} fullWidth&gt;\n&lt;ReactSelect&lt;Option, false&gt;\ninputId={`${id}.group.search`}\nname={`${name}.group`}\nonChange={onChangeGroup}\noptions={group_options}\nplaceholder={groupsPlaceholder}\nvalue={group_value}\nisSearchable={true}\nisDisabled={disabled || readOnly || groups.length === 0}\nstyles={customStyles}\n/&gt;\n&lt;/EuiFormRow&gt;\n&lt;/div&gt;\n&lt;div className=\"user-select\"&gt;\n&lt;EuiFormRow label=\"User\" id={id} fullWidth&gt;\n&lt;ReactSelect&lt;Option&lt;string&gt;, false&gt;\ninputId={`${id}.search`}\nname={name}\nonChange={(selected) =&gt; {\nonChange(selected?.value);\n}}\noptions={user_options}\nplaceholder={userPlaceholder}\nvalue={user_value}\nisSearchable={true}\nisDisabled={disabled || readOnly || users.length === 0}\nstyles={customStyles}\n/&gt;\n&lt;/EuiFormRow&gt;\n&lt;/div&gt;\n&lt;/section&gt;\n&lt;/EuiFormRow&gt;\n&lt;/section&gt;\n&lt;/EuiFlexItem&gt;\n);\n}\n\nexport default connectField(injectIntl(ChoosePerson), { kind: \"leaf\" });\n</code></pre>"},{"location":"architecture/application/forms/#tba","title":"TBA","text":""},{"location":"architecture/application/graphql/","title":"GraphQL documentation","text":"<p>OrchestratorCore comes with a graphql interface that can to be registered after you create your OrchestratorApp. If you add it after registering your <code>SUBSCRIPTION_MODEL_REGISTRY</code> it will automatically create graphql types for them.</p> <p>example:</p> <pre><code>app = OrchestratorCore(base_settings=AppSettings())\n# register SUBSCRIPTION_MODEL_REGISTRY\napp.register_graphql()\n</code></pre>"},{"location":"architecture/application/graphql/#extending-the-query-and-mutation","title":"Extending the Query and Mutation","text":"<p>This is an basic example of how to extend the query. You can do the same to extend Mutation.</p> <pre><code>from orchestrator.graphql import Query, Mutation\n\n\n# Queries\ndef resolve_new(info) -&gt; str:\n    return \"resolve new...\"\n\n\n@strawberry.federation.type(description=\"Orchestrator queries\")\nclass NewQuery(Query):\n    other_processes: Connection[ProcessType] = authenticated_field(\n        resolver=resolve_processes,\n        description=\"resolve_processes used for another field\",\n    )\n    new: str = strawberry.field(resolve_new, description=\"new resolver\")\n\n\napp = OrchestratorCore(base_settings=AppSettings())\n# register SUBSCRIPTION_MODEL_REGISTRY\napp.register_graphql(query=NewQuery)\n</code></pre>"},{"location":"architecture/application/graphql/#domain-models-auto-registration-for-graphql","title":"Domain Models Auto Registration for GraphQL","text":"<p>When using the <code>register_graphql()</code> function, all products in the <code>SUBSCRIPTION_MODEL_REGISTRY</code> will be automatically converted into GraphQL types. The registration process iterates through the list, starting from the deepest product block and working its way back up to the product level.</p> <p>However, there is a potential issue when dealing with a <code>ProductBlock</code> that references itself, as it leads to an error expecting the <code>ProductBlock</code> type to exist.</p> <p>Here is an example of the expected error with a self referenced <code>ProductBlock</code>:</p> <pre><code>strawberry.experimental.pydantic.exceptions.UnregisteredTypeException: Cannot find a Strawberry Type for &lt;class 'products.product_blocks.product_block_file.ProductBlock'&gt; did you forget to register it?\n</code></pre> <p>To handle this situation, you must manually create the GraphQL type for that <code>ProductBlock</code> and add it to the <code>GRAPHQL_MODELS</code> list.</p> <p>Here's an example of how to do it:</p> <pre><code># product_block_file.py\nimport strawberry\nfrom typing import Annotated\nfrom app.product_blocks import ProductBlock\nfrom orchestrator.graphql import GRAPHQL_MODELS\n\n\n# It is necessary to use pydantic type, so that other product blocks can recognize it when typing to GraphQL.\n@strawberry.experimental.pydantic.type(model=ProductBlock)\nclass ProductBlockGraphql:\n    name: strawberry.auto\n    self_reference_block: Annotated[\n        \"ProductBlockGraphql\", strawberry.lazy(\".product_block_file\")\n    ] | None = None\n    ...\n\n\n# Add the ProductBlockGraphql type to GRAPHQL_MODELS, which is used in auto-registration for already created product blocks.\nGRAPHQL_MODELS.update({\"ProductBlockGraphql\": ProductBlockGraphql})\n</code></pre> <p>By following this example, you can effectively create the necessary GraphQL type for <code>ProductBlock</code> and ensure proper registration with <code>register_graphql()</code>. This will help you avoid any <code>Cannot find a Strawberry Type</code> scenarios and enable smooth integration of domain models with GraphQL.</p>"},{"location":"architecture/application/graphql/#scalars-for-auto-registration","title":"Scalars for Auto Registration","text":"<p>When working with special types such as <code>VlanRanges</code> or <code>IPv4Interface</code> in the core module, scalar types are essential for the auto registration process. Scalar types enable smooth integration of these special types into the GraphQL schema, They need to be initialized before <code>register_graphql()</code>.</p> <p>Here's an example of how to add a new scalar:</p> <pre><code>import strawberry\nfrom typing import NewType\nfrom orchestrator.graphql import SCALAR_OVERRIDES\n\nVlanRangesType = strawberry.scalar(\n    NewType(\"VlanRangesType\", str),\n    description=\"Represent the Orchestrator VlanRanges data type\",\n    serialize=lambda v: v.to_list_of_tuples(),\n    parse_value=lambda v: v,\n)\n\n# Add the scalar to the SCALAR_OVERRIDES dictionary, with the type in the product block as the key and the scalar as the value\nSCALAR_OVERRIDES = {\n    VlanRanges: VlanRangesType,\n}\n</code></pre> <p>You can find more examples of scalar usage in the <code>orchestrator/graphql/types.py</code> file. For additional information on Scalars, please refer to the Strawberry documentation on Scalars: https://strawberry.rocks/docs/types/scalars.</p> <p>By using scalar types for auto registration, you can seamlessly incorporate special types into your GraphQL schema, making it easier to work with complex data in the Orchestrator application.</p>"},{"location":"architecture/application/graphql/#federating-with-autogenerated-types","title":"Federating with Autogenerated Types","text":"<p>To enable federation, set the <code>FEDERATION_ENABLED</code> environment variable to <code>True</code>.</p> <p>Federation allows you to federate with subscriptions using the <code>subscriptionId</code> and with product blocks inside the subscription by utilizing any property that includes <code>_id</code> in its name.</p> <p>Below is an example of a GraphQL app that extends the <code>SubscriptionInterface</code>:</p> <pre><code>from typing import Any\n\nimport strawberry\nfrom starlette.applications import Starlette\nfrom starlette.routing import Route\nfrom strawberry.asgi import GraphQL\nfrom uuid import UUID\n\n\n@strawberry.federation.interface_object(keys=[\"subscriptionId\"])\nclass SubscriptionInterface:\n    subscription_id: UUID\n    new_value: str\n\n    @classmethod\n    async def resolve_reference(cls, **data: Any) -&gt; \"SubscriptionInterface\":\n        if not (subscription_id := data.get(\"subscriptionId\")):\n            raise ValueError(\n                f\"Need 'subscriptionId' to resolve reference. Found keys: {list(data.keys())}\"\n            )\n\n        value = new_value_resolver(subscription_id)\n        return SubscriptionInterface(subscription_id=subscription_id, new_value=value)\n\n\n@strawberry.type\nclass Query:\n    hi: str = strawberry.field(resolver=lambda: \"query for other graphql\")\n\n\n# Add `SubscriptionInterface` in types array.\nschema = strawberry.federation.Schema(\n    query=Query,\n    types=[SubscriptionInterface],\n    enable_federation_2=True,\n)\n\napp = Starlette(debug=True, routes=[Route(\"/\", GraphQL(schema, graphiql=True))])\n</code></pre> <p>To run this example, execute the following command:</p> <pre><code>uvicorn app:app --port 4001 --host 0.0.0.0 --reload\n</code></pre> <p>In the <code>supergraph.yaml</code> file, you can federate the GraphQL endpoints together as shown below:</p> <pre><code>federation_version: 2\nsubgraphs:\norchestrator:\nrouting_url: https://orchestrator-graphql-endpoint\nschema:\nsubgraph_url: https://orchestrator-graphql-endpoint\nnew_graphql:\nrouting_url: http://localhost:4001\nschema:\nsubgraph_url: http://localhost:4001\n</code></pre> <p>When both GraphQL endpoints are available, you can compose the supergraph schema using the following command:</p> <pre><code>rover supergraph compose --config ./supergraph.yaml &gt; supergraph-schema.graphql\n</code></pre> <p>The command will return errors if incorrect keys or other issues are present. Then, you can run the federation with the following command:</p> <pre><code>./router --supergraph supergraph-schema.graphql\n</code></pre> <p>Now you can query the endpoint to obtain <code>newValue</code> from all subscriptions using the payload below:</p> <pre><code>{\n\"rationName\":  \"ExampleQuery\",\n\"query\": \"query ExampleQuery {\\n  subscriptions {\\n    page {\\n      newValue\\n    }\\n  }\\n}\\n\",\n\"variables\": {}\n}\n</code></pre>"},{"location":"architecture/application/graphql/#federating-with-specific-subscriptions","title":"Federating with Specific Subscriptions","text":"<p>To federate with specific subscriptions, you need to make a few changes. Here's an example of a specific subscription:</p> <pre><code># `type` instead of `interface_object` and name the class exactly the same as the one in orchestrator.\n@strawberry.federation.type(keys=[\"subscriptionId\"])\nclass YourProductSubscription:\n    subscription_id: UUID\n    new_value: str\n\n    @classmethod\n    async def resolve_reference(cls, **data: Any) -&gt; \"SubscriptionInterface\":\n        if not (subscription_id := data.get(\"subscriptionId\")):\n            raise ValueError(\n                f\"Need 'subscriptionId' to resolve reference. Found keys: {list(data.keys())}\"\n            )\n\n        value = new_value_resolver(subscription_id)\n        return SubscriptionInterface(subscription_id=subscription_id, new_value=value)\n</code></pre>"},{"location":"architecture/application/graphql/#federating-with-specific-subscription-product-blocks","title":"Federating with Specific Subscription Product Blocks","text":"<p>You can also federate a ProductBlock. In this case, the <code>subscriptionInstanceId</code> can be replaced with any product block property containing <code>Id</code>:</p> <pre><code>@strawberry.federation.interface_object(keys=[\"subscriptionInstanceId\"])\nclass YourProductBlock:\n    subscription_instance_id: UUID\n    new_value: str\n\n    @classmethod\n    async def resolve_reference(cls, **data: Any) -&gt; \"YourProductBlock\":\n        if not (subscription_id := data.get(\"subscriptionInstanceId\")):\n            raise ValueError(\n                f\"Need 'subscriptionInstanceId' to resolve reference. Found keys: {list(data.keys())}\"\n            )\n\n        value = \"new value\"\n        return YourProductBlock(subscription_id=subscription_id, new_value=\"new value\")\n</code></pre> <p>By following these examples, you can effectively federate autogenerated types (<code>subscriptions</code> and <code>product blocks</code>) enabling seamless integration across multiple GraphQL endpoints.</p>"},{"location":"architecture/application/python/","title":"Python versions","text":"<p>The orchestrator is ensured to work with multiple versions of Python 3.</p> <p>At SURF the orchestrator runs in a Docker image which makes updating the \"installed python\" as trivial as bumping the base image. But not everyone can or wants to use containers. To this end, the oldest supported Python version is chosen such that it can run on bare metal or virtual Linux servers, without the complexities and risks of compiling a newer version of Python.</p>"},{"location":"architecture/application/python/#adding-support-for-new-versions","title":"Adding support for new versions","text":"<p>We aim to add support for the latest Python 3 release within months of it becoming publicly available.</p>"},{"location":"architecture/application/python/#dropping-support-for-old-versions","title":"Dropping support for old versions","text":"<p>Our policy is to support the same version of Python 3 available in Debian-stable.</p> <p>When there is a new Debian-stable release, we will update the oldest Python version supported by orchestator-core to match it.</p>"},{"location":"architecture/application/python/#example","title":"Example","text":"<p>At the time of writing (April 2023), the latest Debian is 11 which supports Python 3.9, and the latest Python 3.x release is 3.11. Thus the supported versions are: <code>3.9 3.10 3.11</code></p> <p>Debian 12's release is currently estimated for June 2023, and it looks like it will ship with Python 3.11. That means we can reduce the supported versions to: <code>3.11</code></p> <p>Python 3.12 is scheduled to release in October 2023, which we'll then add to the supported versions: <code>3.11 3.12</code></p>"},{"location":"architecture/application/scaling/","title":"Scaling the Orchestrator","text":"<p>By default the Orchestrator is capable to handle a reasonable amount of workflows and tasks. For a larger and more distributed workload we introduced the Celery library.</p> <p>This document describes the two modes in which an Orchestrator instance can run and what you need to configure: 1. running tasks an workflows in a threadpool (default) 2. running the Orchestrator with a number of workflow workers</p>"},{"location":"architecture/application/scaling/#running-workflows-or-tasks-within-a-threadpool","title":"Running workflows or tasks within a threadpool","text":"<p>This is the default configuration. workflows and tasks are both scheduled by the same threadpool with equal priority. If you need to have tasks with a lower priority, you can for example use a scheduler and run them during a quiet period.</p> <p>In AppSettings you will notice the default:</p> <pre><code>class AppSettings(BaseSettings):\n    # fields omitted\n\n    EXECUTOR: str = \"threadpool\"\n\n    # fields omitted\n</code></pre>"},{"location":"architecture/application/scaling/#running-workflows-or-tasks-using-a-worker","title":"Running workflows or tasks using a worker","text":"<p>Celery concepts are introduced in the Documentation.</p> <p>When using Celery, the Orchestrator is split into two parts: the orchestrator-api and the orchestrator-worker.</p> <p>The orchestrator-api functionality is now limited to handling REST requests and delegating them (via one or more queues) to the orchestrator-worker. The workflows are executed in the orchestrator-worker.</p>"},{"location":"architecture/application/scaling/#queues","title":"Queues","text":"<p>Tasks and workflows are submitted on different queues. This allows for independent scaling of workers that handle low priority tasks and high priority tasks simply by letting the workers listen to different queues. Currently, there are two queues defined:</p> <ul> <li>workflows: starting or resuming workflows</li> <li>tasks: starting or resuming tasks</li> </ul> <p>By default, Redis is used for the Celery Broker and Backend. See the next chapter about implementing on how to change this behaviour.</p>"},{"location":"architecture/application/scaling/#implementing-the-worker","title":"Implementing the worker","text":"<p>The orchestrator-core needs to know what workflows a user has defined. This information is only available in the actual application using the orchestrator-core. Here we will use an example as used withing SURF. The file is called tasks.py. First we define our own class derived from the Celery base class:</p> <pre><code>class OrchestratorCelery(Celery):\n    def on_init(self) -&gt; None:\n        from orchestrator import OrchestratorCore\n\n        app = OrchestratorCore(base_settings=AppSettings())\n        init_app(app)  # This will load the workflows\n</code></pre> <p>The <code>init_app</code> should be replaced by your own function that at least makes sure that all the workflows are imported (which make sure that the are registered) so that the worker can recognize them. This is the minimum implementation you need, but you might want to add other initialisation that is needed to execute workflows.</p> <p>Next we instantiate Celery using our own <code>OrchestratorCelery</code> class:</p> <pre><code>broker = f\"redis://{AppSettings().CACHE_URI}\"\nbackend = f\"rpc://{AppSettings().CACHE_URI}/0\"\n\ncelery = OrchestratorCelery(\n    \"proj\", broker=broker, backend=backend, include=[\"orchestrator.services.tasks\"]\n)\n\ncelery.conf.update(result_expires=3600)\n</code></pre> <p>As you can see in the code above we are using Redis as broker. You can of course replace this by RabbitMQ or another broker of your choice. See the Celery documentation for more details.</p> <p><code>\"orchestrator.services.tasks\"</code> is the namespace in orchestrator-core where the Celery tasks can be found. At the moment 4 tasks are defined:</p> <ol> <li><code>tasks.new_task</code>: start a new task (delivered on the Task queue)</li> <li><code>tasks.new_workflow</code>: start a new workflow (delivered on the Workflow queue)</li> <li><code>tasks.resume_task</code>: resume an existing task (delivered on the Task queue)</li> <li><code>tasks.resume_workflow</code>: resume an existing workflow (delivered on the Workflow queue)</li> </ol> <p>Finally, we initialise the orchestrator core:</p> <pre><code>def init_celery() -&gt; None:\n    from orchestrator.services.tasks import initialise_celery\n\n    initialise_celery(celery)\n\n\n# Needed if we load this as a Celery worker because in that case, the application is not started with a user-specified top-level `__main__` module\ninit_celery()\n</code></pre> <p>The code above sets our local Celery instance (which initializes the workflows) as the celery instance that is going to be used by the orchestrator-core. Without this code, the orchestrator-core would be only aware of a limited set of workflows that are part of orchestrator-core itself.</p>"},{"location":"architecture/application/scaling/#running-locally","title":"Running locally","text":"<p>If you want to test your application locally you have to start both the orchestrator-api and one or more workers. For example:</p> <p>Start the orchestrator api:</p> <pre><code>EXECUTOR=\"celery\" uvicorn --reload --host 127.0.0.1 --port 8080 main:app\n</code></pre> <p>Notice that we are setting <code>EXECUTOR</code> to <code>celery</code>. Without that variable the api resorts to the default threadpool.</p> <p>Start a single worker that listens both on the <code>tasks</code> and <code>workflows</code> queue (indicated by the <code>-Q</code> flag):</p> <pre><code>celery -A surf.tasks  worker --loglevel=info -Q new_tasks,resume_tasks,new_workflows,resume_workflows\n</code></pre> <p>Notice that <code>-A surf.tasks</code> indicates the module that contains your 'celery.Celery' instance.</p> <p>The queues are defined in the celery config (see in services/tasks.py):</p> <pre><code>celery.conf.task_routes = {\n    NEW_TASK: {\"queue\": \"tasks\"},\n    NEW_WORKFLOW: {\"queue\": \"workflows\"},\n    RESUME_TASK: {\"queue\": \"tasks\"},\n    RESUME_WORKFLOW: {\"queue\": \"workflows\"},\n}\n</code></pre> <p>If you decide to override the queue names in this configuration, you also have to make sure that you also update the names accordingly after the <code>-Q</code> flag.</p>"},{"location":"architecture/application/tasks/","title":"Scheduling tasks in the Orchestrator","text":"<p>This document covers the moving parts needed to schedule jobs in the orchestrator.</p>"},{"location":"architecture/application/tasks/#the-task-file","title":"The task file","text":"<p>Tasks have a lot in common with regular workflows.</p>"},{"location":"architecture/application/tasks/#task-code","title":"Task code","text":"<p>The task code modules are located in <code>orchestrator/orchestrator/server/workflows/tasks/</code>. Here is a very bare-bones task file:</p> <pre><code>import time\n\nimport structlog\n\nfrom server.targets import Target\nfrom server.types import State\nfrom server.workflow import StepList, done, init, step, workflow\n\nlogger = structlog.get_logger(__name__)\n\n\n@step(\"NSO calls\")\ndef nso_calls() -&gt; State:\n    logger.info(\"Start NSO calls\", ran_at=time.time())\n    time.sleep(5)  # Do stuff\n    logger.info(\"NSO calls finished\", done_at=time.time())\n\n\n@workflow(\"Nightly sync\", target=Target.SYSTEM)\ndef task_sync_from() -&gt; StepList:\n    return init &gt;&gt; nso_calls &gt;&gt; done\n</code></pre> <p>Basically just a workflow with <code>target=Target.SYSTEM</code> - and like a workflow it will need to be registered in  <code>orchestrator/server/workflows/__init__.py</code>:</p> <pre><code># Tasks\nLazyWorkflowInstance(\".tasks.nightly_sync\", \"task_sync_from\")\n</code></pre>"},{"location":"architecture/application/tasks/#the-task-migration","title":"The task migration","text":"<p>And also like a workflow, a migration will need to introduce it to the system. It's a stripped down version of the \"subscription\" workflow migrations:</p> <pre><code>params = dict(\n    name=\"task_sync_from\",\n    target=\"SYSTEM\",\n    description=\"Nightly validate and NSO sync\",\n)\n\n\ndef upgrade() -&gt; None:\n    conn = op.get_bind()\n    conn.execute(\n        sa.text(\n\"\"\"\n            INSERT INTO workflows(name, target, description)\n                VALUES (:name, :target, :description)\n            \"\"\"\n        ),\n        params,\n    )\n    pass\n</code></pre> <p>This just needs to add an entry in the workflows table. No relations with other tables like how the workflow id gets a relation in the products table and etc.</p>"},{"location":"architecture/application/tasks/#running-the-task-in-the-ui","title":"Running the task in the UI","text":"<p>After the migration is applied, the new task will surface in the UI under the tasks tab. It can be manually executed that way. Even if the task does not have any form input, an entry will still need to be made in <code>orchestrator-client/src/locale/en.ts</code> or an error will occur.</p> <pre><code>// ESnet\ntask_sync_from: \"Verify and NSO sync\",\n</code></pre>"},{"location":"architecture/application/tasks/#the-schedule-file","title":"The schedule file","text":"<p>The schedule file is essentially the crontab associated with the task. They are located in <code>orchestrator/server/schedules/</code> - a sample schedule file:</p> <pre><code>from server.schedules.scheduling import scheduler\nfrom server.services.processes import start_process\n\n\n@scheduler(name=\"Nightly sync\", time_unit=\"minutes\", period=1)\ndef run_nightly_sync() -&gt; None:\n    start_process(\"task_sync_from\")\n</code></pre> <p>Yes this runs every minute even though it's called <code>nightly_sync</code>. There are other variations on the time units that can be used:</p> <pre><code>time_unit = \"hour\", period = 1\ntime_unit = \"hours\", period = 6\ntime_unit = \"day\", at = \"03:00\"\ntime_unit = \"day\", at = \"00:10\"\n</code></pre> <p>And similar to the task/workflow file, the schedule file will need to be registered in <code>orchestrator/server/schedules/__init__.py</code>:</p> <pre><code>from server.schedules.scheduling import SchedulingFunction\nfrom server.schedules.nightly_sync import run_nightly_sync\n\nALL_SCHEDULERS: List[SchedulingFunction] = [\n    run_nightly_sync,\n]\n</code></pre>"},{"location":"architecture/application/tasks/#executing-the-task","title":"Executing the task","text":""},{"location":"architecture/application/tasks/#manually-development","title":"Manually / development","text":"<p>When doing development, it is possible to manually make the scheduler run your task even if your Orchestrator instance is not in \"scheduler mode.\" Shell into your running instance and run the following:</p> <pre><code>docker exec -it backend /bin/bash\n./bin/scheduling force run_nightly_sync\n</code></pre> <p>Where <code>run_nightly_sync</code> is the name defined in the schedule file - not the name of the task. Not necessary to run the UI and you can get the logging output.</p>"},{"location":"architecture/application/tasks/#scheduled-execution","title":"Scheduled execution","text":"<p>The scheduler is a separate process - it isn't just a feature in the backend that gets toggled on. It is possible to run them both in a single container. It's a matter of modifying the Dockerfile to use a wrapper script to start the backend (which also runs the migrations) and then invoking the scheduler.</p> <pre><code>EXPOSE 8080\nUSER www-data:www-data\nCMD /usr/src/app/bin/server\n# Comment out the previous command and uncomment the\n# following lines to build a version that runs the\n# backend and scheduer in the same container.\n# COPY ./bin/server ./bin/server\n# COPY ./bin/scheduling ./bin/scheduling\n# COPY ./bin/wrapper ./bin/wrapper\n# CMD /usr/src/app/bin/wrapper\n</code></pre> <p>The scheduler will then run the jobs as they have been scheduled in the schedule files - and they will also be available to be run manually on an ad hoc basis in the UI.</p>"},{"location":"architecture/application/tasks/#developer-notes","title":"Developer notes","text":""},{"location":"architecture/application/tasks/#executing-multiple-tasks","title":"Executing multiple tasks","text":"<p>If one needs to execute multiple tasks in concert with each other, one can not call a task from another task. Which is to say, calling <code>start_process</code> is a \"top level\" call. Trying to call it inside an already invoked task does not work.</p> <p>But the schedule (ie: crontab) files are also code modules so one can achieve the same thing there:</p> <pre><code>@scheduler(name=\"Nightly sync\", time_unit=\"day\", at=\"00:10\")\ndef run_nightly_sync() -&gt; None:\n    subs = Subscription.query.filter(\n        Subscription.description.like(\"Node%Provisioned\")\n    ).all()\n    logger.info(\"Node schedule subs\", subs=subs)\n\n    for sub in subs:\n        sub_id = sub.subscription_id\n        logger.info(\"Validate node enrollment\", sub_id=sub_id)\n        start_process(\"validate_node_enrollment\", [{\"subscription_id\": sub_id}])\n\n    start_process(\"task_sync_from\")\n</code></pre>"},{"location":"architecture/application/websockets/","title":"Websockets","text":"<p>Orchestrator provides a websocket interface through which the frontend can receive real-time updates. This includes:</p> <ul> <li>The process overview pages</li> <li>The process detail page</li> <li>Engine status</li> </ul>"},{"location":"architecture/application/websockets/#implementation","title":"Implementation","text":"<p>To function properly in a scalable architecture, the websocket implentation consists of multiple layers,</p> <p>The main component is the <code>WebSocketManager</code> (WSM) which has the following responsibilities:</p> <ol> <li>Keep track of connected frontend clients</li> <li>Forward messages to all frontend clients</li> <li>Provide an interface to pass messages from a backend process (workflow/task)</li> </ol> <p>In a setup with multiple isolated Orchestrator instances the WSM is initialized multiple times as well, therefore clients can be connected to any arbitrary WSM instance. Letting a backend process broadcast messages to all clients thus requires a message broker, for which we use Redis Pub/Sub.</p> <p>There are 2 WSM implementations: a <code>MemoryWebsocketManager</code> for development/testing, and a <code>BroadcastWebsocketManager</code> that connects to Redis. We'll continue to discuss the latter.</p> <ul> <li><code>BroadcastWebsocketManager.broadcast_data()</code> is called by backend processes, and publishes messages to a channel in Redis [1]</li> <li><code>BroadcastWebsocketManager.sender()</code> starts in a loop for each connected client, subscribes to a channel in Redis, and forwards messages into the websocket connection</li> </ul> <p>[1] Backend processes do not call this function directly, refer to the ProcessDataBroadcastThread section</p> <p>Roughly speaking a message travels through these components: <pre><code>Process\n  -&gt; BroadcastWebsocketManager.broadcast_data()\n  -&gt; Redis channel\n  -&gt; BroadcastWebsocketManager.sender()\n  -&gt; Websocket connection\n  -&gt; Frontend client\n</code></pre></p>"},{"location":"architecture/application/websockets/#processdatabroadcastthread","title":"ProcessDataBroadcastThread","text":"<p>Backend processes are executed in a threadpool and therefore access the same WSM instance. This caused asyncio RuntimeErrors as the async Redis Pub/Sub implementation is not thread-safe.</p> <p>To solve this there is a dedicated <code>ProcessDataBroadcastThread</code> (attached to and managed by the <code>OrchestratorCore</code> app) to perform the actual <code>broadcast_data()</code> call.</p> <p>The API endpoints which start/resume/abort a process call <code>api_broadcast_process_data(request)</code> to acquire a function that can be used to submit process updates into a <code>threading.Queue</code> on which <code>ProcessDataBroadcastThread</code> listens.</p>"},{"location":"architecture/application/workflow/","title":"What is a workflow and how does it work?","text":"<p>The workflow engine is the core of the software, it has been created to execute a number of functions.</p> <ul> <li>Safely and reliable manipulate customer <code>Subscriptions</code> from one state to the next and maintain auditability.</li> <li>Create an API through which programmatically <code>Subscriptions</code> can be manipulated.</li> <li>Execute step functions in order and allow the retry of previously failed process-steps in an idempotent way.</li> <li>Atomically execute workflow functions.</li> </ul>"},{"location":"architecture/application/workflow/#create","title":"Create","text":"<p>The \"base\" workflow out of a set is the <code>CREATE</code> workflow. That will create a subscription and all of the associated workflows \"nest\" under that.</p>"},{"location":"architecture/application/workflow/#create-migration","title":"Create migration","text":"<p>The migration needs to define a specific set of parameters:</p> <pre><code>params_create = dict(\n    name=\"create_node_enrollment\",\n    target=\"CREATE\",\n    description=\"Create Node Enrollment Service\",\n    tag=\"NodeEnrollment\",\n    search_phrase=\"Node Enrollment%\",\n)\n</code></pre> <p>The <code>name</code> is the actual name of the workflow as defined in the workflow code itself:</p> <pre><code>@create_workflow(\n    \"Create Node Enrollment\",\n    initial_input_form=initial_input_form_generator,\n    status=SubscriptionLifecycle.PROVISIONING\n)\ndef create_node_enrollment() -&gt; StepList:\n    return (\n        begin\n        &gt;&gt; construct_node_enrollment_model\n        &gt;&gt; store_process_subscription(Target.CREATE)\n        ...\n        ...\n        ...\n</code></pre> <p>The <code>target</code> is <code>CREATE</code>, <code>description</code> is a human readable label and the <code>tag</code> is a specific string that will be used in all of the associated workflows.</p>"},{"location":"architecture/application/workflow/#create-flow","title":"Create flow","text":"<p>Generally the initial step will be the form generator function to display information and gather user input. The first actual step (<code>construct_node_enrollment_model</code> here) is generally one that takes data gathered in the form input step (and any data gathered from external systems, etc) and constructs the populated domain model.</p> <p>Note that at this point the subscription is created with a lifecycle state of <code>INITIAL</code>.</p> <p>The domain model is then returned as part of the <code>subscription</code> object along with any other data downstream steps might want:</p> <pre><code>@step(\"Construct Node Enrollment model\")\ndef construct_node_enrollment_model(\n    product: UUIDstr, organisation: UUIDstr, esdb_node_id: int, select_node: str, url: str, uuid: str, role_id: str\n) -&gt; State:\n    subscription = NodeEnrollmentInactive.from_product_id(\n        product_id=product, customer_id=organisation, status=SubscriptionLifecycle.INITIAL\n    )\n\n    subscription.ne.esdb_node_id = esdb_node_id\n    subscription.description = f\"Node {select_node} Initial Subscription\"\n    subscription.ne.esdb_node_uuid = uuid\n    subscription.ne.nso_service_id = uuid4()\n    subscription.ne.routing_domain = \"esnet-293\"\n\n    role = map_role(role_id, select_node)\n    site_id = select_node.split(\"-\")[0] # location short name\n\n    return {\n        \"subscription\": subscription,\n        \"subscription_id\": subscription.subscription_id,\n        \"subscription_description\": subscription.description,\n        \"role\": role,\n        \"site_id\": site_id\n    }\n</code></pre> <p>After that the the subscription is created and registered with the orchestrator:</p> <pre><code>    &gt;&gt; store_process_subscription(Target.CREATE)\n</code></pre> <p>The subsequent steps are the actual logic being executed by the workflow. It's a best practice to have each step execute one discrete operation so in case a step fails it can be restarted. To wit if a step contained:</p> <pre><code>@step(\"Do things rather than thing\")\ndef do_things(subscription: NodeEnrollmentProvisioning):\n\n    do_x()\n\n    do_y()\n\n    do_z()\n\n    return {\"subscription\": subscription}\n</code></pre> <p>And <code>do_z()</code> fails, restarting the workflow will execute the first two steps again and that might cause problems.</p> <p>The final step will make any final changes to the subscription information and change the state of the subscription to (usually) <code>PROVISIONING</code> or <code>ACTIVE</code>:</p> <pre><code>@step(\"Update subscription name with node info\")\ndef update_subscription_name_and_description(subscription: NodeEnrollmentProvisioning, select_node: str) -&gt; State:\n    subscription = change_lifecycle(subscription, SubscriptionLifecycle.PROVISIONING)\n    subscription.description = f\"Node {select_node} Provisioned (without system service)\"\n\n    return {\"subscription\": subscription}\n</code></pre> <p>No other magic really, when this step completes successfully the workflow is done and the active subscription will show up in the orchestrator UI.</p>"},{"location":"architecture/application/workflow/#associated-workflows","title":"Associated workflows","text":"<p>Now with an active subscription, the associated workflows (modify, validate, terminate, etc) \"nest\" under the active subscription in the UI. When they are executed they are run \"on\" the subscription they are associated with.</p> <p>Like the <code>CREATE</code> workflow they can have an initial form generator step but they don't necessarily need one. For example a validate workflow probably would not need any additional input since it's just running checks on an existing subscription.</p> <p>These workflows have more in common with each other than not, it's mostly a matter of how they are registered with the system.</p>"},{"location":"architecture/application/workflow/#execution-parameters","title":"Execution parameters","text":"<p>There are a few parameters to finetune workflow execution constraints. The recommended place to alter them is from the workflows module, i.e. in <code>workflows/__init__.py</code>. Refer to the examples below.</p> <ol> <li><code>WF_USABLE_MAP</code>: configure subscription lifecycles on which a workflow is usable</li> </ol> <p>By default, the associated workflow can only be run on a subscription with a lifecycle state set to <code>ACTIVE</code>. This behavior can be changed in the <code>WF_USABLE_MAP</code> data structure:</p> <pre><code>from orchestrator.services.subscriptions import WF_USABLE_MAP\n\nWF_USABLE_MAP.update(\n    {\n        \"validate_node_enrollment\": [\"active\", \"provisioning\"],\n        \"provision_node_enrollment\": [\"active\", \"provisioning\"],\n        \"modify_node_enrollment\": [\"provisioning\"],\n    }\n)\n</code></pre> <p>Now validate and provision can be run on subscriptions in either <code>ACTIVE</code> or <code>PROVISIONING</code> states and modify can only be run on subscriptions in the <code>PROVISIONING</code> state. The exception is terminate, those workflows can be run on subscriptions in any state unless constrained here.</p> <ol> <li><code>WF_BLOCKED_BY_IN_USE_BY_SUBSCRIPTIONS</code>: block modify workflows on subscriptions with unterminated <code>in_use_by</code> subscriptions</li> </ol> <p>By default, only terminate workflows are prohibited from running on subscriptions with unterminated <code>in_use_by</code> subscriptions. This behavior can be changed in the <code>WF_BLOCKED_BY_IN_USE_BY_SUBSCRIPTIONS</code> data structure:</p> <pre><code>from orchestrator.services.subscriptions import WF_BLOCKED_BY_IN_USE_BY_SUBSCRIPTIONS\n\nWF_BLOCKED_BY_IN_USE_BY_SUBSCRIPTIONS.update(\n    {\n        \"modify_node_enrollment\": True\n    }\n)\n</code></pre> <p>With this configuration, both terminate and modify will not run on subscriptions with unterminated <code>in_use_by</code> subscriptions.</p> <ol> <li><code>WF_USABLE_WHILE_OUT_OF_SYNC</code>: allow specific workflows on out of sync subscriptions</li> </ol> <p>By default, only system workflows (tasks) are allowed to run on subscriptions that are not in sync. This behavior can be changed with the <code>WF_USABLE_WHILE_OUT_OF_SYNC</code> data structure:</p> <pre><code>from orchestrator.services.subscriptions import WF_USABLE_WHILE_OUT_OF_SYNC\n\nWF_USABLE_WHILE_OUT_OF_SYNC.extend(\n    [\n        \"modify_description\"\n    ]\n)\n</code></pre> <p>Now this particular modify workflow can be run on subscriptions that are not in sync.</p> <p>Danger</p> <p>It is potentially dangerous to run workflows on subscriptions that are not in sync. Only use this for small and specific usecases, such as editing a description that is only used within orchestrator.</p>"},{"location":"architecture/application/workflow/#initial-state","title":"Initial state","text":"<p>The first step of any of these associated workflows will be to fetch the subscription from the orchestrator:</p> <pre><code>@step(\"Load initial state\")\ndef load_initial_state(subscription_id: UUIDstr) -&gt; State:\n    subscription = NodeEnrollment.from_subscription(subscription_id)\n\n    return {\n        \"subscription\": subscription,\n    }\n</code></pre> <p>The <code>subscription_id</code> is automatically passed in.</p>"},{"location":"architecture/application/workflow/#validate","title":"Validate","text":"<p>Validate workflows run integrity checks on an existing subscription. Checking the state of associated data in an external system for example. The validate migration parameters look something like this:</p> <pre><code>    params = dict(\n        name=\"validate_node_enrollment\",\n        target=\"SYSTEM\",\n        description=\"Validate Node Enrollment before production\",\n        tag=\"NodeEnrollment\",\n        search_phrase=\"Node Enrollment%\",\n    )\n</code></pre> <p>It uses a <code>target</code> of <code>SYSTEM</code> - similar to how tasks are defined. That target is more of a free form sort of thing. Same thing with the <code>name</code> - that's the name of the actual workflow, and the <code>tag</code> is shared by of this set of workflows.</p> <p>Generally the steps raise assertions if a check fails, otherwise return OK to the state:</p> <pre><code>@step(\"Check NSO\")\ndef check_nso(subscription: NodeEnrollment, node_name: str) -&gt; State:\n    device = get_device(device_name=node_name)\n\n    if device is None:\n        raise AssertionError(f\"Device not found in NSO\")\n    return {\"check_nso\": \"OK\"}\n</code></pre>"},{"location":"architecture/application/workflow/#modify","title":"Modify","text":"<p>Very similar to validate but the migration params vary as one would expect with a different <code>target</code>:</p> <pre><code>    params_modify = dict(\n        name=\"modify_node_enrollment\",\n        target=\"MODIFY\",\n        description=\"Modify Node Enrollment\",\n        tag=\"NodeEnrollment\",\n        search_phrase=\"Node Enrollment%\"\n)\n</code></pre> <p>It would make any desired changes to the existing subscription and if need by, change the lifecycle state at the end. For example, for our <code>CREATE</code> that put the initial sub into the state <code>PROVISIONING</code>, a secondary modify workflow will put it into production and then set the state to <code>ACTIVE</code> at the end:</p> <pre><code>@step(\"Activate Subscription\")\ndef update_subscription_and_description(subscription: NodeEnrollmentProvisioning, node_name: str) -&gt; State:\n    subscription = change_lifecycle(subscription, SubscriptionLifecycle.ACTIVE)\n    subscription.description = f\"Node {node_name} Production\"\n\n    return {\"subscription\": subscription}\n</code></pre> <p>These also have the subscription id passed in in the initial step as outlined above.</p>"},{"location":"architecture/application/workflow/#terminate","title":"Terminate","text":"<p>Terminates a workflow and undoes changes that were made.</p> <p>The migration params are as one would suspect:</p> <pre><code>    params = dict(\n        name=\"terminate_node_enrollment\",\n        target=\"TERMINATE\",\n        description=\"Terminate Node Enrollment subscription\",\n        tag=\"NodeEnrollment\",\n        search_phrase=\"Node Enrollment%\",\n    )\n</code></pre> <p><code>target</code> is <code>TERMINATE</code>, <code>name</code> and <code>tag</code> are as you would expect.</p> <p>The first step of these workflow are slightly different as it pulls in the <code>State</code> object rather than just the subscription id:</p> <pre><code>@step(\"Load relevant subscription information\")\ndef load_subscription_info(state: State) -&gt; FormGenerator:\n    subscription = state[\"subscription\"]\n    node = get_detailed_node(subscription[\"ne\"][\"esdb_node_id\"])\n    return {\"subscription\": subscription, \"node_name\": node.get(\"name\")}\n</code></pre>"},{"location":"architecture/application/workflow/#default-workflows","title":"Default Workflows","text":"<p>A Default Workflows mechanism is provided to provide a way for a given workflow to be automatically attached to all Products. To ensure this, modify the <code>DEFAULT_PRODUCT_WORKFLOWS</code> environment variable, and be sure to use <code>helpers.create()</code> in your migration. </p> <p>Alternatively, be sure to execute <code>ensure_default_workflows()</code> within the migration if using <code>helpers.create()</code> is not desirable.</p> <p>By default, <code>DEFAULT_PRODUCT_WORKFLOWS</code> is set to <code>['modify_note']</code>.</p>"},{"location":"architecture/product_modelling/context/","title":"Context","text":"<p>The models described further on assume an Ethernet network that consists of nodes where each node has physical ports. Network services have endpoints that connect to ports. The attributes that are specific to an endpoint are modelled as a service attach point. Examples of such attributes are the layer two label(s) used on that port or a point-to-point IP address. An inventory management system (IMS) is used to keep track of everything that is being deployed, and a network resource manager (NRM), such as NSO or Ansible, is used to provision the services on the network. All IP addresses and prefixes are stored in an IP address management (IPAM) tool.</p>"},{"location":"architecture/product_modelling/introduction/","title":"Introduction","text":"<p>Growing numbers of National Research and Education Networks (NREN) are interested in automating and orchestrating their network portfolio. However, individual NRENs may be at different levels of engagement, ranging from interested but with no concrete plans as yet, to fully automated and orchestrated. Of the many commercial and open-source tools that can be used, the NREN community\u2019s interest appears to be focused on Ansible and NSO for the automation part and on Workflow Orchestrator (WFO) for the orchestration part. Although the WFO is agnostic to the domain it is used in, this section describes, as an example that will be recognised by NREN, a set of network service products that are common to this community and can be used in combination with the Workflow Orchestrator.</p>"},{"location":"architecture/product_modelling/ip_static/","title":"IP static","text":"<p>The modelling of the IP static service is slightly more difficult. Luckily, we are again able to reuse existing product blocks and add or change attributes to meet our needs. First of all, a fixed input is used to distinguish between different types of IP services, in our case it is used to distinguish between static and BGP routing. The Ip_static_virtual_circuit product block reuses the L2_ptp_virtual_circuit product block and adds the ability to administer additional IP settings such as the use of multicast and whether a CERT filter is enabled or not. The list of service attach points is overridden, this time to reflect the fact that the IP static service only has one endpoint. The layer 3 service attach point extends the one at layer 2 and adds a list of customer prefixes, the IPv4/IPv6 MTU, and the IPv4/IPv6 point-to-point addresses used. For this example, we chose to bundle the IP settings in a separate product block to make it possible to be reused by other products, but we could also just have extended the Ip_static_virtual_circuit product block.</p> <p></p> <ul> <li>ip_routing_type: either Static or BGP, for this product set to Static</li> <li>customer_prefixes: list of IPAM ID\u2019s of the customer IP prefixes</li> <li>customer_ipv4_mtu: the customer IPv4 maximum transmission unit</li> <li>customer_ipv6_mtu: the customer IPv6 maximum transmission unit</li> <li>ptp_ipv4_ipam_id: the IPAM id of the IPv4 point-to-point prefix</li> <li>ptp_ipv6_ipam_id: the IPAM id of the IPv6 point-to-point prefix</li> <li>multicast: enable multicast</li> <li>cert_filter: enable CERT filter</li> </ul>"},{"location":"architecture/product_modelling/l2_point_to_point/","title":"L2 Point-to-Point","text":"<p>The Layer 2 point-to-point service is modelled using two product blocks. The l2_point_to_point product block holds the pointers to IMS and the NRM, the speed of the circuit, and whether the speed policer is enabled or not, as well as pointers to the two service attach points. The latter are modelled with the L2_service_attach_point product block and keep track of the port associated with that endpoint and, in the case where 802.1Q has to be enabled, the VLAN range used. The service can either be deployed protected or unprotected in the service provider network. This is administered with the fixed input protection_type.</p> <p></p> <ul> <li>protection_type: this service is either unprotected or protected</li> <li>ims_id: ID of the node in the inventory management system</li> <li>nrm_id: ID of the node in the network resource manager</li> <li>speed: the speed of the point-to-point service in Mbit/s</li> <li>speed_policer: enable the speed policer for this service</li> <li>sap: a constrained list of exactly two Layer2 service attach points</li> <li>vlan_range: range of Layer 2 labels to be used on this endpoint of the service</li> <li>port: link to the Port product block this service endpoint connects to</li> </ul>"},{"location":"architecture/product_modelling/l2_vpn/","title":"L2 VPN","text":"<p>2.4 L2 VPN</p> <p>The Layer 2 VPN service is much like the Layer 2 point-to-point service, which makes it possible to reuse existing product blocks, with a few differences such as the absence of fixed inputs. The L2_vpn_virtual_circuit product block inherits from the L2_ptp_virtual_circuit product block, and adds attributes to (dis)allow VLAN retagging and control over the BUM filter. And because a VPN can have one or more endpoints, unlike a point-to-point that has exactly two endpoints, the list of service attach points is overridden to reflect this.</p> <p></p> <ul> <li>bum_filter: enable broadcast, unknown unicast, and multicast (BUM) traffic filter</li> <li>vlan_retagging: allow VLAN retagging on endpoints</li> <li>sap: a constrained list of at least one Layer2 service attach point</li> </ul>"},{"location":"architecture/product_modelling/modelling/","title":"Modelling","text":"<p>There are several ways in which network service products can be modelled and split up into logical parts (product blocks). This may for example depend on the requirements from the stakeholders, the environment such products are used in, and/or personal taste. It is important to highlight that the modelling process aims to identify the main attributes necessary to describe a product and the relations between product blocks \u2013 it is not prescriptive in terms of design and implementation of a specific product. This approach helps to decouple the two topics and keep them in separate functional domains.</p> <p>Looking at different sets of network service product models we do see that they share a set of core attributes, regardless of which product or product block they belong to. This is not surprising, because the key attributes needed to actually provision a network service on the network are the same for all of these. This sections describes a set of network service product models and their attributes that can either be extended to meet the specific needs of the environment they will be used in, or can serve as a basis for a new constellation of product models. There is no intention to supply a complete set of products that covers all possible NREN network services, but it should be enough to help inspire thinking about (network) product modelling in your organisation.</p>"},{"location":"architecture/product_modelling/node/","title":"Node","text":"<p>The administration handoff in IMS will be different for every organisation. For this example, it is assumed that all administration that comes with the physical installation and first-time configuration of the network node in IMS is done manually by a NOC engineer. This makes the node product rather simple. The only product block that is defined holds pointers to all related information that is stored in the operations support systems (OSS). This includes of course a pointer to the information in IMS, and after the service has been deployed on the network, another pointer to the related information in the NRM. To keep track of all IP addresses and prefixes used across the network service product, the pointers to the IPv4 and IPv6 loopback addresses on the node are also stored.</p> <p></p> <ul> <li>ims_id: ID of the node in the inventory management system</li> <li>nrm_id: ID of the node in the network resource manager</li> <li>ipv4_ipam_id: ID of the node\u2019s iPv4 loopback address in IPAM</li> <li>ipv6_ipam_id: ID of the node\u2019s iPv6 loopback address in IPAM</li> </ul>"},{"location":"architecture/product_modelling/port/","title":"Port","text":"<p>Once a NOC engineer has physically installed a port in a node and added some basic administration to IMS, the port is marked as available and can be further configured through the port product. To distinguish between ports with different speeds (1Gbit/s, 10Gbit/s, etcetera), the fixed input speed is used, which also allows filtering available ports of the right speed. Besides pointers to the administration of the port in IMS and the NRM, configuration options including 802.1Q, Ethernet auto negotiation, and the use of LLDP are registered, as well as a reference to the Node the port is installed in.</p> <p></p> <ul> <li>speed: the speed of the physical interface on the node in Mbit/s</li> <li>ims_id: ID of the node in the inventory management system</li> <li>nrm_id: ID of the node in the network resource manager</li> <li>mode: the port is either untagged, tagged or a link member in an aggregate</li> <li>auto_negotiation: enable Ethernet auto negotiation</li> <li>lldp: enable the link layer discovery protocol</li> <li>node: link to the Node product block the port is residing on</li> </ul>"},{"location":"architecture/product_modelling/product_block_graph/","title":"Product Block Instance Graph","text":"<p>A subscription for a specific customer for a product that is deployed on the network is stored in the Workflow Orchestrator database. The notion of subscription ownership allows for fine-grained control over which customer is allowed to change what attribute. By correctly adding references from one product block to another, a graph of product block instances is generated that accurately reflects the relations between the snippets of network node configuration that are deployed to the network. The graph is automatically added to when a new subscription is created, allowing easy and intuitive navigation through all configuration data. Once every network service is modelled and provisioned to the network through the Workflow Orchestrator, every line of network node configuration can be linked to the corresponding subscription that holds the configuration parameters.</p> <p>The example below shows the product block instance graph for a L2 point-to-point and a L2 VPN service between three ports on three different nodes. The nodes are owned by the respective NREN\u2019s Network Operations Centre (NOC). University A has ports on nodes on two different locations, and uses a L2 point-to-point service to connect these locations. Research Institute B has one port of its own, and uses a L2 VPN service for their collaboration with the university. The business rules that describe the (optional) authorisation logic for connecting subscriptions from different customers to each other are coded in the Workflow Orchestrator workflows related to these products.</p> <p></p>"},{"location":"architecture/product_modelling/standards/","title":"Standards","text":"<p>There are many standards describing how network service products and their attributes can be modelled. Most of these are very detailed as they try to cover as many use cases as possible, which can prove overwhelming. Here we aim to do the opposite and only model the bare minimum. This makes it easier to see the relationship between the network service models, and how each model can be extended with attributes that are specific to the organisation that uses them.</p> <p>A common way of modelling products is to split the models into a customer-facing part that contains all the attributes that are significant to the customer, and a resource-facing part that extends that set of attributes with all the attributes that are needed to actually deploy a service on the network. We assume here that such a separation is being used, where the customer-facing part lives in the Workflow Orchestrator and the resource-facing part lives in a provisioning system such as NSO or Ansible.</p>"},{"location":"architecture/product_modelling/terminology/","title":"Terminology","text":"<p>The data and business rules of the products and product blocks are modelled in Workflow Orchestrator domain models. A product is a collection of one or more product blocks, and zero or more fixed inputs. Fixed inputs are customer-facing attributes that cannot be changed at will by a customer because they are constrained in some way, for example by a physical constraint such as the speed of a port or a financial constraint such as the maximum capacity of a service. Product blocks are collections of resource types (customer-facing attributes) that together describe a set of attributes that can be repeated one or more times within a product and can optionally point to other product blocks. A product block is a logical collection of resource types that taken together make reusable instances. They can be referenced many times from within other products and make it possible to build a logical topology of the network within the orchestrator database. A subscription is a product instantiation for a specific customer. See the rest of the Workflow Orchestrator documentation for more details.</p>"},{"location":"contributing/guidelines/","title":"Contributing","text":"<p>The workflow orchestrator projects welcomes any contributions from any party. If you are interested in contributing or have questions about the project please contact the board: workfloworchestrator.board@commonsconservancy.org or feel free to raise an issue in the project. We will strive to reply to your enquiry A.S.A.P.</p>"},{"location":"contributing/guidelines/#documentation","title":"Documentation","text":"<p>We use MKDOCS as a documentation tool. Please create a PR if you have any additions or contributions to make. All docs can be written in MD or html.</p>"},{"location":"contributing/guidelines/#orchestrator-release","title":"Orchestrator release","text":"<p>The <code>orchestrator-core</code> has no release schedule but is actively used and maintained by the workflow orchestrator group. Creating a new release is done by the developers of the project and the procedure is as follows.</p>"},{"location":"contributing/guidelines/#release-candidates","title":"Release candidates","text":"<p>When creating new features they can be released in so-called <code>pre-releases</code> on github. Depending on the feature type the developer will need to run <code>bumpversion (major|minor|patch)</code> and then <code>bumpversion build --allow-dirty</code> to create a new release candidate. This command will update the <code>.bumpversion.cfg</code> and the <code>orchestrator.__init__.py</code> files.</p> <p>The next step would be to \"Create a new release\" -&gt; \"Fill in the tag and check the box, create tag upon release\" and use the checkbox \"pre-release.\"</p> <p>The code will be pushed to pypi and installed in a project.</p>"},{"location":"contributing/guidelines/#official-releases","title":"Official releases.","text":"<p>Official releases follow the same procedure as described above and can be either created from a release candidate by removing the <code>-rc</code> string from the <code>.bumpversion.cfg</code> and the <code>orchestrator.__init__.py</code> files. After that a new release can be created and the <code>Autogenerate changelog</code> option may be used.</p>"},{"location":"contributing/testing/","title":"Writing unit tests","text":"<p>Notes and lessons learned about writing unit tests for workflows.</p> <p>Point the first, there are <code>test</code> and <code>test_esnet</code> file hierarchies. The latter is a clone of the former with all of the SN specific stuff torn out. Any of our stuff should go in <code>test_esnet</code> but in some cases will still reference code in the <code>test</code> hierarchy. This is covered later.</p>"},{"location":"contributing/testing/#domain-model-tests","title":"Domain model tests","text":"<p>There is a test for the circuit transition domain models here:</p> <pre><code>orchestrator/test_esnet/unit_tests/domain/product_types/test_cts.py\n</code></pre> <p>These are relatively straightforward. There are basic imports which include the domain models being tested:</p> <pre><code>from server.db import Product, db\nfrom server.domain.product_blocks.cts import CircuitTransitionBlockInactive\nfrom server.domain.product_types.cts import CircuitTransition, CircuitTransitionInactive\nfrom server.types import SubscriptionLifecycle\n</code></pre> <p>It pulls in the inactive version of the product block, plus the default and inactive product type models. Then just write functions. They seemed to have defined two kinds of test (you can put them in the same module).</p>"},{"location":"contributing/testing/#new","title":"New","text":"<p>This test just creates a new model in the function. Super easy, instantiate the inactive version of the model and test that the default fields are properly defined.</p>"},{"location":"contributing/testing/#save-and-load","title":"Save and load","text":"<p>This is commented and a bit more complex. In the <code>conftest.py</code> file at the root of the test directory a <code>pytest</code> fixture is defined that creates the model and saves it in the db.</p> <p>Then the test function loads the version from the db, checks the contents, makes some changes, save it, and load it up again.</p> <p>Also pretty simple but you mostly need to know where the fixtures get defined.</p>"},{"location":"contributing/testing/#workflow-tests","title":"Workflow tests","text":"<p>There is a unit test for the ESnet circuit transition workflow here:</p> <pre><code>orchestrator/test_esnet/unit_tests/workflows/cts/test_create_cts.py\n</code></pre> <p>It's mostly complete and is liberally commented.</p>"},{"location":"contributing/testing/#fundamental-imports","title":"Fundamental imports","text":"<p>It uses the <code>pytest</code> framework, and some custom orchestrator code. So we need to pull in some imports for the framework and functions to run the workflow in the test. Finally, the function gets a decorator:</p> <pre><code>import uuid\nimport pytest\n\nfrom server.db import Product, Subscription\nfrom test.unit_tests.workflows import (\n    assert_complete,\n    assert_failed,\n    extract_error,\n    extract_state,\n    run_workflow,\n    assert_suspended,\n    resume_workflow,\n)\n\n\n@pytest.mark.workflow\ndef test_create_cts(responses):\n    pass\n</code></pre>"},{"location":"contributing/testing/#general-flow","title":"General flow","text":"<p>To start it off, just define the initial input content in a data structure and feed it to a function that starts the workflow:</p> <pre><code>initial_state = [\n    {\"product\": str(product.product_id)},\n    {\n        \"organisation\": ESNET_ORG_UUID,\n        \"esnet5_circuit_id\": \"2\",\n        \"esnet6_circuit_id\": \"2\",\n        \"snow_ticket_assignee\": \"mgoode\",\n        \"noc_due_date\": \"2020-07-02 07:00:00\",\n    },\n]\n\nresult, process, step_log = run_workflow(\"create_circuit_transition\", initial_state)\nassert_suspended(result)\nstate = extract_state(result)\n</code></pre> <p>In this example it's a workflow suspends several times so the <code>assert_suspended</code> function is called. If the workflow doesn't have anything like that (ie: it's just one step) you can just let it go and call <code>assert_complete</code>. In the above example, you can pause, and examine the state object to make sure the contents are what is expected.</p> <p>To resume a multi-step/suspended workflow, you do this:</p> <pre><code>confirm_complete_prework = {\n    \"outbound_shipper\": \"fedex\",\n    \"return_shipper\": \"fedex\",\n    \"generate_shipping_details\": \"ACCEPTED\",\n    \"provider_receiving_ticket\": \"23432\",\n    \"provider_remote_hands_ticket\": \"345345\",\n    \"confirm_colo_and_ports\": \"ACCEPTED\",\n    \"complete_mops_info\": \"ACCEPTED\",\n    \"create_pmc_notification\": \"pmc notify\",\n    \"reserve_traffic_generator\": \"ACCEPTED\",\n}\n\nresult, step_log = resume_workflow(process, step_log, confirm_complete_prework)\n</code></pre> <p>One doesn't need to update the main state object, just create a fresh data structure of the new data and call <code>resume_workflow</code> - the new data will be added to the state object. Lather, rinse and repeat until the workflow is complete.</p>"},{"location":"contributing/testing/#http-mocking","title":"HTTP mocking","text":"<p>One non-obvious facet of the test framework is that it forces one to mock any HTTP calls going to an external service. This is defined by a fixture in the <code>conftest.py</code> file - this is compatible with the http lib we are using.</p> <p>Consider a test function prototype:</p> <pre><code>@pytest.mark.workflow\ndef test_create_cts(responses):\n    product = Product.query.filter(Product.name == \"Circuit Transition Service\").one()\n</code></pre> <p>That responses arg being passed in is the aforementioned fixture. This is then passed into your mock classes:</p> <pre><code>esdb = EsdbMocks(responses)\noauth = OAuthMocks(responses)\nsnow = SnowMocks(responses)\n</code></pre> <p><code>Product.name</code> needs to match the first argument of the <code>@create_workflow</code> decorator and needs to be defined as a product in the database or that call will fail.</p>"},{"location":"contributing/testing/#defining-a-mock","title":"Defining a mock","text":"<p>Here is the constructor and a single method from a mock file:</p> <pre><code>class OAuthMocks:\n    def __init__(self, responses):\n        self.responses = responses\n\n    def post_token(self):\n\n        response = r\"\"\"{\n  \"access_token\":\"MTQ0NjJkZmQ5OTM2NDE1ZTZjNGZmZjI3\",\n  \"token_type\":\"bearer\",\n  \"expires_in\":3600,\n  \"refresh_token\":\"IwOGYzYTlmM2YxOTQ5MGE3YmNmMDFkNTVk\",\n  \"scope\":\"create\"\n}\"\"\"\n        response = json.loads(response)\n\n        self.responses.add(\n            \"POST\",\n            f\"/oauth_token.do\",\n            body=json.dumps(response),\n            content_type=\"application/json\",\n        )\n</code></pre> <p>Pretty basic - define what the return payload looks like. One defines an HTTP verb, URI and the like.</p> <p>Also, if you're mocking a call that contains a query string make sure to include the</p> <pre><code>match_querystring = (True,)\n</code></pre> <p>flag to <code>responses.add()</code> or you'll go insane trying to figure out why it didn't register properly.</p> <p>Not going to lie, this part can get kind of tedious depending on the amount of calls you need to mock.</p>"},{"location":"contributing/testing/#registering-with-the-fixture-the-non-obvious-bit","title":"Registering with the fixture (the non-obvious bit)","text":"<p>The way this works is that rather than mimicking the name of the method being mocked, it does a look up using a two-tuple of the verb and the uri. And it needs to be registered with the fixture or else the lookup won't work. So back in the test function, one needs to do this before initiating the workflow:</p> <pre><code>oauth = OAuthMocks(responses)\n...\ntoken = oauth.post_token()\n</code></pre> <p>Even though you haven't run the workflow yet, and you won't use the return value, doing that registers the verb/uri pair with the fixture. Then going forward when the code executes and there is an HTTP call to that verb/uri pair, the contents of that method will be returned (payload, headers, etc).</p> <p>And if you try to cheat, the fixture will stop you. Any un-mocked HTTP call will raise an exception.</p>"},{"location":"contributing/testing/#running-the-tests","title":"Running the tests","text":"<p>The tests need to be run inside the container. First, to enable \"live\" updating, add this to the <code>volumes</code> stanza of the docker compose file:</p> <pre><code>      - ../test:/usr/src/app/test\n</code></pre> <p>Then shell into the container:</p> <pre><code>    docker exec -it backend /bin/bash\n</code></pre> <p>And run the test:</p> <pre><code>root@d30f71ee1afe:/usr/src/app# pytest -s test_esnet/unit_tests/workflows/cts/test_create_cts.py\n</code></pre> <p>The <code>-s</code> flag to <code>pytest</code> is needed if you want to see your print statements. Otherwise <code>pytest</code> will cheerfully eat them.</p>"},{"location":"contributing/testing/#gotchas-and-etc","title":"Gotchas and etc","text":""},{"location":"contributing/testing/#executing-multiple-tasks","title":"Executing multiple tasks","text":"<p>The <code>test_esnet</code> tree is a clone of the SN <code>test</code> tree with all of the SN specific stuff removed. Some tests may still reference code in the <code>test</code> tree - utility testing code for example:</p> <pre><code>from test.unit_tests.workflows import (\n    assert_complete,\n    assert_failed,\n    extract_error,\n    extract_state,\n    run_workflow,\n    assert_suspended,\n    resume_workflow,\n)\n</code></pre> <p>That's by design - those things are core orchestrator code so it stays put.</p> <p>At some point we might want to crib off of SN code or modify it (like some of the mocking code for example) - if so, go ahead and move it into our tree. The goal of this is to have the <code>test_esnet</code> tree be pretty lean and just have our stuff in it. That way we can also just run the entire tree w/out worrying about their stuff.</p>"},{"location":"contributing/testing/#test-db","title":"Test DB","text":"<p>See <code>.env.example</code> on how to set the URI for the database the testing framework uses. The original default was to use your \"production\" local db which had the super helpful side effect of trashing your orchestrator state.</p>"},{"location":"contributing/testing/#initial-state","title":"Initial state","text":"<p>The initial state for the form input is defined in a pretty straightforward way - at least for create workflows:</p> <pre><code>initial_state = [\n    {\"product\": str(product.product_id)},\n    {\n        \"organisation\": ESNET_ORG_UUID,\n        \"esnet5_circuit_id\": \"2\",\n        \"esnet6_circuit_id\": \"2\",\n        \"snow_ticket_assignee\": \"mgoode\",\n        \"noc_due_date\": \"2020-07-02 07:00:00\",\n    },\n]\n\nresult, process, step_log = run_workflow(\"create_circuit_transition\", initial_state)\n</code></pre> <p>But there seems to be a gotcha when defining initial state for a terminate / etc workflow that modifies existing subscriptions:</p> <pre><code># Yes, the initial state is a list of two identical dicts.\n# Why? I don't know. But I do know if you don't do this an\n# maddening form incomplete validation error will happen. -mmg\ninitial_state = [\n    {\"subscription_id\": nes_subscription2},\n    {\n        \"subscription_id\": nes_subscription2,\n    },\n]\n\nresult, process, step_log = run_workflow(\"terminate_node_enrollment\", initial_state)\n</code></pre> <p>So if one gets a vague form validation error when doing this, it might be something alone these lines.</p>"},{"location":"contributing/testing/#insync-true","title":"insync = True","text":"<p>When defining a fixture in <code>conftest.py</code> to make an entry in the testing DB for a subscription that a unit test might consume, make sure to mark the subscription object <code>.insync = True</code>. Otherwise the unit test will fail thinking that it is attached to an active process.</p>"},{"location":"getting-started/base/","title":"Bare application","text":"<p>By following these steps you can start a bare orchestrator-core application that can be used to run workflows.</p> <p>Note</p> <p>The Orchestrator-core is designed to be installed and extended just like a FastAPI or Flask application. For more information about how this works read the Architecture sections.</p>"},{"location":"getting-started/base/#step-1-install-the-package","title":"Step 1 - Install the package:","text":"<p>Install the core.</p> <pre><code>$ pip install orchestrator-core\n---&gt; 100%\nSuccessfully installed orchestrator-core\n</code></pre>"},{"location":"getting-started/base/#step-2-setup-the-database","title":"Step 2 - Setup the database:","text":"<p>Create a postgres database:</p> <pre><code>$ createuser -sP nwa\n$ createdb orchestrator-core -O nwa\n</code></pre>"},{"location":"getting-started/base/#step-3-create-the-mainpy","title":"Step 3 - Create the main.py:","text":"<p>Create a <code>main.py</code> file.</p> <pre><code>from orchestrator import OrchestratorCore\nfrom orchestrator.cli.main import app as core_cli\nfrom orchestrator.settings import AppSettings\n\napp = OrchestratorCore(base_settings=AppSettings())\n\nif __name__ == \"__main__\":\n    core_cli()\n</code></pre>"},{"location":"getting-started/base/#step-4-run-the-database-migrations","title":"Step 4 - Run the database migrations:","text":"<p>Initialize the migration environment.</p> <pre><code>$ PYTHONPATH=. python main.py db init\n$ PYTHONPATH=. python main.py db upgrade heads\n</code></pre>"},{"location":"getting-started/base/#step-5-run-the-app","title":"Step 5 - Run the app","text":"<pre><code>$ uvicorn --reload --host 127.0.0.1 --port 8080 main:app\nINFO:     Uvicorn running on http://127.0.0.1:8080 (Press CTRL+C to quit)\nINFO:     Started reloader process [62967] using watchgod\nujson module not found, using json\nmsgpack not installed, MsgPackSerializer unavailable\n2021-09-28 09:42:14 [warning  ] Database object configured, all methods referencing `db` should work. [orchestrator.db]\nINFO:     Started server process [62971]\n2021-09-28 09:42:14 [info     ] Started server process [62971] [uvicorn.error]\nINFO:     Waiting for application startup.\n2021-09-28 09:42:14 [info     ] Waiting for application startup. [uvicorn.error]\nINFO:     Application startup complete.\n2021-09-28 09:42:14 [info     ] Application startup complete.  [uvicorn.error]\n</code></pre>"},{"location":"getting-started/base/#step-6-profit","title":"Step 6 - Profit","text":"<p>Visit the app to view the api documentation.</p>"},{"location":"getting-started/development/","title":"Setting up a development environment","text":"<p>To add features to the repository follow the following procedure to setup a working development environment.</p>"},{"location":"getting-started/development/#installation-development","title":"Installation (Development)","text":"<p>Install the project and its dependencies to develop on the code.</p>"},{"location":"getting-started/development/#step-1-install-flit","title":"Step 1 - install flit:","text":"<pre><code>pip install flit\n</code></pre>"},{"location":"getting-started/development/#step-2-install-the-development-code","title":"Step 2 - install the development code:","text":"<pre><code>flit install --deps develop --symlink\n</code></pre> <p>Danger</p> <p>Make sure to use the flit binary that is installed in your environment. You can check the correct path by running <pre><code>which flit\n</code></pre></p>"},{"location":"getting-started/development/#running-tests","title":"Running tests","text":"<p>Run the unit-test suite to verify a correct setup.</p>"},{"location":"getting-started/development/#step-1-create-a-database","title":"Step 1 - Create a database","text":"<pre><code>createuser -sP nwa\ncreatedb orchestrator-core-test -O nwa\n</code></pre>"},{"location":"getting-started/development/#step-2-run-tests","title":"Step 2 - Run tests","text":"<pre><code>pytest test/unit_tests\n</code></pre> <p>If you do not encounter any failures in the test, you should be able to develop features in the orchestrator-core.</p>"},{"location":"workshops/advanced/circuit-workflow/","title":"Circuit Workflow","text":"<p>The <code>create_circuit</code> workflow we outlined in the workflow overview section is implemented with the following StepList:</p> <pre><code>begin\n&gt;&gt; construct_circuit_model\n&gt;&gt; store_process_subscription(Target.CREATE)\n&gt;&gt; reserve_ips_in_ipam\n&gt;&gt; set_status(SubscriptionLifecycle.PROVISIONING)\n&gt;&gt; create_circuit_in_netbox\n&gt;&gt; update_circuit_description\n&gt;&gt; update_circuit_in_netbox\n&gt;&gt; provide_config_to_user\n&gt;&gt; set_circuit_in_service\n&gt;&gt; update_circuit_in_netbox\n</code></pre> <p>To see the actual step code, go to <code>workflows/circuit/circuit_create.py</code>. covering the implementation steps of these details is generally out of scope for this workshop, they simply exist to give us a framework to build off of. It's mainly important to understand the general flow of these steps and get a feel for how we are populating the Circuit domain model.</p> <p>Once you have familiarized yourself with the circuit create code, go ahead and try running it! To run it, you need to first enroll at least two nodes, and then you can proceed with setting up a circuit. Take note at the User Input step what the ISIS routing metric is (<code>15000000</code>). This is how a network engineer might set a circuit into a maintenance mode, so our workflow sets this by default with no way to change it. This is fine for the initial provisioning for a circuit, but we need to consider the full lifecycle and how it will be turned into production eventually, which leads us to the root of this exercise. Take extra note of how we are setting that metric to <code>15000000</code> in the shared circuit file, in the function <code>determine_isis_metric()</code>:</p> <pre><code>def determine_isis_metric(under_maintenance: bool) -&gt; int:\n\"\"\"\n    set_isis_metric determines the ISIS metric to use depending on the\n    maintenance state of the circuit.\n\n    Args:\n        under_maintenance (bool): If the circuit is under the maintenance (True)\n        or not (False)\n\n    Returns:\n        int: The ISIS routing metric.\n    \"\"\"\n    if under_maintenance:\n        isis_metric = 15000000\n    else:\n        isis_metric = 10\n\n    return isis_metric\n</code></pre> <p>As you can see in the above function, if the circuit is not under maintenance, we should be setting the <code>isis_metric</code> to <code>10</code>.</p> <p>If you dig deeper, you can see that this function is called in the <code>provide_config_to_user()</code> function where we generate the config that a network engineer would paste into a router, specifically, we pass the subscription field <code>subscription.circuit.under_maintenance</code> (think back to the domain model!) into the <code>determine_isis_metric()</code> function, then we render the config for both sides of the circuit, like so:</p> <pre><code>isis_metric = determine_isis_metric(subscription.circuit.under_maintenance)\n\nrouter_a_config = render_circuit_endpoint_config(\n    node=subscription.circuit.members[0].port.node.node_name,\n    interface=subscription.circuit.members[0].port.port_name,\n    description=subscription.circuit.members[0].port.port_description,\n    address=subscription.circuit.members[0].v6_ip_address,\n    isis_metric=isis_metric,\n)\nrouter_b_config = render_circuit_endpoint_config(\n    node=subscription.circuit.members[1].port.node.node_name,\n    interface=subscription.circuit.members[1].port.port_name,\n    description=subscription.circuit.members[1].port.port_description,\n    address=subscription.circuit.members[1].v6_ip_address,\n    isis_metric=isis_metric,\n)\n</code></pre> <p>Since we already have a modify workflow defined, go ahead and try running it to see if it changes the metric like you would expect. If everything were working, the <code>under_maintenance</code> boolean in the subscription would be set to <code>False</code> and the metric in the configuration would be set to <code>10</code>.</p> <p>To run the modify workflow, navigate to a circuit subscription instance in the orchestrator GUI (create one if you haven't already), click the <code>Actions</code> tab, and then click on <code>Modify the circuit maintenance state</code>. When you run this workflow, you can see that it pulls in the current state of the <code>under_maintenance</code> flag and displays that to the user. If the user wishes, they can then change that and click submit. At this point, you, the astute developer you are, will notice that nothing actually happens, and the subscription domain model still has <code>under_maintenance</code> set to <code>True</code>. You also notice that the config to make the change was actually never displayed to the user. Let's fix that!</p>"},{"location":"workshops/advanced/circuit-workflow/#exercise-1-updating-the-domain-model-via-the-modify-workflow","title":"Exercise 1: Updating the Domain Model via the Modify Workflow","text":"<p>The first step in fixing up our modify workflow is to make sure that we update the domain model with the input provided by the user. To do this, we need to fill out the step called <code>modify()</code>. Currently, it looks quite bleak:</p> <pre><code>@step(\"Modify\")\ndef modify(subscription: Circuit) -&gt; State:\n    logger.debug(\"This is the subscription\", subscription=subscription, type=type(subscription))\n    # DO SOMETHING\n    return {}\n</code></pre> <p>We can see that the <code>modify_initial_input_form_generator()</code> step puts the form <code>user_input</code> onto the state as a dictionary, so we should just be able to access every field from the form directly from the <code>modify()</code> step by name if we add it as an argument to <code>modify()</code> (using the power of the <code>inject_args</code> helper provided by the <code>@step</code> decorator.) Once you have the value that the user provides in your function, go ahead and try to populate the subscription with that value. For an existing example of this, look at the node workflow to see how the input values are handled.</p> <p>Try to go ahead and implement this on your own, however, if you get stuck, here is a working implementation:</p> Example <pre><code>@step(\"Modify\")\ndef modify(subscription: Circuit, under_maintenance: bool) -&gt; State:\n    logger.debug(\n        \"Changing circuit maintenance state\",\n        subscription=subscription,\n        type=type(subscription),\n        old_value=subscription.circuit.under_maintenance,\n        new_value=under_maintenance,\n    )\n    # Set the subscription under_maintenance value to what the user input.\n    subscription.circuit.under_maintenance = under_maintenance\n\n    return {\"subscription\": subscription}\n</code></pre> <p>Once you have made your implementation, save the file, and the orchestrator backend will hot-reload. Run a new modify workflow on a circuit. Once you've run the workflow, you should now be able to go into the subscription data tab and see the new value for the under_maintenance field. If you get really fancy with it, click on the <code>Delta</code> button to see the exact changes to the subscription. If everything was done right, you should see the following change to the subscription:</p> <pre><code>{\n\"circuit\": {\n\"under_maintenance\": false\n}\n}\n</code></pre>"},{"location":"workshops/advanced/circuit-workflow/#exercise-2-updating-the-configuration-based-off-the-domain-model","title":"Exercise 2: Updating the Configuration Based Off the Domain Model","text":"<p>Now that we are actually updating the orchestrator's view of things, we need to go ahead and make sure that our intent is actually applied to the network! Using our simple copy/paste method of applying network intent, this should be a fairly easy fix.</p> <p>First things first, think back to how we are providing the config that we display to the user in the <code>provide_config_to_user()</code> step function. We are simply pulling data out of the subscription domain model and the populating strings with the values. Assuming that the CLI we are using is idempotent, we can simply re-apply all that config with the new values and the device config will be updated.</p> <p>Warning</p> <p>Since this is just an example workshop, this will work for us. In production, you will want to use a much more robust configuration mediation engine, especially in a multi-vendor network, however, for the purposes of this workshop, copy/pasting config will suffice. Additionally, you might want to save values like the isis_metric to the domain model and create a single dispatch service like the netbox example used in this workshop.</p> <p>With this in mind, all we really need to do is take the <code>provide_config_to_user()</code> step from shared, import it into our modify workflow/add it to the steplist and be off to the races!</p> <p>Go ahead and try to do this on your own, however, if you get stuck, here is a working implementation:</p> Example <p>First, go ahead and import the step from the shared file, like so: <pre><code>from workflows.circuit.shared import provide_config_to_user\n</code></pre></p> <p>Now go ahead and add that to the steplist so that your steplist looks like this:</p> <pre><code>def modify_circuit() -&gt; StepList:\n    return (\n        begin\n        &gt;&gt; set_status(SubscriptionLifecycle.PROVISIONING)\n        &gt;&gt; modify\n        &gt;&gt; provide_config_to_user\n        &gt;&gt; set_status(SubscriptionLifecycle.ACTIVE)\n    )\n</code></pre> <p>Once you have made your implementation, save the file, and the orchestrator backend will hot-reload. Run a new modify workflow on a circuit. Once you've run the workflow and have changed the boolean flag for the maintenance state, you will be presented with a fresh set of config to be applied to the network device.</p> <p>Congratulations on fixing this modify workflow!</p>"},{"location":"workshops/advanced/database-migration/","title":"Database Migrations","text":"<p>For the purposes of this workshop, you don't need to mess at all with database migrations (woohoo!). If you are curious about these, please refer to the beginner workshop section on this topic.</p>"},{"location":"workshops/advanced/docker-installation/","title":"Docker Compose Installation Instructions","text":"<p>Here is how you can run the orchestrator-core, orchestrator-core-gui, and netbox with Docker Compose. We have this all setup in our docker-compose.yml file so that you don't have to think about how to start the applications required for this workshop! If you want to read more about how to manually install the Workflow Orchestrator, please refer to the beginner workshop here. The following Docker images are used in this workshop:</p> <ul> <li>orchestrator-core: The workflow orchestrator step engine.</li> <li>orchestrator-core-gui: The GUI for the orchestrator-core.</li> <li>netbox: A free IPAM and SoT system.</li> <li>postgres: The PostgreSQL object-relational database system.</li> <li>redis: An open source, in-memory data store used by netbox</li> </ul>"},{"location":"workshops/advanced/docker-installation/#step-1-prepare-environment","title":"Step 1 - Prepare environment","text":"<p>Ensure that you have docker and docker compose installed on your system. We won't go into deep details on how to do this as we expect you to have the knowledge to provide a working docker setup for this workshop. To make sure that docker is setup properly, run the following checks:</p> <p>First, let's make sure that docker is installed:</p> <pre><code>jlpicard@ncc-1701-d:~$ docker --version\nDocker version 23.0.1, build a5ee5b1dfc\n</code></pre> <p>In this case, we see that version 23.0.1 is installed, which is plenty new enough for this workshop. Any version of docker later than <code>19.03.0</code> should work for this.</p> <p>Next, let's make sure that we have Docker Compose v2 setup on our machine:</p> <pre><code>jlpicard@ncc-1701-d:~$ docker compose version\nDocker Compose version v2.17.2\n</code></pre> <p>Tip</p> <p>If this command does not work and produce a similar output, follow the official Docker guide on installing the Docker Compose v2 plugin.</p>"},{"location":"workshops/advanced/docker-installation/#step-2-start-environment","title":"Step 2 - Start environment","text":"<p>Docker compose will take care of all necessary initialization and startup of the database, orchestrator and GUI:</p> <ol> <li>A postgres container, holding the databases for netbox and the orchestrator</li> <li>A redis container used by netbox.</li> <li>A set of containers spun up by netbox.</li> <li>An orchestrator backend container that runs off main.py</li> <li>Finally, a GUI frontend container is started.</li> </ol> <p>To start all of this, simply clone the repo:</p> <pre><code>jlpicard@ncc-1701-d:~$ git clone git@github.com:workfloworchestrator/example-orchestrator-tnc.git\n</code></pre> <p>and then start the containers!</p> <pre><code>jlpicard@ncc-1701-d:~$ docker compose up -d\n</code></pre>"},{"location":"workshops/advanced/docker-installation/#step-3-open-a-browser","title":"Step 3 - Open a browser","text":"<p>Now point a web browser to <code>http://localhost:3000/</code> and have a look around. This is a functional orchestrator instance and represents an environment where you can perform the exercises that are part of this workshop.</p> <p>Tip</p> <p>Once opened in the browser, ignore the message about the CRM not being responsive. This workshop does not include the setup of an interface to a CRM, fake customers IDs will be used instead.</p>"},{"location":"workshops/advanced/docker-installation/#helpful-items","title":"Helpful Items","text":""},{"location":"workshops/advanced/docker-installation/#resetting-your-environment","title":"Resetting Your Environment","text":"<p>To reset the active state of your environment back to scratch, simply use docker compose to delete volumes, like so:</p> <pre><code>jlpicard@ncc-1701-d:~$ docker compose down -v </code></pre> <p>You can then restart the containers as described above.</p>"},{"location":"workshops/advanced/docker-installation/#accessing-netbox","title":"Accessing Netbox","text":"<p>Netbox can be accessed for troubleshooting and verifying that everything you have done in the workflow is working properly by pointing your web browser to <code>http://localhost:8000</code>. From there, you can login with <code>admin/admin</code>.</p>"},{"location":"workshops/advanced/domain-models/","title":"Domain models","text":""},{"location":"workshops/advanced/domain-models/#introduction","title":"Introduction","text":"<p>First read the Architecture; TL;DR section of the orchestrator core documentation to get an overview of the concepts that will be covered.</p> <p>To put a part of the terminology in context, products are modeled using a set of product blocks. The product attributes are modeled by resource types.  By default all resource types are mutable and can be changed over the lifetime of a subscription. Fixed inputs are used to model immutable attributes.</p> <p>An example of an immutable attribute is for example the speed of a circuit, which are physical properties of the interfaces, and cannot be changed without a field engineer swapping the interface with one with a different speed. Another example is an attribute that is linked to the price of a product, for example the greater the capacity of a product, the higher the price. A customer is not allowed to increase the capacity themselves, they must pay extra first.</p> <p>The products and product blocks for this workshop will be modeled as follows:</p> <ul> <li>product <code>Node</code><ul> <li>product block <code>NodeBlock</code><ul> <li>resource type <code>node_id</code></li> <li>resource type <code>node_named</code></li> <li>resource type <code>ipv4_loopback</code></li> <li>resource type <code>ipv6_loopback</code></li> </ul> </li> </ul> </li> <li>product <code>Circuit</code><ul> <li>fixed input <code>speed</code>:</li> <li>product block <code>CircuitBlock</code><ul> <li>resource type <code>circuit_id</code></li> <li>resource type <code>circuit_description</code></li> <li>resource type <code>under_maintenance</code></li> <li>restricted product block reference <code>members</code> (<code>PortPair</code>, which restricts to exactly 2 <code>Layer3Interface</code> instances)<ul> <li>product block <code>Layer3Interface</code><ul> <li>resource type <code>v6_ip_address</code></li> <li>product block reference <code>port</code> (<code>Port</code>)<ul> <li>resource type <code>port_id</code></li> <li>resource type <code>port_description</code></li> <li>resource type <code>port_name</code></li> <li>product block reference <code>node</code> (<code>NodeBlock</code>)<ul> <li>See <code>NodeBlock</code> from the above <code>Node</code> product.</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>As you can see, a product can be seen as a container for fixed inputs and at least one references to a product block. A product block is a container for resources types and (optional) references to other product blocks. Finally, you can see that product block references may be nested as deep as needed and can be used by multiple products.</p> <p>For more information on product blocks, as well as some exercises for how to write these from scratch, please refer to this section of the beginner workshop..</p>"},{"location":"workshops/advanced/node-workflow/","title":"Node Workflow","text":""},{"location":"workshops/advanced/node-workflow/#exercise-1-update-netbox-status","title":"Exercise 1: Update Netbox Status","text":"<p>The <code>create_node</code> workflow we outlined in the workflow overview section is implemented with the following StepList:</p> <pre><code>begin\n&gt;&gt; construct_node_model\n&gt;&gt; store_process_subscription(Target.CREATE)\n&gt;&gt; fetch_ip_address_information\n&gt;&gt; set_status(SubscriptionLifecycle.PROVISIONING)\n&gt;&gt; provide_config_to_user\n&gt;&gt; set_node_to_active\n&gt;&gt; update_node_in_netbox\n</code></pre> <p>To see the actual step code, go to <code>workflows/node/node_create.py</code>. Covering the implementation steps of these details is generally out of scope for this workshop, they simply exist to give us a framework to build off of. The important concept to understand is that we are going to fill out the currently empty <code>set_node_to_active</code> step to actually perform the functions we need done in netbox. Most of the heavy lifting with netbox is performed in the netbox single dispatch service we've included with the workshop code at <code>services/netbox.py</code> and <code>products/services/netbox/netbox.py</code>. To keep things simple, we won't cover how this single dispatch service works, but rather, just use it. Generally, to use this service, you pass the subscription instance that you want to update in netbox to the <code>build_payload()</code> function and netbox will be updated for you, as shown in the last step of the node create workflow:</p> <pre><code>@step(\"Update Node in Netbox\")\ndef update_node_in_netbox(subscription: NodeProvisioning) -&gt; State:\n\"\"\"Updates a node in Netbox\"\"\"\n    netbox_payload = build_payload(subscription.node, subscription)\n    return {\"netbox_payload\": netbox_payload.dict(), \"netbox_updated\": netbox.update(netbox_payload)}\n</code></pre>"},{"location":"workshops/advanced/node-workflow/#why-update-node-status-externally","title":"Why Update Node Status Externally?","text":"<p>The goal of the orchestrator is generally not to store data in the orchestrator's DB, but rather, to store pointers to data in external systems and keep track of those data. With the way that <code>create_node</code> is currently implemented, we have a bug where it is possible to enroll the same node multiple times, due to how we are filtering our list of nodes. To see this bug, take a look at the first few lines of the <code>initial_input_form_generator</code> function in <code>create_node.py</code>, copied here for your convenience:</p> <pre><code>def initial_input_form_generator(product_name: str) -&gt; FormGenerator:\n\"\"\"Generates the Node Form to display to the user.\"\"\"\n    logger.debug(\"Generating initial input form\")\n    devices = netbox.get_devices(status=\"planned\")\n    choices = [device.name for device in devices]\n    DeviceEnum = Choice(\"Planned devices\", zip(choices, choices))\n</code></pre> <p>The important part of this function here is <code>devices = netbox.get_devices(status=\"planned\")</code>, where we fetch all devices in netbox that are in the <code>planned</code> state using the netbox service and then we construct the rest of the initial input form to present a list of available nodes to run this workflow against. Since we don't have anything in the workflow that then ever changes that status in netbox from planned to a different state, there is nothing stopping us from enrolling the same router over and over again, which is of course not desirable.</p>"},{"location":"workshops/advanced/node-workflow/#implement-the-update-step","title":"Implement The Update Step","text":"<p>The example netbox service provided with this workshop code is very convenient for updating data in netbox via the orchestrator. The general way this works, is that we have pre-written dataclasses that match the expected netbox API payloads, and made an orchestrator service that you can pass the subscription object to. Once the netbox service receives the subscription object, all you need to do is update the domain model with the right value, and then the code in the <code>update_node_in_netbox</code> step of the node create workflow will call the netbox service and update things appropriately in that system. As mentioned above, using this single dispatch netbox client, we simply use the <code>netbox.build_payload()</code> method, and pass in a domain model to update netbox. This works very cleanly, as single dispatch looks at the type hint of the domain model to determine how to send the payload. Note in the examples given in the code that we send both the specific product block that we want updated in the external system as well as the overall subscription so that the netbox service can use metadata from the subscription, such as general subscription attributes or information from other product blocks that are part of this subscription when constructing the payload.</p> <p>To take advantage of this netbox service to implement the update step, all you need to do is the following in the <code>set_node_to_active</code> step of the node workflow:</p> <ol> <li>Modify the subscription object so that you set the <code>node_status</code> domain model field to active</li> <li>Return the subscription object so that the subscription is saved in the database, thus completing updating the domain model.</li> </ol> <p>For a hint, look at lines 60-63 of <code>workflows/node/node_create.py</code> to see how to update the domain model for this subscription\u2014Try to do this on your own, however, if you get stuck, here is a working implementation:</p> Example <pre><code>@step(\"Set Node to active\")\ndef set_node_to_active(subscription: NodeProvisioning) -&gt; State:\n\"\"\"Updates a node to be Active\"\"\"\n    subscription.node.node_status = \"active\"\n    return {\"subscription\": subscription}\n</code></pre> <p>Once you have made your implementation, save the file, and the orchestrator backend will hot-reload. Run a new node create workflow on a node. Once you've run the workflow, you should now be able to go into netbox and see nodes set to active, like so:</p> <p></p> <p>Warning</p> <p>Keep in mind that This won't go back and fix the nodes that have been enrolled before we implemented this fix. In a production deployment you would need to go and fix this data manually or via some scripting, however, in this scenario, we can simply reset our environment to a blank slate like so:</p> <pre><code>jlpicard@ncc-1701-d:~$ docker compose down -v &amp;&amp; docker compose up -d\n</code></pre>"},{"location":"workshops/advanced/overview/","title":"Advanced Workshop Overview","text":""},{"location":"workshops/advanced/overview/#intended-audience","title":"Intended audience","text":"<p>This workshop is intended for those who have run through the beginner Workflow Orchestrator workshop, but is also accessible to those who are new to the Workflow Orchestrator. The main goal of this workshop is to introduce you to how to write orchestrator workflows that talk to external systems, as well as teaching you how to relate products to other products, using the dependency model of the Workflow Orchestrator.</p> <p>Tip</p> <p>Knowledge of the Python programming language, Docker, and the Unix command line interface are prerequisites for this workshop.</p>"},{"location":"workshops/advanced/overview/#topics","title":"Topics","text":"<ul> <li>Installation   Detailed instructions are given on how to prepare your environment and install the orchestrator and GUI using docker compose.</li> <li>Start applications   Outline how to start the Workflow Orchestrator backend and GUI using docker compose.</li> <li>Create Node and Circuit Product   Through a simple network node and network circuit scenario, a set of products is created showing how domain models are defined.</li> <li>Domain models     Explains the benefits of the use of domain models and shows how the hierarchy of products, product blocks, fixed inputs and resource types are used to create product subscriptions for customers.</li> <li>Database migration     Use the orchestrator CLI to create an Alembic database migration based on the domain models that describe the created products and product blocks.</li> <li>Create Node and Circuit Workflows   For the Node and Circuit products, we will make CREATE workflows. The use of input forms is explained as part of defining the create workflow. The Node product will introduce connecting to an external system from within a workflow, and then the Circuit workflow will build upon the existing Node subscriptions to highlight the ability to link products in the Workflow Orchestrator as dependencies. The circuit workflow will also demonstrate using the forms library to create complex objects in external systems.</li> </ul>"},{"location":"workshops/advanced/overview/#workshop-folder-layout","title":"Workshop folder layout","text":"<p>This workshop uses the following folder layout:</p> <pre><code>.\n\u251c\u2500\u2500 docker-compose.yml\n\u251c\u2500\u2500 docs\n\u2502   \u251c\u2500\u2500 advanced_workshop.png\n\u2502   \u2514\u2500\u2500 orch_advanced_workshop_architecture.png\n\u251c\u2500\u2500 etc\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 main.py\n\u251c\u2500\u2500 migrations\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 products\n\u2502   \u251c\u2500\u2500 product_blocks\n\u2502   \u2502   \u251c\u2500\u2500 circuit.py\n\u2502   \u2502   \u251c\u2500\u2500 node.py\n\u2502   \u2502   \u251c\u2500\u2500 user.py\n\u2502   \u2502   \u2514\u2500\u2500 user_group.py\n\u2502   \u2514\u2500\u2500 product_types\n\u2502       \u251c\u2500\u2500 circuit.py\n\u2502       \u251c\u2500\u2500 node.py\n\u2502       \u251c\u2500\u2500 user.py\n\u2502       \u2514\u2500\u2500 user_group.py\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 utils.py\n\u2514\u2500\u2500 workflows\n    \u251c\u2500\u2500 circuit\n    \u2502   \u251c\u2500\u2500 create_circuit.py\n    \u2502   \u251c\u2500\u2500 modify_circuit.py\n    \u2502   \u251c\u2500\u2500 shared.py\n    \u2502   \u251c\u2500\u2500 terminate_circuit.py\n    \u2502   \u2514\u2500\u2500 validate_circuit.py\n    \u251c\u2500\u2500 node\n    \u2502   \u251c\u2500\u2500 create_node.py\n    \u2502   \u2514\u2500\u2500 validate_node.py\n    \u2514\u2500\u2500 shared.py\n</code></pre>"},{"location":"workshops/advanced/scenario/","title":"Scenario","text":"<p>During this workshop a set of products will be created together with the needed workflows to manage enrolling network nodes into the Workflow Orchestrator and creating circuits between nodes. The products will be just complex enough to show the basic capabilities of products, product blocks, fixed inputs, resource types and workflows in the workflow orchestrator, and in addition to the lessons taught in the beginner workshop, we will cover nesting product blocks and products together.</p> <p>The following attributes need to be either stored from user input, pulled from external systems, or created dynamically in external systems:</p> <ul> <li>Node<ul> <li>node_id: The ID of the node in netbox.</li> <li>node_name: The name of the node</li> <li>ipv4_loopback: The IPv4 loopback address on the network node.</li> <li>ipv6_loopback: The IPv6 loopback address on the network node.</li> </ul> </li> <li>Circuit<ul> <li>speed: The speed of the circuit.</li> <li>circuit_id: The ID number of the circuit/cable in netbox that represents this circuit.</li> <li>circuit_description: The human-friendly description for this circuit.</li> <li>under_maintenance:</li> <li>PortPair: A pair of exactly two Layer3Interface objects<ul> <li>Layer3Interface<ul> <li>v6_ip_address: The IPv6 address on a Layer3 network interface.</li> <li>Port: A single instance of a port object<ul> <li>port_id: The ID number of the port in netbox</li> <li>port_description: The description that will live on the port in network config.</li> <li>port_name: The actual name of the port (i.e. 1/1/c1/, ge-0/0/1, etc.)</li> <li>node: A reference to the Node subscription instance that this port lives on.</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"workshops/advanced/workflow-introduction/","title":"Introduction","text":"<p>The workflow engine is the core of the orchestrator, it is responsible for the following functions:</p> <ul> <li> <p>Safely and reliable manipulate customer Subscriptions from one state to the next and maintain auditability.</p> </li> <li> <p>Create an API through which Subscriptions can be manipulated programmatically.</p> </li> <li> <p>Execute step functions in order and allow the retry of previously failed process-steps in an idempotent way.</p> </li> <li> <p>Atomically execute workflow functions.</p> </li> </ul> <p>For more details on what constitutes a workflow, refer to this section of the beginner workshop.</p> <p>For the purposes of this workshop, we have provided you with already functional workflows that we will be slightly modifying. The two main workflows we will be working on are <code>create_node</code> and <code>create_circuit</code>; overviews of these workflows are below:</p>"},{"location":"workshops/advanced/workflow-introduction/#node-creation-workflow","title":"Node Creation workflow","text":"<p>The node create workflow is the first step for configuring our mock network. This is the most basic workflow example, and simply talks to our DCIM/IPAM (Netbox) to determine which nodes are ready to be enrolled in the orchestrator and then ultimately presents the user with the config to apply on that device to complete the initial provisioning of that node. The general workflow outline is as follows:</p> <ol> <li>Present a list of valid routers to enroll (<code>initial_input_form_generator</code>)</li> <li>Populate the subscription's domain model initial values(<code>construct_node_model</code>)</li> <li>Store the subscription in the database (Using the built-in <code>store_process_subscription</code> step)</li> <li>Grab IP address information that has been defined for this device in netbox (<code>fetch_ip_address_information</code>)</li> <li>Display the configuration to the user that they will use to provision the device and ask them to confirm once it has been applied (<code>provide_config_to_user</code>)</li> <li>Update the subscription state to <code>PROVISIONING</code> (Using the built-in <code>set_status</code> step)</li> <li>Update the subscription lifecycle state to <code>ACTIVE</code>.</li> </ol> <p>One of our tasks as part of this workshop will be to implement a new step after we set the subscription status to <code>PROVISIONING</code> that updates the status of a node in netbox from <code>planning</code> to <code>active</code>. We'll do this by creating a function called <code>update_node_status_netbox</code>.</p> <p>Additionally, we will play with adding in validations to the node create workflow that will prevent us from somehow enrolling a node more than once.</p>"},{"location":"workshops/advanced/workflow-introduction/#circuit-creation-workflow","title":"Circuit Creation Workflow","text":"<p>The circuit create workflow builds upon the subscriptions created by the <code>node_create</code> workflow and allow us to further configure our mock network by creating circuits between nodes. This is a more advanced workflow example which talks to the orchestrator's DB as well as our DCIM/IPAM to grab lists of available nodes and ports, then ultimately presents the user with the config to apply on that device so that they can complete the provisioning of that circuit. The general workflow outline is as follows:</p> <ol> <li>Gather user input for creating the circuit (<code>initial_input_form_generator</code>)<ol> <li>Present the user with a list of possible routers to use for the \"A-side\" node</li> <li>Present the user with a list of possible routers to use for the \"B-side\" node (ensuring A-side and B-side are mutually exclusive devices)</li> <li>Present the user with a list of available ports on each router (pulled from netbox)</li> </ol> </li> <li>Populate the subscription's domain model initial values and create the planned connection in netbox(<code>construct_circuit_model</code>)</li> <li>Store the subscription in the database (Using the built-in <code>store_process_subscription</code> step)</li> <li>Talk to our IPAM to find the next free subnet to use on the circuit between these two nodes (<code>reserve_ips_in_ipam</code>)</li> <li>Display the configuration to the user that they will use to provision the device and ask them to confirm once it has been applied(<code>provide_config_to_user</code>)</li> <li>Update the subscription state to <code>PROVISIONING</code> (Using the built-in <code>set_status</code> step)</li> <li>Update the status of the connection in netbox from <code>planned</code> to <code>connected</code></li> <li>Update the description of the subscription in the orchestrator DB.</li> <li>Update the subscription lifecycle state to <code>ACTIVE</code>.</li> </ol> <p>One of our tasks as part of this workshop will be to fill out the modify workflow for the circuit so that we can transition this circuit from the default maintenance mode to a normal traffic mode mode.</p>"},{"location":"workshops/beginner/create-user-group/","title":"Create UserGroup workflow","text":""},{"location":"workshops/beginner/create-user-group/#exercise-1-create-usergroup-workflow","title":"Exercise 1: create UserGroup workflow","text":"<p>The create workflow will produce a subscription for a specific customer on the UserGroup product. This is done by executing the following workflow steps:</p> <pre><code>init\n&gt;&gt; create_subscription\n&gt;&gt; store_process_subscription(Target.CREATE)\n&gt;&gt; initialize_subscription\n&gt;&gt; provision_user_group\n&gt;&gt; set_status(SubscriptionLifecycle.ACTIVE)\n&gt;&gt; resync\n&gt;&gt; done\n</code></pre> <p>The builtin steps <code>init</code> and <code>done</code> are always part of a workflow and mark the begin and end of the workflow. Three other builtin steps are being used here that are almost always part of a create workflow:</p> <ul> <li> <p>store_process_subscription </p> <p>The orchestrator executes a workflow in a process, and keeps track of all workflows that have been run to create, modify or terminate a subscription, so there is an administrative trail of what happened to each subscription and when.  This step is used to enter this information into the database. The argument <code>Target.CREATE</code> indicates that this is a create workflow. The reason that this step is not the first step directly after <code>init</code> is because we need to know the subscription ID that is not yet known at that point in the workflow.</p> </li> <li> <p>set_status</p> <p>At the end of the workflow, after the subscription has been created, and the interaction with all OSS and BSS was successfully finished, the subscription state is set to <code>SubscriptionLifecycle.ACTIVE</code>. From this moment on the modify and terminate workflows can be started on this subscription.</p> </li> <li> <p>resync</p> <p>Every subscription has a notion of being in sync or not with all OSS and BSS. There are several moments that a subscription is deemed to be out of sync, one is during the time a workflow is active for a subscription, another is when a validation workflow (will be explained in the advanced workshop) has detected a discrepancy between the subscription and any of the OSS or BSS. One important side effect of a subscription being out of sync is that no new workflows can be started for that subscription, this is to protect for actions being taken on information that is possibly not accurate. The <code>resync</code> step sets the subscription in sync.</p> </li> </ul> <p>The three remaining steps are custom to this workflow:</p> <ul> <li> <p>create_subscription</p> <p>This step will create a new virgin subscription on a product. Every product has the standard method <code>from_product_id()</code> that takes two mandatory arguments: <code>product_id</code> and <code>customer_id</code>. Use this method on the <code>UserGroupInactive</code> product to create a subscription in state <code>SubscriptionLifecycle.INITIAL</code>. Because there is no CRM used during this beginner workshop the customer UUID can be faked.</p> <p>Make sure that this step returns a <code>Dict</code> with at least the keys <code>'subscription'</code> and <code>'subscription_id'</code> to make the orchestrator merge these keys into the workflow <code>State</code>. When leaving a step the orchestrator will also automatically commit the <code>'subscription'</code> to the database. The <code>'subscription_id'</code> key is needed by the <code>store_process_subscription</code> step.</p> </li> <li> <p>initialize_subscription</p> <p>This step will initialize the resource types based on the input from the user. In this case only the name of the group needs to be assigned. Also set the subscription description to something meaningful at this stage. After this, the subscription can be transitioned to <code>SubscriptionLifecycle.PROVISIONING</code>, this will trigger checks to make sure that all mandatory resource types are present for that lifecycle state. Every product has the standard method <code>from_other_lifecycle()</code> to accomplish this, this method takes the original subscription and the targe lifecycle state as arguments.</p> <p>Make sure that this step returns a <code>Dict</code> with also at least the key <code>'subscription'</code> to merge the modified subscription into the workflow state and have the orchestrator commit the subscription changes to the database.</p> </li> <li> <p>provision_user_group</p> <p>Now the user group can be provisioned in all OSS and BSS as necessary.  As there is no actual user group provisioning system during this workshop, this interaction is being faked. The returned (fake) group ID is assigned to the intended resource type.</p> <p>Yet again make sure that this step returns a <code>Dict</code> with at least the key <code>'subscription'</code>.</p> </li> </ul> <p>The only thing left that is needed is an initial input form generator function with one string input field to ask the user for the name of the user group.</p> <p>Use the skeleton below to create the file <code>workflows/user_group/create_user_group.py</code>:</p> <pre><code>from uuid import uuid4\n\nfrom orchestrator.forms import FormPage\nfrom orchestrator.targets import Target\nfrom orchestrator.types import FormGenerator, State, SubscriptionLifecycle, UUIDstr\nfrom orchestrator.workflow import done, init, step, workflow\nfrom orchestrator.workflows.steps import resync, set_status, store_process_subscription\nfrom orchestrator.workflows.utils import wrap_create_initial_input_form\n\nfrom products.product_types.user_group import UserGroupInactive, UserGroupProvisioning\n\n# initial input form generator\n...\n\n# create subscription step\n...\n\n# initialize subscription step\n...\n\n# provision user group step\n...\n\n# create user group workflow\n...\n</code></pre> <p>Spoiler: for inspiration look at an example implementation of the user group create workflow </p>"},{"location":"workshops/beginner/create-user/","title":"Create User workflow","text":""},{"location":"workshops/beginner/create-user/#exercise-1-create-user-workflow","title":"Exercise 1: create User workflow","text":"<p>The create <code>User</code> workflow is very similar to the create <code>UserGroup</code> workflow, the major difference is the increased number of user inputs needed to initialize the subscription. This workflow uses the following steps: </p> <pre><code>init\n&gt;&gt; create_subscription\n&gt;&gt; store_process_subscription(Target.CREATE)\n&gt;&gt; initialize_subscription\n&gt;&gt; provision_user\n&gt;&gt; set_status(SubscriptionLifecycle.ACTIVE)\n&gt;&gt; resync\n&gt;&gt; done\n</code></pre> <p>There is one important difference though, one of the user inputs on the input form is special: the selection of the user group the user belongs to. It is not just an integer or a string, but the user must be able to select a user group out of a list of already provisioned user groups. For this the database will be queried to obtain a list of active user group subscriptions, and a special input field type is used to display a dropdown input field on the input form. </p> <p>In the orchestrator, all access to the database is implemented using SQLAlchemy, and queries can be formulated using the classes from <code>orchestrator.db.models</code> that map to the tables in the database. The following query is all that is needed to get a list of <code>active</code> <code>UserGroup</code> subscriptions:</p> <pre><code>from orchestrator.db.models import ProductTable, SubscriptionTable\n\n...\n        SubscriptionTable.query.join(ProductTable)\n        .filter(\n            ProductTable.product_type == \"UserGroup\",\n            SubscriptionTable.status == \"active\",\n        )\n        .with_entities(SubscriptionTable.subscription_id, SubscriptionTable.description)\n        .all()\n...\n</code></pre> <p>The <code>orchestrator.forms.validators</code> package provides a standard input component called <code>choice_list</code> that will create the indicated enumeration and expects an iterator that returns tuples containing a label and a value. The iterator is created making use of the standard Python <code>zip</code> function. This input component will show a dropdown with all labels and returns a list of associated chosen keys.  The amount of entries that may be chosen is controlled by the <code>min_items</code> and <code>max_items</code> arguments.</p> <p>Putting everything together, the user group selector looks like this:</p> <pre><code>def user_group_selector() -&gt; list:\n    user_group_subscriptions = {}\n    for user_group_id, user_group_description in (\n        SubscriptionTable.query.join(ProductTable)\n        .filter(\n            ProductTable.product_type == \"UserGroup\",\n            SubscriptionTable.status == \"active\",\n        )\n        .with_entities(SubscriptionTable.subscription_id, SubscriptionTable.description)\n        .all()\n    ):\n        user_group_subscriptions[str(user_group_id)] = user_group_description\n\n    return choice_list(\n        Choice(\"UserGroupEnum\", zip(user_group_subscriptions.keys(), user_group_subscriptions.items())),\n        min_items=1,\n        max_items=1,\n    )\n</code></pre> <p>And can now be used in the input form as follows:</p> <pre><code>user_group_ids: user_group_selector()\n</code></pre> <p>In the subscription initialization step the <code>group</code> resource type of the <code>UserBlock</code> product block is assigned with the the <code>UserGroupBlock</code> from the <code>UserGroup</code> subscription:</p> <pre><code>subscription.user.group = UserGroup.from_subscription(user_group_ids[0]).user_group\n</code></pre> <p>Use the skeleton below to create the file <code>workflows/user/create_user.py</code>:</p> <pre><code>from typing import List, Optional\nfrom uuid import uuid4\n\nfrom orchestrator.db.models import ProductTable, SubscriptionTable\nfrom orchestrator.forms import FormPage\nfrom orchestrator.forms.validators import Choice, choice_list\nfrom orchestrator.targets import Target\nfrom orchestrator.types import FormGenerator, State, SubscriptionLifecycle, UUIDstr\nfrom orchestrator.workflow import done, init, step, workflow\nfrom orchestrator.workflows.steps import resync, set_status, store_process_subscription\nfrom orchestrator.workflows.utils import wrap_create_initial_input_form\n\nfrom products.product_types.user import UserInactive, UserProvisioning\nfrom products.product_types.user_group import UserGroup\n\n# user group selector\n...\n\n# initial input form generator\n...\n\n# create subscription step\n...\n\n# initialize subscription step\n...\n\n# provision user step\n...\n\n# create user workflow\n...\n</code></pre> <p>Spoiler: for inspiration look at an example implementation of the user create workflow </p>"},{"location":"workshops/beginner/database-migration/","title":"Database migration","text":""},{"location":"workshops/beginner/database-migration/#introduction","title":"Introduction","text":"<p>The orchestrator uses SQLAlchemy, the Python SQL toolkit and Object Relational Mapper, as interface to the database. Alembic, which is part of SQLAlchemy, is used for the creation, management, and invocation of database change management scripts.  </p> <p>Now that the product and product block domain models have been created, it is time to create an Alembic database migration to insert this information into the database. All the SQL statements needed for this migration can be written by hand, but knowledge about the database tables and how they are used is required to write correct statements. Luckily, the orchestrator comes with helper functions, located at <code>orchestrator/migrations/helpers</code>, that produce the needed SQL statements.  These helper functions make use of a set of simple data structures that describe the domain models and workflows that need to be added to the database. Recently a third option was added, the orchestrator is now able to detect differences between the database and the registered product domain models and create all needed SQL statements for you.</p> <p>Below we will make use of the ability of the orchestrator to create database migrations for us.</p>"},{"location":"workshops/beginner/database-migration/#exercise-1-add-products-to-registry","title":"Exercise 1: add products to registry","text":"<p>In order to use the products that were defined earlier, the orchestrator needs to know about their existence. This is done by adding the products with a description to the <code>SUBSCRIPTION_MODEL_REGISTRY</code>.</p> <p>The products can be added to the registry in <code>main.py</code>, but for this exercise the registry will be updated by the <code>products</code> module, this keeps the registration code close to the definition of the products and nicely separated from the rest of the code.</p> <p>Create the file <code>products/__init__.py</code> and add the following code:</p> <pre><code>from orchestrator.domain import SUBSCRIPTION_MODEL_REGISTRY\n\nfrom products.product_types.user import User\nfrom products.product_types.user_group import UserGroup\n\nSUBSCRIPTION_MODEL_REGISTRY.update(\n    {\n        \"User Group\": UserGroup,\n        \"User internal\": User,\n        \"User external\": User,\n    }\n)\n</code></pre> <p>To make Python execute this code, add the following import statement to <code>main.py</code>:</p> <pre><code>import products\n</code></pre>"},{"location":"workshops/beginner/database-migration/#exercise-2-create-database-migration","title":"Exercise 2: create database migration","text":"<p>To manually create a database migration a Python environment is needed, as  created with the manual installation steps, to run the orchestrator from the  command line. When using Docker compose an example migration is being used.</p>"},{"location":"workshops/beginner/database-migration/#docker-compose","title":"Docker compose","text":"<p>Copy the example product and product block migration:</p> <pre><code>(\ncd migrations/versions/schema\n  curl --remote-name https://raw.githubusercontent.com/workfloworchestrator/example-orchestrator-beginner/main/examples/2022-11-11_45984f4b8010_add_user_and_usergroup_products.py\n)\n</code></pre>"},{"location":"workshops/beginner/database-migration/#manual","title":"Manual","text":"<p>The orchestrator command line interface offers the <code>db migrate-domain-models</code> command to create a database migration based on the differences between the database and the registered products. In most cases this command will be able to detect all changes, but in more complex situations it will ask the user for additional input to create the correct migration. For new products it will also ask for user-friendly descriptions for the products, product blocks, resource types and fixed inputs, as well as information that is not defined in the domain models like product and product block tags, and values for the fixed inputs to differentiate the products of the same product type.</p> <p>Create the migration with the following command, have a look at the overview below when in doubt of the correct answer to the questions, and make sure that the product type entered exactly matches the product types defined in the domain models, including upper/lowercase: </p> <pre><code>PYTHONPATH=. python main.py db migrate-domain-models \"Add User and UserGroup products\"\n</code></pre> <p>When finished have a look at the migration created in the folder <code>migrations/versions/schema</code>.</p> <p>Note</p> <p>While creating the migration, the order of the questions/answers may be  different from the order in the overview below. Therefore do not blindly copy/paste the answers.</p> <p>Example</p> <p>Create new products Product: UserGroup User Group Supply the product description: user group administration Supply the product tag: GROUP Product: User User internal Supply the product description: user administration - internal Supply the product tag: USER_INT Product: User User external Supply the product description: user administration - external Supply the product tag: USER_EXT</p> <p>Create fixed inputs Supply fixed input value for product User internal and fixed input affiliation: internal Supply fixed input value for product User external and fixed input affiliation: external</p> <p>Create product blocks Product block: UserGroupBlock Supply the product block description: user group block Supply the product block tag: UGB Product block: UserBlock Supply the product block description: user block Supply the product block tag: UB</p> <p>Create resource types Supply description for new resource type group_name: name of the user group Supply description for new resource type group_id: id of the user group Supply description for new resource type username: name of the user Supply description for new resource type age: age of the user Supply description for new resource type user_id: id of the user</p>"},{"location":"workshops/beginner/database-migration/#exercise-3-perform-database-migration","title":"Exercise 3: perform database migration","text":"<p>To create a representation of the products in the database that matches the domain models, the database migration created above is executed.</p>"},{"location":"workshops/beginner/database-migration/#docker-compose_1","title":"Docker compose","text":"<p>The Docker compose environment contains a initialization container that will  always upgrade the database to the latest heads. To trigger this you only have to restart the environment.</p>"},{"location":"workshops/beginner/database-migration/#manual_1","title":"Manual","text":"<p>One way to manually migrate to the latest schemas is to explicitly upgrade the database to the revision that was just created  with <code>db upgrade &lt;revision&gt;</code>. Another way is to upgrade to the latest heads again, as was done during the initialisation of the database.</p> <pre><code>PYTHONPATH=. python main.py db upgrade heads\n</code></pre> <p>Look at what the migration added to the database by either querying the database directly:</p> <pre><code>psql orchestrator-core\n</code></pre> <p>or by using the orchestrator API:</p> <pre><code>curl http://127.0.0.1:8080/api/products/ | jq\n</code></pre> <p>or by browsing through the orchestrator meta data through the GUI at:</p> <pre><code>http://localhost:3000/metadata/products\n</code></pre> <p>or all of the above.</p> <p>The metadata/products page should look as following:</p> <p></p> <p>Example</p> <p>if the database migration is incorrect, use this example  Add User and UserGroup products  migration</p>"},{"location":"workshops/beginner/debian/","title":"Debian 11 (bullseye) installation instructions","text":"<p>How to manually install the orchestrator-core and orchestrator-core-gui on  Debian 11  (Bullseye) is described in the following steps.</p>"},{"location":"workshops/beginner/debian/#step-1-install-dependencies","title":"Step 1 - Install dependencies","text":"<p>First make sure the debian install is up to date. Then install the following software dependencies:</p> <ul> <li>PostgreSQL (version &gt;=11)</li> <li>Git</li> <li>virtualenvwrapper</li> <li>Node.js (version 14)</li> </ul> <pre><code>sudo apt update\nsudo apt upgrade --yes\ncurl -sL https://deb.nodesource.com/setup_14.x | sudo bash -\nsudo apt-get install --yes postgresql git virtualenvwrapper nodejs\n</code></pre>"},{"location":"workshops/beginner/debian/#step-2-database-setup","title":"Step 2 - Database setup","text":"<p>In step 1 the database server is already started as part of the apt installation procedure. If the database server was previously installed and stopped then start it again. Create the database with the following commands, use <code>nwa</code> as password:</p> <pre><code>sudo -u postgres createuser -sP nwa\nsudo -u postgres createdb orchestrator-core -O nwa\n</code></pre> <p>For debug purposes, interact directly with the database by starting the  PostgresSQL interactive terminal:</p> <pre><code>sudo -u postgres psql orchestrator-core\n</code></pre>"},{"location":"workshops/beginner/debian/#step-3-install-orchestrator","title":"Step 3 - Install orchestrator","text":"<p>The minimal version of Python is 3.9. Before the orchestrator core and all its dependencies are installed, a Python virtual environment is created:</p> <pre><code>mkdir example-orchestrator\ncd example-orchestrator\nsource /usr/share/virtualenvwrapper/virtualenvwrapper.sh\nmkvirtualenv --python python3.9 example-orchestrator\n</code></pre> <p>Make sure that the just created Python virtual environment is active before installing the orchestrator-core:</p> <pre><code>pip install orchestrator-core\n</code></pre> <p>A next time in a new shell, be sure to activate the Python virtual environment again:</p> <pre><code>source /usr/share/virtualenvwrapper/virtualenvwrapper.sh\nworkon example-orchestrator\n</code></pre>"},{"location":"workshops/beginner/debian/#step-4-init-orchestrator","title":"Step 4 - Init orchestrator:","text":"<p>Create a <code>main.py</code> file with the following content:</p> <pre><code>from orchestrator import OrchestratorCore\nfrom orchestrator.cli.main import app as core_cli\nfrom orchestrator.settings import AppSettings\n\napp = OrchestratorCore(base_settings=AppSettings())\n\nif __name__ == \"__main__\":\n    core_cli()\n</code></pre> <p>Commit the just created main.py to git:</p> <pre><code>git init --initial-branch main\ngit config --local user.email \"you@example.com\"\ngit config --local user.name \"Your Name\"\ngit add main.py\ngit commit -m \"Initial commit\"\n</code></pre> <p>Note that your local git must contain at least one commit because otherwise the <code>db init</code> below will fail.</p> <p>Initialize the database and run all the database migrations:</p> <pre><code>PYTHONPATH=. python main.py db init\nPYTHONPATH=. python main.py db upgrade heads\n</code></pre>"},{"location":"workshops/beginner/debian/#step-5-install-orchestrator-client","title":"Step 5 - Install orchestrator client","text":"<p>Install the orchestrator client in the parent directory of the  example-orchestrator:</p> <pre><code>cd ..\ngit clone https://github.com/workfloworchestrator/orchestrator-core-gui.git\n</code></pre> <p>Install the Yarn package manager and use it to install the orchestrator  client dependencies:</p> <pre><code>cd orchestrator-core-gui/\nsudo npm install --global yarn\nsudo npm install --global --save-dev npm-run-all\nyarn install\n</code></pre>"},{"location":"workshops/beginner/debian/#step-6-init-orchestrator-client","title":"Step 6 - Init orchestrator client:","text":"<p>Use the supplied environment variable defaults:</p> <pre><code>cp .env.local.example .env.local\n</code></pre> <p>And make the following changes to <code>.env.local</code>:</p> <pre><code># change the existing REACT_APP_BACKEND_URL variable value into:\nREACT_APP_BACKEND_URL=http://your_ip_address_here:3000\n# and add the following:\nDANGEROUSLY_DISABLE_HOST_CHECK=true\n</code></pre> <p>The <code>custom-example</code> folder contains some SURF specific modules that can be used as an example. It must be linked to the folder <code>custom</code> in order for the app to start:</p> <pre><code>(cd src &amp;&amp; ln -s custom-example custom)\n</code></pre>"},{"location":"workshops/beginner/docker/","title":"Docker compose installation instructions","text":"<p>How to run the orchestrator-core and orchestrator-core-gui with Docker  Compose is described in the steps below. The following Docker images are  used:</p> <ul> <li>orchestrator-core:   The workflow orchestrator step engine.</li> <li>orchestrator-core-gui:   The GUI for the orchestrator-core.</li> <li>postgres:   The PostgreSQL object-relational database system.</li> <li>busybox:   The swiss army knife of embedded linux.</li> </ul>"},{"location":"workshops/beginner/docker/#step-1-prepare-environment","title":"Step 1 - Prepare environment","text":"<p>First create the folder to hold the example orchestrator that is being build  during this workshop. Then copy the <code>docker-compose.yml</code> and  <code>orchestrator-core-gui.env</code> to control and configure the environment, and  copy a <code>main.py</code> that contains a rudimentary orchestrator application.</p> <pre><code>mkdir example-orchestrator\ncd example-orchestrator\ncurl --remote-name https://raw.githubusercontent.com/workfloworchestrator/example-orchestrator-beginner/main/docker-compose.yml\ncurl --remote-name https://raw.githubusercontent.com/workfloworchestrator/example-orchestrator-beginner/main/orchestrator-core-gui.env\ncurl --remote-name https://raw.githubusercontent.com/workfloworchestrator/example-orchestrator-beginner/main/examples/main.py\n</code></pre> <p>Commit the copied files to a local git repository:</p> <pre><code>git init --initial-branch main\ngit config --local user.email \"you@example.com\"\ngit config --local user.name \"Your Name\"\ngit add .\ngit commit -m \"Initial commit\"\n</code></pre> <p>Note that your local git repository must contain at least one commit because  otherwise the database initializations step below will fail.</p>"},{"location":"workshops/beginner/docker/#step-2-start-environment","title":"Step 2 - Start environment","text":"<p>Docker compose will take care of all necessary initialization and startup of the database, orchestrator and GUI:</p> <ol> <li>the busybox container creates the folder <code>db_data</code>, if not yet present</li> <li>then a postgres container creates a database, if it does not exist     already, after which the database server is started</li> <li>an orchestrator container is used to initialize the database, if     not already initialized, and creates an <code>alembic.ini</code> file and a     <code>migrations</code> folder for the database migrations</li> <li>a second run of the orchestrator container will upgrade the database to     the latest heads, and will do so everytime the environment is started</li> <li>then a third run of the orchestrator container will use <code>main.py</code> to     run the orchestrator</li> <li>finally, the GUI is started in the orchestrator-core-gui container</li> </ol> <pre><code>docker compose up\n</code></pre>"},{"location":"workshops/beginner/docker/#step-3-open-a-browser","title":"Step 3 - Open a browser","text":"<p>Now point a browser to:</p> <p><pre><code>http://localhost:3000/\n</code></pre> and have a look around. </p> <p>Note</p> <p>Once opened in the browser, ignore the message about the CRM not being responsive, this workshop does not include the setup of an interface to a CRM, fake customers IDs will be used instead.</p>"},{"location":"workshops/beginner/domain-models/","title":"Domain models","text":""},{"location":"workshops/beginner/domain-models/#introduction","title":"Introduction","text":"<p>First read the Architecture; TLDR section of the orchestrator core documentation to get an overview of the concepts that will be covered.</p> <p>To put a part of the terminology in context, products are modeled using a set of product blocks. The product attributes are modeled by resource types.  By default all resource types are mutable and can be changed over the lifetime of a subscription. Fixed inputs are used to model immutable attributes.</p> <p>An example of an immutable attribute is for example the speed of a network interface, which is a physical property of the interface, and cannot be changed without a field engineer swapping the interface with one with a different speed. Another example is an attribute that is linked to the price of a product, for example the greater the capacity of a product, the higher the  price. A customer is not allowed to increase the capacity himself, he has  to pay extra first.</p> <p>The products and product blocks for this workshop will be modeled as follows:</p> <ul> <li>product UserGroup<ul> <li>product block reference user_group (UserGroupBlock)</li> </ul> </li> <li>product User<ul> <li>fixed input affiliation</li> <li>product block reference user (UserBlock)</li> </ul> </li> <li>product block UserGroupBlock<ul> <li>resource type group_name</li> <li>resource type group_id</li> </ul> </li> <li>product block UserBlock<ul> <li>resource type username</li> <li>resource type age</li> <li>resource type user_id</li> <li>product block reference group (UserGroupBlock)</li> </ul> </li> </ul> <p>A product can be seen as a container for fixed inputs and (at least one) references to a product block, and a product block as a container for resources types and (optional) references to other product blocks. Product block references may be nested as deep as needed.</p>"},{"location":"workshops/beginner/domain-models/#exercise-1-create-usergroup-product-block","title":"Exercise 1: create UserGroup product block","text":"<p>Read the Domain models section of the orchestrator core documentation to learn more about domain models and how they are defined. For now, skip the code examples Product Model a.k.a SubscriptionModel and Advanced Use Cases.</p> <p>Use the following skeleton to create the file <code>user_group.py</code> in the <code>products/product_blocks</code> folder and define the <code>UserGroupBlockInactive</code>, <code>UserGroupBlockProvisioning</code> and <code>UserGroupBlock</code> domain models describing the user group product block in the lifecycle states <code>INITIAL</code>, <code>PROVISIONING</code> and <code>ACTIVE</code>:</p> <pre><code>from typing import Optional\n\nfrom orchestrator.domain.base import ProductBlockModel\nfrom orchestrator.types import SubscriptionLifecycle\n\n# UserGroupBlockInactive with all resource types optional\n...\n\n# UserGroupBlockProvisioning with only resource type group_id optional\n...\n\n# UserGroupBlock with all resource types mandatory\n... \n</code></pre> <p>Example</p> <p>for inspiration look at an example implementation of the user group product block </p>"},{"location":"workshops/beginner/domain-models/#exercise-2-create-usergroup-product","title":"Exercise 2: create UserGroup product","text":"<p>Return to the Domain models section of the orchestrator core documentation and look at the code example Product Model a.k.a SubscriptionModel.</p> <p>Use the following skeleton to create the file <code>user_group.py</code> in the <code>products/product_types</code> folder and define the <code>UserGroupInactive</code>, <code>UserGroupProvisioning</code> and <code>UserGroup</code> domain models describing the user group product in its different lifecycle states:</p> <pre><code>from orchestrator.domain.base import SubscriptionModel\nfrom orchestrator.types import SubscriptionLifecycle\n\nfrom products.product_blocks.user_group import UserGroupBlock, UserGroupBlockInactive, UserGroupBlockProvisioning\n\n# UserGroupInactive\n...\n\n# UserGroupProvisioning\n...\n\n# UserGroup\n...\n</code></pre> <p>Example</p> <p>for inspiration look at an example implementation of the user group product </p>"},{"location":"workshops/beginner/domain-models/#exercise-3-create-user-product-block","title":"Exercise 3: create User product block","text":"<p>Use the following skeleton to create the file <code>user.py</code> in the <code>products/product_blocks</code> folder and define the <code>UserBlockInactive</code>, <code>UserBlockProvisioning</code> and <code>UserBlock</code> domain models describing the user group product block in its different lifecycle states:</p> <pre><code>from typing import Optional\n\nfrom orchestrator.domain.base import ProductBlockModel\nfrom orchestrator.types import SubscriptionLifecycle\n\nfrom products.product_blocks.user_group import UserGroupBlock, UserGroupBlockInactive, UserGroupBlockProvisioning\n\n# UserBlockInactive with only product block reference group mandatory\n...\n\n# UserBlockProvisioning with only resource type user_id and age optional\n...\n\n# UserBlock with only resource type age optional\n...\n</code></pre> <p>Example</p> <p>for inspiration look at an example implementation of the user product block </p>"},{"location":"workshops/beginner/domain-models/#exercise-4-create-user-product","title":"Exercise 4: create User product","text":"<p>Use the following skeleton to create the file <code>user.py</code> in the <code>products/product_types</code> folder and define the <code>UserInactive</code>, <code>UserProvisioning</code> and <code>User</code> domain models describing the user product in its different lifecycle states.</p> <p>Note that the <code>strEnum</code> type from the orchestrator is used, which uses the standard python module <code>enum</code> to define an enumeration of strings, to create a type to be used for the fixed input <code>affiliation</code>.</p> <pre><code>from orchestrator.domain.base import SubscriptionModel\nfrom orchestrator.types import SubscriptionLifecycle, strEnum\n\nfrom products.product_blocks.user import UserBlock, UserBlockInactive, UserBlockProvisioning\n\nclass Affiliation(strEnum):\n    internal = \"internal\"\n    external = \"external\"\n\n# UserInactive(SubscriptionModel\n...\n\n# UserProvisioning\n...\n\n# User\n...\n</code></pre> <p>Example</p> <p>for inspiration look at an example implementation of the user product </p>"},{"location":"workshops/beginner/explore/","title":"Explore the GUI and API","text":""},{"location":"workshops/beginner/explore/#is-the-example-orchestrator-working","title":"Is the example orchestrator working?","text":"<p>If the orchestrator setup including the products that were created as part of this workshop do not work for some reason or the other, it is possible to quickly setup a working example orchestrator by  following the steps below (on Debian add <code>sudo</code> where needed):</p> <pre><code>git clone https://github.com/workfloworchestrator/example-orchestrator-beginner.git\ncd example-orchestrator-beginner\n# only drop database when you are really sure!\necho 'drop database \"orchestrator-core\";' | psql postgres\ncreatedb orchestrator-core -O nwa\nsource virtualenvwrapper.sh\nworkon example-orchestrator || mkvirtualenv --python python3.9 example-orchestrator\npip install orchestrator-core\nPYTHONPATH=. python main.py db init\ncp -av examples/*add_user_and_usergroup* migrations/versions/schema\nPYTHONPATH=. python main.py db upgrade heads\nENABLE_WEBSOCKETS=True uvicorn --host 127.0.0.1 --port 8080 main:app\n</code></pre>"},{"location":"workshops/beginner/explore/#explore","title":"Explore","text":"<p>It is now time to explore the GUI and API. With the set of products created during this workshop users and groups can be created, modified and deleted. The easiest way is by using the GUI at:</p> <pre><code>http://localhost:3000/\n</code></pre> <p>But also check out the API at:</p> <pre><code>http://127.0.0.1:8080/api/docs\n</code></pre> <p>And look at the products:</p> <pre><code>curl http://127.0.0.1:8080/api/products/ | jq\n</code></pre> <p>Or get a list of subscriptions:</p> <pre><code>curl http://127.0.0.1:8080/api/subscriptions/all | jq\n</code></pre> <p>Or inspect the instantiated domain model for a subscription ID:</p> <pre><code>curl http://127.0.0.1:8080/api/subscriptions/domain-model/&lt;subscription_id&gt; | jq\n</code></pre> <p>And for the adventurers, create a subscription from the command line:</p> <pre><code>curl -X POST \\\n-H \"Content-Type: application/json\" \\\nhttp://127.0.0.1:8080/api/processes/create_user_group \\\n--data '[\n  {\n    \"product\": \"a03eb19a-8a83-4964-85ea-98371f1d87f8\"\n  },\n  {\n    \"group_name\": \"Test Group\"\n  }\n]'\n</code></pre>"},{"location":"workshops/beginner/input-forms/","title":"Input forms","text":""},{"location":"workshops/beginner/input-forms/#introduction","title":"Introduction","text":"<p>The orchestrator GUI is a ReactJS application that runs in front of the orchestrator. It will consume the orchestrator API and enable the user to interact with the products, subscriptions and processes that are built and run in the orchestrator.</p> <p>The GUI uses Elastic-UI as framework for standard components and Uniforms to parse JSON-Schema produced by the forms endpoints in the core and render the correct components and widgets.</p>"},{"location":"workshops/beginner/input-forms/#input-form-generator","title":"Input form generator","text":"<p>An input form generator function is needed to define the fields and the type of the fields to be shown in the GUI to allow the user to input information needed to instantiate a subscription based on a product. A simple input form generator function looks as follows:</p> <pre><code>def initial_input_form_generator(product_name: str) -&gt; FormGenerator:\n    class CreateProductForm(FormPage):\n        class Config:\n            title = product_name\n\n        user_input: str\n\n    form_input = yield CreateProductForm\n\n    return form_input.dict()\n</code></pre> <p>All forms use <code>FormPage</code> as base and can be extended with the form input fields needed. In this case a string input field will be shown, the text entered will be assigned to <code>user_input</code>. All inputs from all input fields are then returned as a <code>Dict</code> and will be merged into the <code>State</code>. The <code>product_name</code> argument comes from the initial <code>State</code>. </p> <p>The optional <code>Config</code> class can be used to pass configuration information to Uniforms. In this case Uniforms is asked to show a input form page with the name of the product as title.</p> <p>The helper functions <code>wrap_create_initial_input_form</code>, for create workflows, and <code>wrap_modify_initial_input_form</code>, for modify and terminate workflows, are used to integrate the input form into the workflow and perform all needed <code>State</code> management. A common pattern used is:</p> <pre><code>@workflow(\n    \"Create product subscription\",\n    initial_input_form=wrap_create_initial_input_form(initial_input_form_generator),\n    target=Target.CREATE,\n)\ndef create_product_subscription():\n    return init &gt;&gt; create_subscription &gt;&gt; done\n</code></pre> <p>The <code>wrap_*</code> helper functions pre-populates the <code>State</code> with information needed by the initial input form. For the create workflow the <code>product</code> ID and <code>product_name</code> are added to the <code>State</code>:</p> <pre>\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      workflow start       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502\nproduct\nproduct_name\n\u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502       input form(s)       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502\nproduct\nproduct_name\nuser_input\n\u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      create workflow      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <p>And for the modify and terminate workflows the <code>product</code> ID, <code>organisation</code>  ID and <code>subscription_id</code> are added to the <code>State</code></p> <pre>\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      workflow start       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502\nproduct\norganisation\nsubscription_id\n\u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502       input form(s)       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502\nproduct\norganisation\nsubscription_id\nuser_input\n\u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 modify/terminate workflow \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre>"},{"location":"workshops/beginner/macos/","title":"MacOS version 12 (Monterey) installation instructions","text":"<p>How to manually install the orchestrator-core and orchestrator-core-gui on  MacOS version 12 (Monterey) is described in the following steps.</p>"},{"location":"workshops/beginner/macos/#step-1-install-dependencies","title":"Step 1 - Install dependencies","text":"<p>This installation instruction assumes the use of Homebrew. The following software dependencies need to be installed:</p> <ul> <li>Python 3.9</li> <li>PostgreSQL (version &gt;=11)</li> <li>virtualenvwrapper (or use any other tool to create virtual Python    environments)</li> <li>Node.js (version 14)</li> <li>yarn</li> </ul> <pre><code>brew install python@3.9 postgresql@13 virtualenvwrapper node@14 yarn\n</code></pre>"},{"location":"workshops/beginner/macos/#step-2-database-setup","title":"Step 2 - Database setup","text":"<p>Start the database server and create the database with the following commands, use <code>nwa</code> as password:</p> <pre><code>brew services start postgresql@13\ncreateuser -sP nwa\ncreatedb orchestrator-core -O nwa\n</code></pre> <p>For debug purposes, interact directly with the database by starting the PostgresSQL interactive terminal:</p> <pre><code>psql orchestrator-core\n</code></pre>"},{"location":"workshops/beginner/macos/#step-3-install-orchestrator","title":"Step 3 - Install orchestrator","text":"<p>The minimal version of Python is 3.9. Before the orchestrator core and all its dependencies are installed, a Python virtual environment is created:</p> <pre><code>mkdir example-orchestrator\ncd example-orchestrator\nsource virtualenvwrapper.sh\nmkvirtualenv --python python3.9 example-orchestrator\n</code></pre> <p>Make sure that the just created Python virtual environment is active before installing the orchestrator-core:</p> <pre><code>pip install orchestrator-core\n</code></pre> <p>A next time in a new shell, be sure to activate the Python virtual environment again:</p> <pre><code>source virtualenvwrapper.sh\nworkon example-orchestrator\n</code></pre>"},{"location":"workshops/beginner/macos/#step-4-init-orchestrator","title":"Step 4 - Init orchestrator:","text":"<p>Create a <code>main.py</code> file with the following content:</p> <pre><code>from orchestrator import OrchestratorCore\nfrom orchestrator.cli.main import app as core_cli\nfrom orchestrator.settings import AppSettings\n\napp = OrchestratorCore(base_settings=AppSettings())\n\nif __name__ == \"__main__\":\n    core_cli()\n</code></pre> <p>Commit the just created main.py to git:</p> <pre><code>git init --initial-branch main\ngit config --local user.email \"you@example.com\"\ngit config --local user.name \"Your Name\"\ngit add main.py\ngit commit -m \"Initial commit\"\n</code></pre> <p>Note that your local git must contain at least one commit, otherwise the <code>db init</code> below will fail.</p> <p>Initialize the database and run all the database migrations:</p> <pre><code>PYTHONPATH=. python main.py db init\nPYTHONPATH=. python main.py db upgrade heads\n</code></pre>"},{"location":"workshops/beginner/macos/#step-5-install-orchestrator-client","title":"Step 5 - Install orchestrator client","text":"<p>Install the orchestrator client in the parent directory of the  example-orchestrator:</p> <pre><code>cd ..\ngit clone https://github.com/workfloworchestrator/orchestrator-core-gui.git\n</code></pre> <p>When multiple version of Node.js are installed, make sure node@14 is being used, this can be achieved by explicitly prepending it to the shell PATH.  Use the Yarn package manager to install the orchestrator client dependencies:</p> <pre><code>cd orchestrator-core-gui/\nexport PATH=\"/usr/local/opt/node@14/bin:$PATH\"\nyarn install\n</code></pre>"},{"location":"workshops/beginner/macos/#step-6-init-orchestrator-client","title":"Step 6 - Init orchestrator client:","text":"<p>Use the supplied environment variable defaults:</p> <pre><code>cp .env.local.example .env.local\n</code></pre> <p>And make the following changes to <code>.env.local</code>:</p> <pre><code># change the existing REACT_APP_BACKEND_URL variable value into:\nREACT_APP_BACKEND_URL=http://127.0.0.1:3000\n# and add the following:\nDANGEROUSLY_DISABLE_HOST_CHECK=true\n</code></pre> <p>The <code>custom-example</code> folder contains some SURF specific modules that can be used as an example. It must be linked to the folder <code>custom</code> in order for the app to start:</p> <pre><code>(cd src &amp;&amp; ln -s custom-example custom)\n</code></pre>"},{"location":"workshops/beginner/modify-user-group/","title":"Modify UserGroup workflow","text":""},{"location":"workshops/beginner/modify-user-group/#exercise-1-modify-usergroup-workflow","title":"Exercise 1: modify UserGroup workflow","text":"<p>The modify workflow can be used to change some or all of the resource types of an existing subscription. In this case the following workflow steps will be used:</p> <pre><code>init\n&gt;&gt; store_process_subscription(Target.MODIFY)\n&gt;&gt; unsync\n&gt;&gt; modify_user_group_subscription\n&gt;&gt; resync\n&gt;&gt; done\n</code></pre> <p>Besides the subscription administration that needs to be done, which probably already starts to look familiar, there is only one custom step that needs to be implemented. Most of the builtin steps were already discussed, but the <code>unsync</code> step is new. As can be guessed, this step has the opposite effect as the <code>resync</code> step, it sets an subscription out of sync for the duration of the modify which prohibits other workflows being started for this subscription.</p> <p>The <code>modify_user_group_subscription</code> step has three simple tasks. It will store the changed name of the group in the resource type <code>user_group</code> of the subscription. Secondly, it will change the subscription description to reflect the changed user group name. And last but not least, the user group name is also updated in the imaginary external user group provisioning system. Do not forget to return a <code>Dict</code> with a key <code>'subscription'</code> to merge the updated subscription into the workflow <code>State</code>, otherwise updates to the subscription will also not be saved to the database.</p> <p>The only thing remaining now is to create an initial input form generator that will show an input form with a string input field that shows the existing user group name, and allows for changes to be made. This is established by assigning the existing value to the input field used to enter the user group name. And as an extra, a second read-only field will be shown with the user group ID. For the latter the forms helper function <code>ReadOnlyField</code> can be used in the following way:</p> <pre><code>group_id: int = ReadOnlyField(subscription.user_group.group_id)\n</code></pre> <p>But where does the instantiated subscription come from in the initial input form generator? Remember that the workflow <code>State</code> available to the input form does not include the <code>subscription</code>, it only has the <code>subscription_id</code> at that stage. Luckily every product has a standard <code>from_subscription()</code> method that takes an subscription ID as argument that will fetch the subscription from the database and returns a fully instantiated domain model.  Remember to use the <code>wrap_modify_initial_input_form</code> wrapper for this modify workflow to make the subscription ID available to the input form.</p> <p>Use the skeleton below to create the file <code>workflows/user_group/modify_user_group.py</code>:</p> <pre><code>from orchestrator.forms import FormPage, ReadOnlyField\nfrom orchestrator.targets import Target\nfrom orchestrator.types import FormGenerator, State, UUIDstr\nfrom orchestrator.workflow import done, init, step, workflow\nfrom orchestrator.workflows.steps import resync, store_process_subscription, unsync\nfrom orchestrator.workflows.utils import wrap_modify_initial_input_form\n\nfrom products.product_types.user_group import UserGroup\n\n# initial input form generator\n...\n\n# modify user group step\n...\n\n# modify user group workflow\n...\n</code></pre> <p>Spoiler: for inspiration look at an example implementation of the user group modfiy workflow </p>"},{"location":"workshops/beginner/modify-user/","title":"Modify User workflow","text":""},{"location":"workshops/beginner/modify-user/#exercise-1-modify-user-workflow","title":"Exercise 1: modify User workflow","text":"<p>The modify <code>User</code> workflow is also very similar to the modify <code>UserGroup</code> workflow, except for the different set of resource types that can be changed. This workflow uses the following steps: </p> <pre><code>init\n&gt;&gt; store_process_subscription(Target.MODIFY)\n&gt;&gt; unsync\n&gt;&gt; modify_user_subscription\n&gt;&gt; resync\n&gt;&gt; done\n</code></pre> <p>To show the current user group in the dropdown on the input form, the subscription ID of that user group is needed. But the <code>User</code> subscription only contains a reference to the <code>UserGroupBlock</code>, not the <code>UserGroup</code> subscription that is needed. Luckily, every instantiated product block has an attribute <code>owner_subscription_id</code> that contains the subscription ID of the subscription owning this product block instance.</p> <p>The <code>choice_list</code> input both returns a list as result and expects a list of values that it uses to display the currently selected item(s). The following will display a dropdown showing the currently selected user group: </p> <pre><code>user_group_id: user_group_selector() = [str(subscription.user.group.owner_subscription_id)]\n</code></pre> <p>Use the skeleton below to create the file <code>workflows/user/modify_user.py</code>, and note that the <code>user_group_selector</code> from the create workflow is being reused:</p> <pre><code>from typing import List, Optional\n\nfrom orchestrator.forms import FormPage\nfrom orchestrator.targets import Target\nfrom orchestrator.types import FormGenerator, State, UUIDstr\nfrom orchestrator.workflow import done, init, step, workflow\nfrom orchestrator.workflows.steps import resync, store_process_subscription, unsync\nfrom orchestrator.workflows.utils import wrap_modify_initial_input_form\n\nfrom products.product_types.user import User\nfrom products.product_types.user_group import UserGroup\nfrom workflows.user.create_user import user_group_selector\n\n# initial input form generator\n...\n\n# modify user step\n...\n\n# modify user workflow\n...\n</code></pre> <p>Spoiler: for inspiration look at an example implementation of the user modfiy workflow </p>"},{"location":"workshops/beginner/overview/","title":"Overview beginner workshop","text":""},{"location":"workshops/beginner/overview/#intended-audience","title":"Intended audience","text":"<p>This workshop is intended for everybody who is new to the workflow orchestrator and wants to learn how to install and run the applications, and create a first working set of products and associated workflows.</p> <p>Knowledge of the Python programming language and the Unix command line interface are prerequisites to do this workshop.</p>"},{"location":"workshops/beginner/overview/#topics","title":"Topics","text":"<ul> <li>Installation   Detailed instructions are given on how to prepare your environment and   install the orchestrator and GUI. Instructions for both Debian and MacOS are   included.</li> <li>Start applications   Shows a simple way of starting the orchestrator and GUI. </li> <li>Create User and User Group products   Through a simple user and group management scenario a set of products is    created showing how domain models are defined.<ul> <li>Domain models    Explains the benefits of the use of domain models and shows how the     hierarchy of products, product blocks, fixed inputs and resource    types is used to create product subscriptions for customers.</li> <li>Database migration   Use the orchestrator to create an Alembic database migration based on the    domain models that describe the created products and product blocks.</li> </ul> </li> <li>Create User and User Group workflows   For both the User And User Group products a set of create, modify and    terminate workflows will be created. The use of input forms is explained    as part of defining the create workflow. This will show how a simple    product block hierarchy is created.</li> </ul>"},{"location":"workshops/beginner/overview/#workshop-folder-layout","title":"Workshop folder layout","text":"<p>This workshop uses the following folder layout:</p> <pre><code>beginner-workshop\n\u2502\n\u251c\u2500\u2500 example-orchestrator\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 main.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 migrations\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 ...\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 products\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 product_blocks\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 user.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 user_group.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 product_types\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 user.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 user_group.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 workflows\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 user\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 create_user.py\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 modify_user.py\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 terminate_user.py\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 user_group\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 create_user_group.py\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 modify_user_group.py\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 terminate_user_group.py\n\u2502\n\u2514\u2500\u2500 orchestrator-core-gui\n \u00a0\u00a0 \u2514\u2500\u2500 ...\n</code></pre> <p>The <code>orchestrator-core-gui</code> folder will be cloned from GitHub. The <code>example-orchestrator</code> folder will be used for the orchestrator that is created during this workshop.  Although any layout of the latter folder will work, it is encouraged to use the suggested folder layout and filenames during this workshop.</p>"},{"location":"workshops/beginner/register-workflows/","title":"Register workflows","text":"<p>The orchestrator needs to know which workflows are available for which products. This is a two stage registration process. The workflows need to be registered as a workflow function in the code and a mapping between workflow and product_type needs to be added to the database through migration script. First we will add the workflow functions. For creating the migration script, we can either let the <code>cli</code> create an empty one and fill it manually or use the <code>db migrate-workflows</code> command to generate one based on the diffs between the registered workflows in the code and the database.</p>"},{"location":"workshops/beginner/register-workflows/#step-1-map-workflow-function-to-package","title":"Step 1: Map workflow function to package","text":"<p>Registering workflow functions in the code is done by creating appropriate <code>LazyWorkflowInstance</code> instances that maps a workflow function to the Python package where it is defined.</p> <p>For example, the <code>LazyWorkflowInstance</code> for the <code>UserGroup</code> create workflow looks like this:</p> <pre><code>from orchestrator.workflows import LazyWorkflowInstance\n\nLazyWorkflowInstance(\"workflows.user_group.create_user_group\", \"create_user_group\")\n</code></pre> <p>Add the <code>LazyWorkflowInstance</code> calls for all six workflows to <code>workflows/__init__. py</code>, and add <code>import workflows</code> to <code>main.py</code> so the instances are created as part of the workflow package.</p> <p>Example</p> <p>for inspiration look at an example implementation of the lazy workflow instances </p>"},{"location":"workshops/beginner/register-workflows/#step-2-register-workflow-in-database","title":"Step 2: Register workflow in database","text":"<p>There are several ways to complete this step:</p> <ul> <li>Copy the example workflows migration file from the example repository</li> <li>Use the <code>db migrate-workflows</code> generator script</li> <li>Create an empty migration file and edit it</li> </ul>"},{"location":"workshops/beginner/register-workflows/#copy-the-example-workflows-migration","title":"Copy the example workflows migration","text":"<pre><code>(\ncd migrations/versions/schema\n  curl --remote-name https://raw.githubusercontent.com/workfloworchestrator/example-orchestrator-beginner/main/examples/2022-11-12_8040c515d356_add_user_and_usergroup_workflows.py\n)\n</code></pre> <p>And restart the Docker compose environment.</p>"},{"location":"workshops/beginner/register-workflows/#migrate-workflows-generator-script","title":"Migrate workflows generator script","text":"<p>Similar to <code>db migrate-domain-models</code>, the orchestrator command line interface offers the <code>db migrate-workflows</code> command that walks you through a menu to create a database migration file based on the difference between the registered workflows in the code and the database.</p> <p>Start with the following command:</p> <pre><code>python main.py db migrate-workflows \"add User and UserGroup workflows\"\n</code></pre> <p>Navigate through the menu to add the six workflows to the corresponding <code>User</code> or <code>UserGroup</code> product type. After confirming a migration file will be added to <code>migrations/versions/schema</code> The migration can be run with:</p> <pre><code>python main.py db upgrade heads\n</code></pre>"},{"location":"workshops/beginner/register-workflows/#manual","title":"Manual","text":"<p>Create a new empty database migration with the following command:</p> <pre><code>PYTHONPATH=. python main.py db revision --head data --message \"add User and UserGroup workflows\"\n</code></pre> <p>This will create an empty database migration in the folder <code>migrations/versions/schema</code>. For the migration we will make use of the migration helper functions <code>create_workflow</code> and <code>delete_workflow</code> that both expect a <code>Dict</code> that describes the workflow registration to be added or deleted from the database.</p> <p>To add all User and UserGroup workflows in bulk a list of <code>Dict</code> is created, for only the UserGroup create workflow the list looks like this:</p> <pre><code>from orchestrator.targets import Target\n\nnew_workflows = [\n    {\n        \"name\": \"create_user_group\",\n        \"target\": Target.CREATE,\n        \"description\": \"Create user group\",\n        \"product_type\": \"UserGroup\",\n    },\n]\n</code></pre> <p>This registers the workflow function <code>create_user_group</code> as a create workflow for the <code>UserGroup</code> product.</p> <p>Add a list of <code>Dict</code>s describing the create, modify and terminate workflows for both the <code>UserGroup</code> and <code>User</code> products to the migration that was created above.</p> <p>The migration <code>upgrade</code> and <code>downgrade</code> functions will just loop through the list:</p> <pre><code>from orchestrator.migrations.helpers import create_workflow, delete_workflow\n\n\ndef upgrade() -&gt; None:\n    conn = op.get_bind()\n    for workflow in new_workflows:\n        create_workflow(conn, workflow)\n\n\ndef downgrade() -&gt; None:\n    conn = op.get_bind()\n    for workflow in new_workflows:\n        delete_workflow(conn, workflow[\"name\"])\n</code></pre> <p>Run the migration with the following command:</p> <pre><code>PYTHONPATH=. python main.py db upgrade heads\n</code></pre>"},{"location":"workshops/beginner/scenario/","title":"Scenario","text":"<p>During this workshop a set of products will be created together with the needed workflows to administer users and user groups. The products will be just complex enough to show the basic capabilities of products, product blocks, fixed inputs, resource types and workflows in the workflow orchestrator.</p> <p>The following user and user group attributes will be stored:</p> <ul> <li>UserGroup<ul> <li>group_name: name of the user group</li> <li>group_id: ID of the group in an imaginary group management system </li> </ul> </li> <li>User<ul> <li>affiliation: the user's affiliation (internal or external)</li> <li>username: name of the user</li> <li>age: age of the user</li> <li>user_id: ID of the user in an imaginary user management system</li> <li>group: group the user belongs to</li> </ul> </li> </ul>"},{"location":"workshops/beginner/start-applications/","title":"Start orchestrator and client","text":""},{"location":"workshops/beginner/start-applications/#manual","title":"Manual","text":""},{"location":"workshops/beginner/start-applications/#start-orchestrator","title":"Start orchestrator","text":"<p>From the <code>example-orchestrator</code> folder, use Uvicorn to start the orchestrator:</p> <pre><code>uvicorn --host 127.0.0.1 --port 8080 main:app\n</code></pre> <p>Visit the app to view the API documentation.</p>"},{"location":"workshops/beginner/start-applications/#start-client","title":"Start client","text":"<p>From the <code>orchestrator-core-gui</code> folder, initialize your shell environment with the variables from <code>.env.local</code> and start the client:</p> <pre><code>source .env.local\n# on MacOS make sure node@14 is being used\nexport PATH=\"/usr/local/opt/node@14/bin:$PATH\"\nyarn start\n</code></pre> <p>Point a web browser to <code>$REACT_APP_BACKEND_URL</code>.</p>"},{"location":"workshops/beginner/start-applications/#docker-compose","title":"Docker compose","text":"<p>Using Docker compose the only thing needed to start all application is to run:</p> <pre><code>docker compose up\n</code></pre> <p>And point a browser to <code>http://localhost:3000/</code>.</p> <p>Note</p> <p>Once opened in the browser, ignore the message about the CRM not being responsive, this workshop does not include the setup of an interface to a CRM, fake customers IDs will be used instead.</p>"},{"location":"workshops/beginner/terminate-user-group/","title":"Terminate UserGroup workflow","text":""},{"location":"workshops/beginner/terminate-user-group/#exercise-1-terminate-usergroup-workflow","title":"Exercise 1: terminate UserGroup workflow","text":"<p>The terminate workflow is intended to end an subscription on a product for a customer, releasing all provisioned resources.  The terminate workflow for the <code>UserGroup</code> product uses the following steps:</p> <pre><code>init\n&gt;&gt; store_process_subscription(Target.TERMINATE)\n&gt;&gt; unsync\n&gt;&gt; deprovision_user_group\n&gt;&gt; set_status(SubscriptionLifecycle.TERMINATED)\n&gt;&gt; resync\n&gt;&gt; done\n</code></pre> <p>All builtin steps used here were already discussed and follow the same pattern, record the workflow process that was started for this subscription with <code>store_process_subscription</code>, and protect the steps that modify the subscription and/or OSS and BSS with <code>unsync</code> and <code>resync</code>. The only extra builtin step used in the terminate worfklow is <code>set_status</code> to set the subscription lifecycle state to<code>TERMINATED</code>.</p> <p>The only task for the custom step <code>deprovision_user_group</code> is to deprovision the user group from the imaginary group provisioning system.</p> <p>Also no new or changed input is needed for this workflow, instead the input form is used to ask the user if he/she really wants to terminate the subscription. This is done by using a <code>Label</code> to show a question on the input form:</p> <pre><code>from orchestrator.forms.validators import Label\n\n...\n    class TerminateForm(FormPage):\n        are_you_sure: Label = f\"Are you sure you want to remove {subscription.description}?\"\n...\n</code></pre> <p>Remember that you can use the standard product method <code>from_subscription()</code> to fetch the subscription from the database.</p> <p>Besides the question if the user really wants to terminate the subscription, only the Cancel and Submit button are shown on the input form. If the user clicks the Cancel button then the terminate workflow is not started, so nothing really happens. If the user clicks the Submit button then the terminate workflow is started and will execute all steps that in the end will result in a terminated subscription. Note that none of the workflow steps is using user input from the <code>State</code> because there was no user input given.</p> <p>Use the skeleton below to create the file <code>workflows/user_group/terminate_user_group.py</code>:</p> <pre><code>from orchestrator.forms import FormPage\nfrom orchestrator.forms.validators import Label\nfrom orchestrator.targets import Target\nfrom orchestrator.types import InputForm, State, SubscriptionLifecycle, UUIDstr\nfrom orchestrator.workflow import done, init, step, workflow\nfrom orchestrator.workflows.steps import resync, set_status, store_process_subscription, unsync\nfrom orchestrator.workflows.utils import wrap_modify_initial_input_form\n\nfrom products import UserGroup\n\n# initial input form generator\n...\n\n# deprovision user group step\n...\n\n# terminate user group workflow\n...\n</code></pre> <p>Spoiler: for inspiration look at an example implementation of the user group terminate workflow </p>"},{"location":"workshops/beginner/terminate-user/","title":"Terminate User workflow","text":""},{"location":"workshops/beginner/terminate-user/#exercise-1-terminate-user-workflow","title":"Exercise 1: terminate User workflow","text":"<p>Nothing more needs to be explained at this stage, the terminate user workflow is almost identical to the terminate user group workflow. As a reminder, and for completeness, the terminate workflow for the <code>User</code> product uses  following steps:</p> <pre><code>init\n&gt;&gt; store_process_subscription(Target.TERMINATE)\n&gt;&gt; unsync\n&gt;&gt; deprovision_user\n&gt;&gt; set_status(SubscriptionLifecycle.TERMINATED)\n&gt;&gt; resync\n&gt;&gt; done\n</code></pre> <p>Use the skeleton below to create the file <code>workflows/user/terminate_user.py</code>:</p> <pre><code>from orchestrator.forms import FormPage\nfrom orchestrator.forms.validators import Label\nfrom orchestrator.targets import Target\nfrom orchestrator.types import InputForm, SubscriptionLifecycle, UUIDstr\nfrom orchestrator.workflow import done, init, step, workflow\nfrom orchestrator.workflows.steps import resync, set_status, store_process_subscription, unsync\nfrom orchestrator.workflows.utils import wrap_modify_initial_input_form\n\nfrom products import User\n\n# initial input form generator\n...\n\n# deprovision user step\n...\n\n# terminate user workflow\n...\n</code></pre> <p>Spoiler: for inspiration look at an example implementation of the user terminate workflow </p>"},{"location":"workshops/beginner/workflow-introduction/","title":"Introduction","text":"<p>The workflow engine is the core of the orchestrator, it is responsible for the following functions:</p> <ul> <li> <p>Safely and reliable manipulate customer Subscriptions from one state to the   next and maintain auditability.</p> </li> <li> <p>Create an API through which Subscriptions can be manipulated programmatically.</p> </li> <li> <p>Execute step functions in order and allow the retry of previously failed   process-steps in an idempotent way.</p> </li> <li> <p>Atomically execute workflow functions.</p> </li> </ul> <p>A workflow is the combination of an initial input form, used to acquire input from the user, and a list of workflow steps. Four types of workflows are distinguished: <code>CREATE</code> workflows that will produce a subscription on a product for a specific customer, <code>MODIFY</code> workflows to manipulate existing subscriptions, <code>TERMINATE</code> workflows to end the subscription on a product for a customer, and <code>SYSTEM</code> workflows that run scheduled and do not have an input form. The latter type of workflows is also referred to as tasks, and can for example be used to validate subscriptions against external operations  support systems (OSS) and business support systems (BSS). The same workflow step can be used in multiple workflows, and a set of workflow steps can be combined in a step list and can be reused as well.  </p> <p>Ideally workflow steps are idempotent. In case a workflow step fails, this allows for save retry functionality without possible unwanted side effects or new failures. This is especially important when a step is used to communicate with external OSS and BSS. But in practice it will not always be possible to make a step one hundred percent idempotent, thus requiring manual intervention before a step can be retried. Note that the workflow steps created in this beginner workshop are not written with idempotency in mind. </p> <p>The <code>workflow</code> decorator takes a description, initial input form, and a target as input and turns a function into a workflow that returns a step list to be executed by the workflow engine in a workflow process. A minimal workflow looks like this:</p> <pre><code>@workflow(\n    \"Create product subscription\",\n    initial_input_form=initial_input_form_generator,\n    target=Target.CREATE,\n)\ndef create_product_subscription():\n    return init &gt;&gt; create_subscription &gt;&gt; done\n</code></pre> <p>Information between workflow steps is passed using <code>State</code>, which is nothing more than a collection of key/value pairs, in Python represented by a <code>Dict</code>, with string keys and arbitrary values. Between steps the <code>State</code> is serialized to JSON and stored in the database. The <code>step</code> decorator is used to turn a function into a workflow step, all arguments to the step function will automatically be initialised with the value from the matching key in the <code>State</code>. In turn the step function will return a <code>Dict</code> of new and/or modified key/value pairs that will be merged into the <code>State</code> to be consumed by the next step. The serialization and deserialization between JSON and the indicated Python types is done automatically. A minimal workflow step looks as follows:</p> <pre><code>@step(\"Create subscription\")\ndef create_subscription(\n    product: UUIDstr,\n    user_input: str,\n) -&gt; State:\n    subscription = build_subscription(product, user_input)\n    return {\"subscription\": subscription}\n</code></pre> <p>The <code>product</code> and <code>user_input</code> arguments are filled from the corresponding key/value pairs in the <code>State</code>, and the new <code>subscription</code> key/value is added to the state to be used by one of the following steps.</p> <p>Every workflow starts with the builtin step <code>init</code> and ends with the builtin step <code>done</code>, with an arbitrary list of other builtin steps or custom steps in between. </p>"}]}